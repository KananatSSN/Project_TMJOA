{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "#tf version should be 2.9.0\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r\"C:\\Users\\Admin\\Desktop\\Data\\data_x\"\n",
    "results_dir = os.path.join(dir_path, r'logs\\EfficientNet-B5')\n",
    "models_dir = os.path.join(dir_path, r'models\\EfficientNet-B5')\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.isdir(models_dir):\n",
    "    os.makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of your dataset\n",
    "\n",
    "TRAIN_PATH = r\"C:\\Users\\Admin\\Desktop\\Data\\data_x\\train\"\n",
    "TRAIN_LABEL_PATH = r\"C:\\Users\\Admin\\Desktop\\Data\\data_x\\train_classification.csv\"\n",
    "\n",
    "VAL_PATH = r\"C:\\Users\\Admin\\Desktop\\Data\\data_x\\validation\"\n",
    "VAL_LABEL_PATH = r\"C:\\Users\\Admin\\Desktop\\Data\\data_x\\validation_classification.csv\"\n",
    "\n",
    "IMG_DIM = (224,224)\n",
    "INPUT_SHAPE = (224,224,3)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 100\n",
    "x_axis_inc = 1 # for plotting the training acc and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_original_df = pd.read_csv(TRAIN_LABEL_PATH)\n",
    "train_original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dataframe(df):\n",
    "    # Create a list to hold the new rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate through each row in the original dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Duplicate the row 234 times (for indices 0-233)\n",
    "        for i in range(224):\n",
    "            new_row = row.copy()\n",
    "            new_row['ID'] = f\"{row['ID']}_x_{i:03d}\"  # Wrap the original ID and add suffix\n",
    "            one_hot_vector = np.zeros(224)\n",
    "            one_hot_vector[i] = 1\n",
    "            new_row['one_hot_vector'] = one_hot_vector.tolist()\n",
    "            new_rows.append(new_row)\n",
    "    \n",
    "    # Create a new dataframe from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "train_original_df = train_original_df.sample(frac=1, random_state=rand_seed).reset_index(drop=True)\n",
    "\n",
    "train_c_erosion_label = train_original_df[['ID','c_erosion']]\n",
    "# Create the new DataFrame\n",
    "train_df = expand_dataframe(train_c_erosion_label)\n",
    "\n",
    "# Shuffle the training data\n",
    "#train_df = train_df.sample(frac=1, random_state=rand_seed).reset_index(drop=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_number = '160'\n",
    "# train_df = train_df[train_df['ID'].str.endswith(slice_number)]\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = train_df\n",
    "target_labels = data_labels['c_erosion']\n",
    "data_labels['image_path'] =  data_labels.apply(lambda row: (os.path.join(TRAIN_PATH, str(row['ID'])) + '.jpg'), axis=1)\n",
    "data_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def check_valid_files(df, column_name='image_path'):\n",
    "    # Create a new column for validity\n",
    "    df['is_valid_file'] = df[column_name].apply(os.path.isfile)\n",
    "    \n",
    "    # Print summary\n",
    "    total = len(df)\n",
    "    valid = df['is_valid_file'].sum()\n",
    "    invalid = total - valid\n",
    "    \n",
    "    print(f\"Total files: {total}\")\n",
    "    print(f\"Valid files: {valid}\")\n",
    "    print(f\"Invalid files: {invalid}\")\n",
    "    \n",
    "    # If there are invalid files, you can get them like this:\n",
    "    if invalid > 0:\n",
    "        print(\"\\nInvalid files:\")\n",
    "        invalid_files = df[~df['is_valid_file']][column_name]\n",
    "        for file in invalid_files:\n",
    "            print(file)\n",
    "    \n",
    "    return df\n",
    "\n",
    "check_valid_file_df = check_valid_files(data_labels)\n",
    "\n",
    "# You can access the results in the DataFrame\n",
    "check_valid_file_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_original_df = pd.read_csv(VAL_LABEL_PATH)\n",
    "val_c_erosion_label = val_original_df[['ID','c_erosion']]\n",
    "# Create the new DataFrame\n",
    "val_df = expand_dataframe(val_c_erosion_label)\n",
    "\n",
    "# Shuffle the validation data\n",
    "#val_df = val_df.sample(frac=1, random_state=rand_seed).reset_index(drop=True)\n",
    "\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter for only the desired slice (ened with 'slice_nunber')\n",
    "# val_df = val_df[val_df['ID'].str.endswith(slice_number)]\n",
    "\n",
    "# val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = val_df[['ID','c_erosion','one_hot_vector']]\n",
    "target_val_labels = val_labels['c_erosion']\n",
    "val_labels['image_path'] =  val_labels.apply(lambda row: (os.path.join(VAL_PATH, str(row['ID'])) + '.jpg'), axis=1)\n",
    "val_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_valid_file_df = check_valid_files(val_labels)\n",
    "check_valid_file_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_patient = 100 # max = 364, can't load more than 100 on my machine\n",
    "\n",
    "train_data_size = 224*number_of_patient\n",
    "validation_data_size = 12096 # max = 12096\n",
    "\n",
    "print(train_data_size, validation_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([img_to_array(load_img(img, target_size=IMG_DIM))\n",
    "                       for img in data_labels['image_path'][0:train_data_size].values.tolist()]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_number_data = np.array([np.array(one_hot_vector) for one_hot_vector in data_labels['one_hot_vector'][0:train_data_size].values.tolist()]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = np.array([img_to_array(load_img(img, target_size=IMG_DIM))\n",
    "                       for img in val_labels['image_path'][0:validation_data_size].values.tolist()]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_layer_number_data = np.array([np.array(one_hot_vector) for one_hot_vector in val_labels['one_hot_vector'][0:validation_data_size].values.tolist()]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Dataset Size:', train_data.shape)\n",
    "print('Validation Dataset Size:', val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_in = tf.keras.applications.efficientnet.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = train_data.astype('int')\n",
    "x_val_new = val_data.astype('int')\n",
    "layer_number_data_new = layer_number_data.astype('int')\n",
    "val_layer_number_data_new = val_layer_number_data.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_in = prep_in(x_train_new)\n",
    "x_val_in = prep_in(x_val_new)\n",
    "layer_number_data = prep_in(layer_number_data_new)\n",
    "val_layer_number_data = prep_in(val_layer_number_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_enc = target_labels[0:train_data_size].to_numpy()\n",
    "val_labels_enc = target_val_labels[0:validation_data_size].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the VGG16 model so we can do transfer learning\n",
    "base_model = tf.keras.applications.EfficientNetB5(input_shape=INPUT_SHAPE, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If stuck here try\n",
    "\n",
    "conda uninstall h5py\n",
    "pip uninstall h5py \n",
    "\n",
    "and then\n",
    "\n",
    "conda install h5py==3.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of layers in the base model: ', len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in base_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slapping AvgPool > 1024_Dense > 512_Dense > 1_output on top of the EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in base_model.layers[10:]]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_train = 'EfficientNetB5_A_FE_train_224'\n",
    "log_file = os.path.join(results_dir, 'EfficientNetB5_A_FE_train_224.csv')\n",
    "model_path = os.path.join(models_dir, 'EfficientNetB5_A_FE_224.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model on top\n",
    "\n",
    "layer_number_input = tf.keras.Input(shape=(224,))\n",
    "inputs = tf.keras.Input(shape=INPUT_SHAPE)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing 'training=False'. This is important for fine-tuning\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "#x = Dense(1024, activation='sigmoid')(x)\n",
    "x = tf.keras.layers.Concatenate(axis=1)([x, layer_number_input])\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=[inputs,layer_number_input], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    CSVLogger(log_file),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(models_dir,\"Eff-epoch-{epoch:02d}.h5\"),\n",
    "        save_weights_only=False,\n",
    "        save_best_only=False,\n",
    "        save_freq='epoch',\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), # change from_logits=True\n",
    "              optimizer= keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              metrics = [keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[x_train_in,layer_number_data], \n",
    "                    y=train_labels_enc,\n",
    "                    validation_data=([x_val_in,val_layer_number_data], val_labels_enc),\n",
    "                    epochs=EPOCH, \n",
    "                    verbose=1, \n",
    "                    callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example2D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
