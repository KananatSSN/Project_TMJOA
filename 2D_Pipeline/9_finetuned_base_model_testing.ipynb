{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def test_model(model, test_data):\n",
    "    # Initialize metrics\n",
    "    precision_metric = tf.keras.metrics.Precision()\n",
    "    recall_metric = tf.keras.metrics.Recall()\n",
    "    accuracy_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    \n",
    "    # Iterate over the test data\n",
    "    for x, y_true in test_data:\n",
    "        y_pred = model.predict(x)\n",
    "        \n",
    "        # Update metrics\n",
    "        precision_metric.update_state(y_true, y_pred)\n",
    "        recall_metric.update_state(y_true, y_pred)\n",
    "        accuracy_metric.update_state(y_true, y_pred)\n",
    "    \n",
    "    # Calculate final metric values\n",
    "    precision = precision_metric.result().numpy()\n",
    "    recall = recall_metric.result().numpy()\n",
    "    accuracy = accuracy_metric.result().numpy()\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Binary_Accuracy': accuracy,\n",
    "        'F1_Score': f1_score\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Specify the root folder containing model subfolders\n",
    "    models_root = r\"C:\\Users\\acer\\Desktop\\result_padding_exp\\result_0px\\models\"\n",
    "    \n",
    "    # Specify the CSV file path for results\n",
    "    results_file = r\"C:\\Users\\acer\\Desktop\\result_padding_exp\\result_0px\\basemodel_test_result.csv.txt\"\n",
    "    \n",
    "    # Specify the directory containing your test data\n",
    "    data_dir = \"path/to/your/test/data\"\n",
    "    \n",
    "    # Load your test data\n",
    "    print(\"Loading test data...\")\n",
    "    test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Create a list to store results\n",
    "    results_list = []\n",
    "    \n",
    "    # Iterate through model folders\n",
    "    for model_folder in os.listdir(models_root):\n",
    "        model_folder_path = os.path.join(models_root, model_folder)\n",
    "        \n",
    "        # Skip if not a directory\n",
    "        if not os.path.isdir(model_folder_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing models in folder: {model_folder}\")\n",
    "        \n",
    "        # Iterate through all .h5 files in the model folder\n",
    "        for model_file in os.listdir(model_folder_path):\n",
    "            if model_file.endswith(\".h5\"):\n",
    "                model_path = os.path.join(model_folder_path, model_file)\n",
    "                full_model_name = f\"{model_folder}/{model_file}\"\n",
    "                \n",
    "                print(f\"Testing model: {full_model_name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Load the model\n",
    "                    model = load_model(model_path)\n",
    "                    \n",
    "                    # Test the model and get metrics\n",
    "                    metrics = test_model(model, test_data)\n",
    "                    \n",
    "                    # Add model name and metrics to results\n",
    "                    result_row = {'Model_Name': full_model_name}\n",
    "                    result_row.update(metrics)\n",
    "                    results_list.append(result_row)\n",
    "                    \n",
    "                    print(f\"Successfully tested {full_model_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error testing {full_model_name}: {str(e)}\")\n",
    "                    # Add failed model to results with NaN values\n",
    "                    result_row = {\n",
    "                        'Model_Name': full_model_name,\n",
    "                        'Precision': float('nan'),\n",
    "                        'Recall': float('nan'),\n",
    "                        'Binary_Accuracy': float('nan'),\n",
    "                        'F1_Score': float('nan')\n",
    "                    }\n",
    "                    results_list.append(result_row)\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Reorder columns to put Model_Name first\n",
    "    columns_order = ['Model_Name', 'Precision', 'Recall', 'Binary_Accuracy', 'F1_Score']\n",
    "    results_df = results_df[columns_order]\n",
    "    \n",
    "    # Round numeric values to 4 decimal places\n",
    "    numeric_columns = ['Precision', 'Recall', 'Binary_Accuracy', 'F1_Score']\n",
    "    results_df[numeric_columns] = results_df[numeric_columns].round(4)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nTest Results Summary:\")\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    # Display summary statistics grouped by model folder\n",
    "    print(\"\\nSummary Statistics by Model Type:\")\n",
    "    summary_by_model = results_df.copy()\n",
    "    summary_by_model['Model_Type'] = summary_by_model['Model_Name'].apply(lambda x: x.split('/')[0])\n",
    "    summary_stats = summary_by_model.groupby('Model_Type')[numeric_columns].mean().round(4)\n",
    "    print(summary_stats.to_string())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dmodelGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
