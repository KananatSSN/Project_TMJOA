{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def file_to_ndarray(filepath):\n",
    "    # Check the file extension\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "    #print(file_extension)\n",
    "    \n",
    "    try:\n",
    "        if file_extension in ['.nii', '.nii.gz', '.gz']:  # Handle gzipped or regular NIfTI files\n",
    "            # Load the NIfTI file\n",
    "            nii_img = nib.load(filepath)\n",
    "            # Convert to ndarray\n",
    "            data = nii_img.get_fdata()\n",
    "            #print(f\"Loaded NIfTI file: {filepath}\")\n",
    "        else:\n",
    "            print(\"Unsupported file format.\")\n",
    "            return None\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\47-4881 L 2014.nii.gz'\n",
    "\n",
    "voxel = file_to_ndarray(filepath)\n",
    "print(voxel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_array_slice(array3d, axis=0, slice_num=0):\n",
    "    \"\"\"\n",
    "    Display a 2D slice from a 3D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    array3d : numpy.ndarray\n",
    "        Input 3D array\n",
    "    axis : int\n",
    "        Axis along which to take the slice (0, 1, or 2)\n",
    "    slice_num : int\n",
    "        Index of the slice to display\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(array3d, np.ndarray) or array3d.ndim != 3:\n",
    "        raise ValueError(\"Input must be a 3D numpy array\")\n",
    "    \n",
    "    if axis not in [0, 1, 2]:\n",
    "        raise ValueError(\"Axis must be 0, 1, or 2\")\n",
    "    \n",
    "    # Get the maximum valid slice number for the chosen axis\n",
    "    max_slice = array3d.shape[axis] - 1\n",
    "    if slice_num > max_slice:\n",
    "        raise ValueError(f\"Slice number must be between 0 and {max_slice} for axis {axis}\")\n",
    "    \n",
    "    # Extract the slice based on the axis\n",
    "    if axis == 0:\n",
    "        slice_2d = array3d[slice_num, :, :]\n",
    "        title = f\"Slice {slice_num} along axis 0 (front to back)\"\n",
    "    elif axis == 1:\n",
    "        slice_2d = array3d[:, slice_num, :]\n",
    "        title = f\"Slice {slice_num} along axis 1 (top to bottom)\"\n",
    "    else:  # axis == 2\n",
    "        slice_2d = array3d[:, :, slice_num]\n",
    "        title = f\"Slice {slice_num} along axis 2 (left to right)\"\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(slice_2d, cmap='viridis')\n",
    "    plt.colorbar(label='Value')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display different slices\n",
    "display_array_slice(voxel, axis=0, slice_num=112)  # Show third slice along axis 0\n",
    "display_array_slice(voxel, axis=1, slice_num=112)  # Show fourth slice along axis 1\n",
    "display_array_slice(voxel, axis=2, slice_num=112)  # Show fifth slice along axis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(ndarray):\n",
    "    \n",
    "    flat_array = ndarray.flatten()\n",
    "\n",
    "    # Define the bin edges from -4000 to 4000 with a bin size of 10\n",
    "    bins = np.arange(-4000, 4001, 10)  # 2001 to include the endpoint 2000 in the last bin\n",
    "\n",
    "    # Compute histogram\n",
    "    histogram_values, bin_edges = np.histogram(flat_array, bins=bins)\n",
    "\n",
    "    # Convert histogram values to list\n",
    "    #histogram_list = histogram_values.tolist()\n",
    "\n",
    "    return histogram_values, bin_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_list, bin_edges = compute_histogram(voxel)\n",
    "print(histogram_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_histogram_peaks_normal(arr, bin_edges, variance1=1.0, variance2=1.0, height=None, distance=1, prominence=None):\n",
    "    \"\"\"\n",
    "    Plot histogram with local peaks and normal distributions centered at the two highest peaks\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : numpy.ndarray\n",
    "        1D input array of histogram heights\n",
    "    bin_edges : numpy.ndarray\n",
    "        Array of bin edges (should be len(arr) + 1)\n",
    "    variance1 : float\n",
    "        Variance for the normal distribution at the highest peak\n",
    "    variance2 : float\n",
    "        Variance for the normal distribution at the second highest peak\n",
    "    height : float or None\n",
    "        Minimum height of peaks\n",
    "    distance : int\n",
    "        Minimum horizontal distance between peaks\n",
    "    prominence : float or None\n",
    "        Minimum prominence of peaks\n",
    "    \"\"\"\n",
    "    if len(bin_edges) != len(arr) + 1:\n",
    "        raise ValueError(\"bin_edges should have length equal to arr length + 1\")\n",
    "        \n",
    "    # Find local peaks\n",
    "    peaks, properties = find_peaks(arr, height=height, distance=distance, prominence=prominence)\n",
    "    peak_heights = arr[peaks]\n",
    "    \n",
    "    # Sort peaks by height\n",
    "    peak_order = np.argsort(peak_heights)[::-1]\n",
    "    peaks_sorted = peaks[peak_order]\n",
    "    heights_sorted = peak_heights[peak_order]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot histogram bars\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    plt.bar(bin_centers, arr, width=bin_edges[1]-bin_edges[0], \n",
    "            alpha=0.5, color='blue', label='Histogram')\n",
    "    \n",
    "    # Plot peaks\n",
    "    peak_x_positions = bin_centers[peaks_sorted]\n",
    "    plt.scatter(peak_x_positions, heights_sorted, \n",
    "                c='red', s=100, label='Local Peaks')\n",
    "    \n",
    "    # Add normal distributions for top two peaks\n",
    "    colors = ['g', 'm']  # green for first peak, magenta for second\n",
    "    styles = ['--', ':']  # dashed for first peak, dotted for second\n",
    "    variances = [variance1, variance2]\n",
    "    \n",
    "    for i in range(min(2, len(peaks_sorted))):\n",
    "        peak_center = bin_centers[peaks_sorted[i]]\n",
    "        peak_height = heights_sorted[i]\n",
    "        variance = variances[i]\n",
    "        \n",
    "        # Create x range centered around the peak\n",
    "        x_normal = np.linspace(peak_center - 4*np.sqrt(variance), \n",
    "                             peak_center + 4*np.sqrt(variance), \n",
    "                             200)\n",
    "        \n",
    "        # Calculate normal distribution\n",
    "        y_normal = norm.pdf(x_normal, peak_center, np.sqrt(variance))\n",
    "        \n",
    "        # Scale the normal distribution to match peak height\n",
    "        y_normal = y_normal * (peak_height / np.max(y_normal))\n",
    "        \n",
    "        # Plot normal distribution\n",
    "        plt.plot(x_normal, y_normal, \n",
    "                color=colors[i], \n",
    "                linestyle=styles[i],\n",
    "                label=f'Normal at Peak {i+1} (σ²={variance:.1f})', \n",
    "                linewidth=2)\n",
    "        \n",
    "        # Add peak labels\n",
    "        plt.annotate(f'Peak {i+1}', \n",
    "                    (peak_center, peak_height),\n",
    "                    xytext=(5, 5),\n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('Bin Center')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram with Local Peaks and Normal Distributions')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return peaks_sorted, heights_sorted, peak_x_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_val, bin_edges = compute_histogram(voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_y = histogram_val[1:]\n",
    "input_x = bin_edges[1:]\n",
    "\n",
    "# Plot with different variances for each peak\n",
    "peaks, heights, peak_centers = plot_histogram_peaks_normal(\n",
    "    input_y[300:600],\n",
    "    input_x[300:601],\n",
    "    variance1=10000,    # Variance for highest peak\n",
    "    variance2=22000,    # Variance for second peak\n",
    "    height=0,      # No minimum height\n",
    "    distance=20,       # Minimum 5 bins between peaks\n",
    "    prominence=2000     # Minimum prominence\n",
    ")\n",
    "\n",
    "print(\"Peak bin centers:\", peak_centers)\n",
    "print(\"Peak heights:\", heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def rescale_by_probability(image, target_mean, variance):\n",
    "    \"\"\"\n",
    "    Rescale pixel values based on their probability of being the pixel of interest\n",
    "    under a normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : numpy.ndarray\n",
    "        Input grayscale image array\n",
    "    target_mean : float\n",
    "        The target pixel value (mean of the normal distribution)\n",
    "    variance : float\n",
    "        Variance of the normal distribution\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Rescaled image where each pixel value represents the probability\n",
    "        of that pixel being the pixel of interest\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    rescaled = image.copy().astype(float)\n",
    "    \n",
    "    # Calculate probability for each pixel value\n",
    "    probabilities = norm.pdf(rescaled, target_mean, np.sqrt(variance))\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    probabilities = (probabilities - probabilities.min()) / (probabilities.max() - probabilities.min())\n",
    "    \n",
    "    # Optional: Convert to uint8 for visualization (0-255)\n",
    "    rescaled = (probabilities * 255).astype(np.uint8)\n",
    "    \n",
    "    return rescaled\n",
    "\n",
    "def visualize_rescaling(original, rescaled, target_mean, variance):\n",
    "    \"\"\"\n",
    "    Visualize original and rescaled images side by side\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image\n",
    "    im1 = ax1.imshow(original, cmap='gray')\n",
    "    ax1.set_title('Original Image')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Rescaled image\n",
    "    im2 = ax2.imshow(rescaled, cmap='gray')\n",
    "    ax2.set_title(f'Probability Map\\n(mean={target_mean}, variance={variance})')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_2d = voxel[112, :, :]\n",
    "\n",
    "# Target pixel value of 100 with some variance\n",
    "target_mean = 355\n",
    "variance = 10000  # Wide variance to show the effect\n",
    "\n",
    "# Rescale the image\n",
    "rescaled_image_air = rescale_by_probability(slice_2d, target_mean, variance)\n",
    "\n",
    "# Visualize results\n",
    "visualize_rescaling(slice_2d, rescaled_image_air, target_mean, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_2d = voxel[112, :, :]\n",
    "\n",
    "# Target pixel value of 100 with some variance\n",
    "target_mean = 935\n",
    "variance = 22000  # Wide variance to show the effect\n",
    "\n",
    "# Rescale the image\n",
    "rescaled_image_bone = rescale_by_probability(slice_2d, target_mean, variance)\n",
    "\n",
    "# Visualize results\n",
    "visualize_rescaling(slice_2d, rescaled_image_bone, target_mean, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rescale_range(array, a, b):\n",
    "    \"\"\"\n",
    "    Rescale values in range [a,b] to [0,255] based on their position in the range.\n",
    "    Values outside [a,b] become 0.\n",
    "    \n",
    "    Parameters:\n",
    "    array: np.ndarray - Input array\n",
    "    a: float - Lower bound of the range\n",
    "    b: float - Upper bound of the range\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = array.copy()\n",
    "    \n",
    "    # Set values outside [a,b] to 0\n",
    "    result[result < a] = 0\n",
    "    result[result > b] = 0\n",
    "    \n",
    "    # Find values within the range\n",
    "    mask = (result >= a) & (result <= b)\n",
    "    \n",
    "    # Linear rescaling of values in range [a,b] to [0,255]\n",
    "    result[mask] = ((result[mask] - a) / (b - a)) * 255\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# array = your_array\n",
    "# rescaled = rescale_range(array, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_2d = voxel[112, :, :]\n",
    "a = 355\n",
    "b = 935\n",
    "adjust_const = 0.25\n",
    "adjust = int((b-a)*adjust_const)\n",
    "print(b-a, adjust)\n",
    "\n",
    "area_of_uncertainty = rescale_range(slice_2d, a+adjust, b-adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rescaling(slice_2d, area_of_uncertainty, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have three 2D arrays of the same shape\n",
    "# Convert them to uint8 if they aren't already\n",
    "red = rescaled_image_air.astype(np.uint8)\n",
    "green = area_of_uncertainty.astype(np.uint8)\n",
    "blue = rescaled_image_bone.astype(np.uint8)\n",
    "\n",
    "# Combine into RGB\n",
    "rgb_image = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(rgb_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(source_folder, destination_base, train_ratio=0.7, val_ratio=0.2):\n",
    "   for split in ['train', 'val', 'test']:\n",
    "       os.makedirs(os.path.join(destination_base, split), exist_ok=True)\n",
    "   \n",
    "   files = [f for f in os.listdir(source_folder) if f.endswith('.nii.gz')]\n",
    "   random.shuffle(files)\n",
    "   \n",
    "   n_files = len(files)\n",
    "   n_train = int(n_files * train_ratio)\n",
    "   n_val = int(n_files * val_ratio)\n",
    "   \n",
    "   train_files = files[:n_train]\n",
    "   val_files = files[n_train:n_train + n_val]\n",
    "   test_files = files[n_train + n_val:]\n",
    "   \n",
    "   for files, split in [(train_files, 'train'), \n",
    "                       (val_files, 'val'), \n",
    "                       (test_files, 'test')]:\n",
    "       for f in files:\n",
    "           shutil.copy2(os.path.join(source_folder, f),\n",
    "                       os.path.join(destination_base, split, f))\n",
    "\n",
    "# Usage\n",
    "source_folder = r'D:\\Kananat\\_Segmented_Preprocessed_expand5px'\n",
    "destination_folder = r'D:\\Kananat\\_dataset'\n",
    "split_dataset(source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def process_nii_files(folder_path):\n",
    "   # Initialize histogram bins\n",
    "   bins = np.arange(-1500, 2001)  # -1500 to 2000 inclusive\n",
    "   total_hist = np.zeros(len(bins)-1)\n",
    "   \n",
    "   # Process each file\n",
    "   for filename in os.listdir(folder_path):\n",
    "       if filename.endswith('.nii.gz'):\n",
    "           # Load image\n",
    "           print(filename)\n",
    "           img = nib.load(os.path.join(folder_path, filename))\n",
    "           data = img.get_fdata()\n",
    "           \n",
    "           # Calculate histogram for this image\n",
    "           hist, _ = np.histogram(data, bins=bins)\n",
    "           total_hist += hist\n",
    "           \n",
    "           # Clear memory\n",
    "           del data\n",
    "           img = None\n",
    "\n",
    "   return total_hist, bins\n",
    "\n",
    "# Usage\n",
    "folder_path = r'D:\\Kananat\\_dataset\\train'\n",
    "histogram, bin_edges = process_nii_files(folder_path)\n",
    "\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(bin_edges[:-1], histogram, width=1)\n",
    "plt.xlabel('Voxel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Voxel Value Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "\n",
    "def fit_gmm(histogram_data, bin_edges):\n",
    "   # Create dataset by repeating values according to histogram frequencies\n",
    "   data = []\n",
    "   for i in range(len(histogram_data)):\n",
    "       count = int(histogram_data[i])\n",
    "       if count > 0:\n",
    "           # Use bin edges to create uniform samples within each bin\n",
    "           samples = np.random.uniform(bin_edges[i], bin_edges[i+1], count)\n",
    "           data.extend(samples)\n",
    "   \n",
    "   data = np.array(data).reshape(-1, 1)\n",
    "   \n",
    "   # Fit GMM\n",
    "   gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "   gmm.fit(data)\n",
    "   \n",
    "   return gmm.means_.flatten(), gmm.covariances_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, variance = fit_gmm(histogram, bin_edges)\n",
    "print(mean,variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_y = histogram\n",
    "input_x = bin_edges\n",
    "\n",
    "# Plot with different variances for each peak\n",
    "peaks, heights, peak_centers = plot_histogram_peaks_normal(\n",
    "    input_y,\n",
    "    input_x,\n",
    "    variance1=23000,    # Variance for highest peak\n",
    "    variance2=34000,    # Variance for second peak\n",
    "    height=0,      # No minimum height\n",
    "    distance=500,       # Minimum 5 bins between peaks\n",
    "    prominence=50000     # Minimum prominence\n",
    ")\n",
    "\n",
    "print(\"Peak bin centers:\", peak_centers)\n",
    "print(\"Peak heights:\", heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\Kananat\\_dataset\\val\\58-42016 L.nii.gz'\n",
    "\n",
    "voxel = file_to_ndarray(filepath)\n",
    "\n",
    "slice_2d = voxel[112, :, :]\n",
    "\n",
    "# Target pixel value of 100 with some variance\n",
    "target_mean = 313\n",
    "variance = 23000  # Wide variance to show the effect\n",
    "\n",
    "# Rescale the image\n",
    "rescaled_image_air = rescale_by_probability(slice_2d, target_mean, variance)\n",
    "\n",
    "# Target pixel value of 100 with some variance\n",
    "target_mean = 875\n",
    "variance = 34000  # Wide variance to show the effect\n",
    "\n",
    "# Rescale the image\n",
    "rescaled_image_bone = rescale_by_probability(slice_2d, target_mean, variance)\n",
    "\n",
    "a = 313\n",
    "b = 875\n",
    "adjust_const = 0.25\n",
    "adjust = int((b-a)*adjust_const)\n",
    "print(b-a, adjust)\n",
    "\n",
    "area_of_uncertainty = rescale_range(slice_2d, a+adjust, b-adjust)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have three 2D arrays of the same shape\n",
    "# Convert them to uint8 if they aren't already\n",
    "red = rescaled_image_air.astype(np.uint8)\n",
    "green = area_of_uncertainty.astype(np.uint8)\n",
    "blue = rescaled_image_bone.astype(np.uint8)\n",
    "\n",
    "# Combine into RGB\n",
    "rgb_image = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(rgb_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_ndarray(filepath):\n",
    "    # Check the file extension\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "    #print(file_extension)\n",
    "    \n",
    "    try:\n",
    "        if file_extension in ['.nii', '.nii.gz', '.gz']:  # Handle gzipped or regular NIfTI files\n",
    "            # Load the NIfTI file\n",
    "            nii_img = nib.load(filepath)\n",
    "            # Convert to ndarray\n",
    "            data = nii_img.get_fdata()\n",
    "            #print(f\"Loaded NIfTI file: {filepath}\")\n",
    "        else:\n",
    "            print(\"Unsupported file format.\")\n",
    "            return None\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the file: {e}\")\n",
    "        return None\n",
    "\n",
    "def fit_gmm(histogram_data, bin_edges):\n",
    "   # Create dataset by repeating values according to histogram frequencies\n",
    "   data = []\n",
    "   for i in range(len(histogram_data)):\n",
    "       count = int(histogram_data[i])\n",
    "       if count > 0:\n",
    "           # Use bin edges to create uniform samples within each bin\n",
    "           samples = np.random.uniform(bin_edges[i], bin_edges[i+1], count)\n",
    "           data.extend(samples)\n",
    "   \n",
    "   data = np.array(data).reshape(-1, 1)\n",
    "   \n",
    "   # Fit GMM\n",
    "   gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "   gmm.fit(data)\n",
    "   \n",
    "   return gmm.means_.flatten(), gmm.covariances_.flatten()\n",
    "\n",
    "def rescale_by_probability(image, target_mean, variance):\n",
    "    \"\"\"\n",
    "    Rescale pixel values based on their probability of being the pixel of interest\n",
    "    under a normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : numpy.ndarray\n",
    "        Input grayscale image array\n",
    "    target_mean : float\n",
    "        The target pixel value (mean of the normal distribution)\n",
    "    variance : float\n",
    "        Variance of the normal distribution\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Rescaled image where each pixel value represents the probability\n",
    "        of that pixel being the pixel of interest\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    rescaled = image.copy().astype(float)\n",
    "    \n",
    "    # Calculate probability for each pixel value\n",
    "    probabilities = norm.pdf(rescaled, target_mean, np.sqrt(variance))\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    probabilities = (probabilities - probabilities.min()) / (probabilities.max() - probabilities.min())\n",
    "    \n",
    "    # Optional: Convert to uint8 for visualization (0-255)\n",
    "    rescaled = (probabilities * 255).astype(np.uint8)\n",
    "    \n",
    "    return rescaled\n",
    "\n",
    "def rescale_range(array, a, b):\n",
    "    \"\"\"\n",
    "    Rescale values in range [a,b] to [0,255] based on their position in the range.\n",
    "    Values outside [a,b] become 0.\n",
    "    \n",
    "    Parameters:\n",
    "    array: np.ndarray - Input array\n",
    "    a: float - Lower bound of the range\n",
    "    b: float - Upper bound of the range\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = array.copy()\n",
    "    \n",
    "    # Set values outside [a,b] to 0\n",
    "    result[result < a] = 0\n",
    "    result[result > b] = 0\n",
    "    \n",
    "    # Find values within the range\n",
    "    mask = (result >= a) & (result <= b)\n",
    "    \n",
    "    # Linear rescaling of values in range [a,b] to [0,255]\n",
    "    result[mask] = ((result[mask] - a) / (b - a)) * 255\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filepath = r\"C:\\Users\\acer\\Desktop\\Data\\47-4881 L 2014.nii.gz\"\n",
    "voxel = file_to_ndarray(filepath)\n",
    "\n",
    "# Compute histogram\n",
    "flat_array = voxel.flatten()\n",
    "bins = np.arange(-500, 1500, 1)  # -500 to 1500 size 1\n",
    "\n",
    "histogram_values, bin_edges = np.histogram(flat_array, bins=bins)\n",
    "mean, variance = fit_gmm(histogram_values, bin_edges)\n",
    "print(f\"mean : {mean}, variance : {variance}\")\n",
    "\n",
    "slice_2d = voxel[112, :, :]\n",
    "rescaled_image_air = rescale_by_probability(slice_2d, int(mean[0]), int(variance[0]))\n",
    "rescaled_image_bone = rescale_by_probability(slice_2d, int(mean[1]), int(variance[1]))\n",
    "\n",
    "a = mean[0]\n",
    "b = mean[1]\n",
    "adjust_const = 0.25\n",
    "adjust = int((b-a)*adjust_const)\n",
    "area_of_uncertainty = rescale_range(slice_2d, a+adjust, b-adjust)\n",
    "\n",
    "red = rescaled_image_air.astype(np.uint8)\n",
    "green = area_of_uncertainty.astype(np.uint8)\n",
    "blue = rescaled_image_bone.astype(np.uint8)\n",
    "\n",
    "# Combine into RGB\n",
    "rgb_image = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(rgb_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gmm(histogram_data, bin_edges):\n",
    "   # Create dataset by repeating values according to histogram frequencies\n",
    "   data = []\n",
    "   for i in range(len(histogram_data)):\n",
    "       count = int(histogram_data[i])\n",
    "       if count > 0:\n",
    "            # Use bin edges to create uniform samples within each bin\n",
    "            samples = np.random.uniform(bin_edges[i], bin_edges[i+1], count)\n",
    "            data.extend(samples)\n",
    "   \n",
    "   data = np.array(data).reshape(-1, 1)\n",
    "   \n",
    "   # Fit GMM\n",
    "   gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "   gmm.fit(data)\n",
    "\n",
    "   output_mean = np.sort(gmm.means_.flatten())\n",
    "   output_variances = gmm.covariances_.flatten()\n",
    "\n",
    "   if output_mean[0] != gmm.means_.flatten()[0] :\n",
    "        output_variances = np.flip(gmm.covariances_.flatten())\n",
    "\n",
    "   return output_mean, output_variances\n",
    "\n",
    "def rescale_uncertainty(array, a, b):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = array.copy()\n",
    "    \n",
    "    # Set values outside [a,b] to 0\n",
    "    result[result < a] = 0\n",
    "    result[result > b] = 0\n",
    "    \n",
    "    # Find values within the range\n",
    "    mask = (result >= a) & (result <= b)\n",
    "    \n",
    "    # Linear rescaling of values in range [a,b] to [0,255]\n",
    "    result[mask] = ((result[mask] - a) / (b - a)) * 255\n",
    "    \n",
    "    return result\n",
    "\n",
    "def rescale_by_probability(image, target_mean, variance):\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    rescaled = image.copy().astype(float)\n",
    "    \n",
    "    # Calculate probability for each pixel value\n",
    "    probabilities = norm.pdf(rescaled, target_mean, np.sqrt(variance))\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    probabilities = (probabilities - probabilities.min()) / (probabilities.max() - probabilities.min())\n",
    "    \n",
    "    # Optional: Convert to uint8 for visualization (0-255)\n",
    "    rescaled = (probabilities * 255).astype(np.uint8)\n",
    "    \n",
    "    return rescaled\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pad_image(image, target_size=(224, 224, 3)):\n",
    "    # Calculate padding amounts\n",
    "    h_padding = (target_size[0] - image.shape[0]) // 2\n",
    "    w_padding = (target_size[1] - image.shape[1]) // 2\n",
    "    \n",
    "    # Calculate extra padding if odd dimension\n",
    "    h_extra = (target_size[0] - image.shape[0]) % 2\n",
    "    w_extra = (target_size[1] - image.shape[1]) % 2\n",
    "    \n",
    "    # Pad the image\n",
    "    padded_image = np.pad(\n",
    "        image,\n",
    "        ((h_padding, h_padding + h_extra),  # Height padding\n",
    "         (w_padding, w_padding + w_extra),  # Width padding\n",
    "         (0, 0)),                          # No padding for channels\n",
    "        mode='constant',\n",
    "        constant_values=0\n",
    "    )\n",
    "    \n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def slice_extraction(input_folder, output_base_dir):\n",
    "    nii_files = glob(os.path.join(input_folder, '*.nii.gz'))\n",
    "\n",
    "    for nii_file in nii_files:\n",
    "        print(f\"Processing : {nii_file}\")\n",
    "\n",
    "        img = nib.load(nii_file)\n",
    "        data = img.get_fdata()\n",
    "        \n",
    "        base_name = os.path.splitext(os.path.splitext(os.path.basename(nii_file))[0])[0]\n",
    "\n",
    "        flat_array = data.flatten()\n",
    "        bins = np.arange(-500, 1500, 1)  # -500 to 1500 size 1\n",
    "        histogram_values, bin_edges = np.histogram(flat_array, bins=bins)\n",
    "\n",
    "        mean, variance = fit_gmm(histogram_values, bin_edges)\n",
    "        print(f\"mean : {mean}, variance : {variance}\")\n",
    "\n",
    "        for i in range(224):\n",
    "\n",
    "            if i%10 != 0 :\n",
    "                continue\n",
    "\n",
    "            slice_2d = data[i, 0:156 , 0:156]\n",
    "\n",
    "            empty_count = np.sum(slice_2d < -2000)\n",
    "            empty_ratio = empty_count / (slice_2d.shape[0] * slice_2d.shape[1])\n",
    "\n",
    "            if empty_ratio > 0.9:\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracting slice number : {i}\")\n",
    "\n",
    "            rescaled_image_air = rescale_by_probability(slice_2d, int(mean[0]), int(variance[0]))\n",
    "            rescaled_image_bone = rescale_by_probability(slice_2d, int(mean[1]), int(variance[1]))\n",
    "\n",
    "            adjust_const = 0.25\n",
    "            adjust = int((mean[1]-mean[0])*adjust_const)\n",
    "            area_of_uncertainty = rescale_uncertainty(slice_2d, mean[0]+adjust, mean[1]-adjust)\n",
    "\n",
    "            red = rescaled_image_air.astype(np.uint8)\n",
    "            green = area_of_uncertainty.astype(np.uint8)\n",
    "            blue = rescaled_image_bone.astype(np.uint8)\n",
    "\n",
    "            rgb_image = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "            rgb_image = pad_image(rgb_image)\n",
    "\n",
    "            out_file = os.path.join(output_base_dir, f\"{base_name}_{i:03d}.jpg\")\n",
    "\n",
    "            img = Image.fromarray(rgb_image)\n",
    "            img.save(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\52-15242 R.nii.gz\n",
      "mean : [247.95125756 740.14255891], variance : [15395.27118204 34864.93204678]\n",
      "Extracting slice number : 30\n",
      "Extracting slice number : 40\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Extracting slice number : 170\n",
      "Extracting slice number : 180\n",
      "Extracting slice number : 190\n",
      "Extracting slice number : 200\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\54-21497 L 2016.nii.gz\n",
      "mean : [356.43864997 876.00377692], variance : [19520.96609581 20996.83125927]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\58-651 L.nii.gz\n",
      "mean : [279.23409835 814.78177432], variance : [15796.08311316 29580.4079994 ]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\60-38868 R.nii.gz\n",
      "mean : [403.77148602 884.40639292], variance : [30809.71767119 29106.85913584]\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\61-16015 L.nii.gz\n",
      "mean : [261.39157556 825.13298092], variance : [24699.66374584 19890.5319639 ]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\61-6974 R.nii.gz\n",
      "mean : [339.8013783  868.68580236], variance : [22459.96964137 16004.97880654]\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\62-700851 R.nii.gz\n",
      "mean : [210.5702978  751.88620507], variance : [ 9893.10788472 45599.8358701 ]\n",
      "Extracting slice number : 30\n",
      "Extracting slice number : 40\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\62-701261 R.nii.gz\n",
      "mean : [299.60030255 803.31617947], variance : [17791.65876596 25170.36017634]\n",
      "Extracting slice number : 40\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\63-12252 R.nii.gz\n",
      "mean : [363.55738426 955.59361892], variance : [24351.77509453 25577.81697869]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\63-20205 R.nii.gz\n",
      "mean : [382.7642512  874.89236691], variance : [15825.14078084 23941.20543821]\n",
      "Extracting slice number : 20\n",
      "Extracting slice number : 30\n",
      "Extracting slice number : 40\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\63-700385 R.nii.gz\n",
      "mean : [ 395.63008735 1012.97156664], variance : [31725.75362007 29721.24673696]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Extracting slice number : 170\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\64-6935 L.nii.gz\n",
      "mean : [348.95602685 884.10260914], variance : [18338.76844794 44868.34402912]\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Extracting slice number : 170\n",
      "Extracting slice number : 180\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\64-8619 L 2021.nii.gz\n",
      "mean : [ 412.69087529 1001.72539413], variance : [32716.83903796 14775.77740695]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Extracting slice number : 170\n",
      "Extracting slice number : 180\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\65-11496 L.nii.gz\n",
      "mean : [306.82486759 815.47411805], variance : [15622.17597195 31029.13654224]\n",
      "Extracting slice number : 40\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Extracting slice number : 170\n",
      "Extracting slice number : 180\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\65-14582 R 2023.nii.gz\n",
      "mean : [243.32955926 861.05174877], variance : [ 9680.05891495 51543.50844344]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Extracting slice number : 170\n",
      "Extracting slice number : 180\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\65-14920 L.nii.gz\n",
      "mean : [339.79983019 869.96647869], variance : [25956.32599693 20365.03528117]\n",
      "Extracting slice number : 40\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\65-1579 R.nii.gz\n",
      "mean : [311.14158799 932.47258393], variance : [22520.01391889 37907.46691705]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\65-2425 L 2022 02 17.nii.gz\n",
      "mean : [356.60471078 971.17146521], variance : [23316.40905666 37911.80976445]\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Extracting slice number : 150\n",
      "Extracting slice number : 160\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\65-700003 R.nii.gz\n",
      "mean : [372.94108886 976.30944418], variance : [25148.47501795 32895.70815769]\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n",
      "Processing : D:\\Kananat\\_dataset\\test\\erosion_1\\66-4559 R.nii.gz\n",
      "mean : [365.68702304 770.00215528], variance : [28361.53751722 48189.37174045]\n",
      "Extracting slice number : 50\n",
      "Extracting slice number : 60\n",
      "Extracting slice number : 70\n",
      "Extracting slice number : 80\n",
      "Extracting slice number : 90\n",
      "Extracting slice number : 100\n",
      "Extracting slice number : 110\n",
      "Extracting slice number : 120\n",
      "Extracting slice number : 130\n",
      "Extracting slice number : 140\n"
     ]
    }
   ],
   "source": [
    "input_folder = r\"D:\\Kananat\\_dataset\\test\\erosion_1\"\n",
    "output_base_dir = r\"D:\\Kananat\\_dataset_2d\\test\\erosion_1\"\n",
    "\n",
    "slice_extraction(input_folder, output_base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dmodelGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
