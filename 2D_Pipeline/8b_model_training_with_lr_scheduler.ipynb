{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(tf.__version__, np.__version__)\n",
    "\n",
    "# Expected output 2.9.0, 1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect GPU and limit memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU found and successfully configured\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus != []:\n",
    "    print(gpus)\n",
    "    for gpu in gpus: \n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPU on this machine\")\n",
    "\n",
    "# Expected output [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "if USE_GPU == False:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found and successfully configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_function(model_name):\n",
    "    \"\"\"Get the appropriate preprocessing function for each model\"\"\"\n",
    "    preprocess_dict = {\n",
    "        'DenseNet201': tf.keras.applications.densenet.preprocess_input,\n",
    "        'EfficientNetB7': tf.keras.applications.efficientnet.preprocess_input,\n",
    "        'EfficientNetV2L': tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "        'InceptionResNetV2': tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "        'InceptionV3': tf.keras.applications.inception_v3.preprocess_input,\n",
    "        'MobileNetV3Large': tf.keras.applications.mobilenet_v3.preprocess_input,\n",
    "        'NASNetLarge': tf.keras.applications.nasnet.preprocess_input,\n",
    "        'ResNet152': tf.keras.applications.resnet.preprocess_input,\n",
    "        'ResNet152V2': tf.keras.applications.resnet_v2.preprocess_input,\n",
    "        'VGG19': tf.keras.applications.vgg19.preprocess_input,\n",
    "        'Xception': tf.keras.applications.xception.preprocess_input\n",
    "    }\n",
    "    return preprocess_dict.get(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, preprocess_fn):\n",
    "    \"\"\"Apply preprocessing to a dataset\"\"\"\n",
    "    def preprocess(x, y):\n",
    "        return preprocess_fn(x), y\n",
    "    \n",
    "    return data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(train_data_dir, model_name, train_data_ratio=0.8, batch_size=8):\n",
    "    print(f\"Loading training data from {train_data_dir}...\")\n",
    "\n",
    "    train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        train_data = preprocess_data(train_data, preprocess_fn)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_size = int(len(train_data)*train_data_ratio)\n",
    "    val_size = int(len(train_data)*(1-train_data_ratio))\n",
    "    train = train_data.take(train_size)\n",
    "    val = train_data.skip(train_size).take(val_size)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    train = train.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val = val.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "def load_test_data(test_data_dir, model_name, batch_size=8):\n",
    "    print(f\"Loading testing data from {test_data_dir}...\")\n",
    "\n",
    "    test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        test_data = preprocess_data(test_data, preprocess_fn)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    test_data = test_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = ['DesnseNet201']\n",
    "INPUT_SHAPE = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_model = ['DenseNet201', 'EfficientNetB7', 'EfficientNetV2L','InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'NASNetLarge', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "\n",
    "def get_base_model(model_name, input_shape):\n",
    "    model_dict = {\n",
    "        'DenseNet201': tf.keras.applications.DenseNet201,\n",
    "        'EfficientNetB7': tf.keras.applications.EfficientNetB7,\n",
    "        'EfficientNetV2L': tf.keras.applications.EfficientNetV2L,\n",
    "        'InceptionResNetV2': tf.keras.applications.InceptionResNetV2,\n",
    "        'InceptionV3': tf.keras.applications.InceptionV3,\n",
    "        'MobileNetV3Large': tf.keras.applications.MobileNetV3Large,\n",
    "        'NASNetLarge': tf.keras.applications.NASNetLarge,\n",
    "        'ResNet152': tf.keras.applications.ResNet152,\n",
    "        'ResNet152V2': tf.keras.applications.ResNet152V2,\n",
    "        'VGG19': tf.keras.applications.VGG19,\n",
    "        'Xception': tf.keras.applications.Xception,\n",
    "    }\n",
    "    \n",
    "    if model_name not in model_dict:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    return model_dict[model_name](input_shape=input_shape, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, input_shape):\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    print(\"Built model\")\n",
    "    pd.set_option('max_colwidth', None)\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "    pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "\n",
    "# input_shape = (224, 224, 3)\n",
    "# base_model = get_base_model(model_name, input_shape)\n",
    "# model = build_model(base_model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name, results_dir, models_dir, initial_lr=\"001\"):\n",
    "    log_file = os.path.join(results_dir, f\"{model_name}_adaptive_lr{initial_lr}.csv\")\n",
    "    \n",
    "    return [\n",
    "        CSVLogger(log_file),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(models_dir, f\"{model_name}_adaptive_lr{initial_lr}.h5\"),\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            save_freq='epoch',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',        # Monitors validation loss\n",
    "            factor=0.1,               # Multiplies learning rate by 0.1 (reduces by 90%)\n",
    "            patience=5,               # Waits 3 epochs before reducing\n",
    "            verbose=1,                # Prints message when reducing LR\n",
    "            min_delta=1e-4,          # Minimum change in loss to be considered an improvement\n",
    "            min_lr=1e-4              # Won't reduce LR below this value\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, results_dir, lr_suffix=\"001\"):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "    plt.title(f'{model_name} Loss (LR: 0.{lr_suffix})')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"{model_name}_loss_lr{lr_suffix}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, test_data, models_dir, results_dir):\n",
    "    \"\"\"\n",
    "    Test model and save metrics to CSV\n",
    "    \n",
    "    Parameters:\n",
    "        model_name: Name of the model\n",
    "        test_data: Test dataset\n",
    "        models_dir: Directory containing model files\n",
    "        results_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(os.path.join(models_dir, f\"{model_name}_adaptive_lr001.h5\"))\n",
    "    \n",
    "    # Initialize metrics\n",
    "    precision = Precision()\n",
    "    recall = Recall()  # Same as Sensitivity\n",
    "    accuracy = BinaryAccuracy()\n",
    "    \n",
    "    # Lists to store predictions and true labels for specificity calculation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Evaluate model on test data\n",
    "    for batch in test_data.as_numpy_iterator():\n",
    "        X, y = batch\n",
    "        yhat = model.predict(X)\n",
    "        \n",
    "        # Update metrics\n",
    "        precision.update_state(y, yhat)\n",
    "        recall.update_state(y, yhat)\n",
    "        accuracy.update_state(y, yhat)\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend([1 if pred >= 0.5 else 0 for pred in yhat])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_value = float(precision.result())\n",
    "    recall_value = float(recall.result())  # Sensitivity\n",
    "    accuracy_value = float(accuracy.result())\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    f1_score = 2 * (precision_value * recall_value) / (precision_value + recall_value)\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    # True Negatives / (True Negatives + False Positives)\n",
    "    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n",
    "    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_value,\n",
    "        'Precision': precision_value,\n",
    "        'Recall (Sensitivity)': recall_value,\n",
    "        'Specificity': specificity,\n",
    "        'F1 Score': f1_score\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Test Results:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Model':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    csv_file = os.path.join(results_dir, 'model_metrics.csv')\n",
    "    \n",
    "    # Check if file exists to determine if we need to write headers\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metrics.keys())\n",
    "        \n",
    "        # Write headers if file doesn't exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerow(metrics)\n",
    "    \n",
    "    print(f\"\\nMetrics saved to: {csv_file}\")\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_names, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=100, batch_size=32):\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"Training {model_name}...\")\n",
    "\n",
    "        train, val = load_training_data(train_data_dir, model_name, train_data_ratio=0.8, batch_size=batch_size)\n",
    "\n",
    "        # Set up model-specific directories\n",
    "        results_dir = os.path.join(results_base_dir, model_name)\n",
    "        models_dir = os.path.join(models_base_dir, model_name)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Build model\n",
    "        input_shape = (224, 224, 3)\n",
    "        base_model = get_base_model(model_name, input_shape)\n",
    "        model = build_model(base_model, input_shape)\n",
    "\n",
    "        # Compile model with initial learning rate\n",
    "        initial_lr = 1e-2\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "            metrics=[keras.metrics.BinaryAccuracy()]\n",
    "        )\n",
    "\n",
    "        # Print model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Set up callbacks with adaptive learning rate and early stopping\n",
    "        callbacks = get_callbacks(model_name, results_dir, models_dir)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train,\n",
    "            validation_data=val,\n",
    "            epochs=max_epochs,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history, model_name, results_dir)\n",
    "\n",
    "        # Test model\n",
    "        test_data = load_test_data(test_data_dir, batch_size=1)\n",
    "        test_model(model_name, test_data, models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EfficientNetB7...\n",
      "Loading training data from D:\\Kananat\\TF_TMJOA_jpg_x_5px...\n",
      "Found 34203 files belonging to 2 classes.\n",
      "Built model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb7 (Functional)  (None, 7, 7, 2560)       64097687  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2560)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2622464   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,245,464\n",
      "Trainable params: 3,147,777\n",
      "Non-trainable params: 64,097,687\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.5347 - binary_accuracy: 0.7511\n",
      "Epoch 1: val_loss improved from inf to 0.48939, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 1958s 1s/step - loss: 0.5347 - binary_accuracy: 0.7511 - val_loss: 0.4894 - val_binary_accuracy: 0.7177 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.4463 - binary_accuracy: 0.7894\n",
      "Epoch 2: val_loss improved from 0.48939 to 0.39744, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 2220s 1s/step - loss: 0.4463 - binary_accuracy: 0.7894 - val_loss: 0.3974 - val_binary_accuracy: 0.8182 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.4094 - binary_accuracy: 0.8089\n",
      "Epoch 3: val_loss did not improve from 0.39744\n",
      "1710/1710 [==============================] - 2212s 1s/step - loss: 0.4094 - binary_accuracy: 0.8089 - val_loss: 0.4022 - val_binary_accuracy: 0.8027 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.3765 - binary_accuracy: 0.8258\n",
      "Epoch 4: val_loss did not improve from 0.39744\n",
      "1710/1710 [==============================] - 1909s 1s/step - loss: 0.3765 - binary_accuracy: 0.8258 - val_loss: 0.4035 - val_binary_accuracy: 0.8078 - lr: 0.0100\n",
      "Epoch 5/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.3546 - binary_accuracy: 0.8387\n",
      "Epoch 5: val_loss improved from 0.39744 to 0.37159, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 2219s 1s/step - loss: 0.3546 - binary_accuracy: 0.8387 - val_loss: 0.3716 - val_binary_accuracy: 0.8337 - lr: 0.0100\n",
      "Epoch 6/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.3311 - binary_accuracy: 0.8503\n",
      "Epoch 6: val_loss improved from 0.37159 to 0.32864, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 2220s 1s/step - loss: 0.3311 - binary_accuracy: 0.8503 - val_loss: 0.3286 - val_binary_accuracy: 0.8570 - lr: 0.0100\n",
      "Epoch 7/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.3194 - binary_accuracy: 0.8557\n",
      "Epoch 7: val_loss improved from 0.32864 to 0.32795, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 2219s 1s/step - loss: 0.3194 - binary_accuracy: 0.8557 - val_loss: 0.3279 - val_binary_accuracy: 0.8563 - lr: 0.0100\n",
      "Epoch 8/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.3019 - binary_accuracy: 0.8662\n",
      "Epoch 8: val_loss did not improve from 0.32795\n",
      "1710/1710 [==============================] - 2213s 1s/step - loss: 0.3019 - binary_accuracy: 0.8662 - val_loss: 0.3286 - val_binary_accuracy: 0.8596 - lr: 0.0100\n",
      "Epoch 9/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2920 - binary_accuracy: 0.8711\n",
      "Epoch 9: val_loss improved from 0.32795 to 0.30303, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 2220s 1s/step - loss: 0.2920 - binary_accuracy: 0.8711 - val_loss: 0.3030 - val_binary_accuracy: 0.8757 - lr: 0.0100\n",
      "Epoch 10/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2774 - binary_accuracy: 0.8789\n",
      "Epoch 10: val_loss did not improve from 0.30303\n",
      "1710/1710 [==============================] - 2211s 1s/step - loss: 0.2774 - binary_accuracy: 0.8789 - val_loss: 0.3388 - val_binary_accuracy: 0.8554 - lr: 0.0100\n",
      "Epoch 11/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2680 - binary_accuracy: 0.8838\n",
      "Epoch 11: val_loss did not improve from 0.30303\n",
      "1710/1710 [==============================] - 2211s 1s/step - loss: 0.2680 - binary_accuracy: 0.8838 - val_loss: 0.3334 - val_binary_accuracy: 0.8642 - lr: 0.0100\n",
      "Epoch 12/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2599 - binary_accuracy: 0.8875\n",
      "Epoch 12: val_loss improved from 0.30303 to 0.28493, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 2220s 1s/step - loss: 0.2599 - binary_accuracy: 0.8875 - val_loss: 0.2849 - val_binary_accuracy: 0.8737 - lr: 0.0100\n",
      "Epoch 13/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2514 - binary_accuracy: 0.8917\n",
      "Epoch 13: val_loss did not improve from 0.28493\n",
      "1710/1710 [==============================] - 2181s 1s/step - loss: 0.2514 - binary_accuracy: 0.8917 - val_loss: 0.2945 - val_binary_accuracy: 0.8857 - lr: 0.0100\n",
      "Epoch 14/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2467 - binary_accuracy: 0.8957\n",
      "Epoch 14: val_loss did not improve from 0.28493\n",
      "1710/1710 [==============================] - 2193s 1s/step - loss: 0.2467 - binary_accuracy: 0.8957 - val_loss: 0.2896 - val_binary_accuracy: 0.8877 - lr: 0.0100\n",
      "Epoch 15/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2397 - binary_accuracy: 0.8993\n",
      "Epoch 15: val_loss improved from 0.28493 to 0.25969, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 2201s 1s/step - loss: 0.2397 - binary_accuracy: 0.8993 - val_loss: 0.2597 - val_binary_accuracy: 0.8984 - lr: 0.0100\n",
      "Epoch 16/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2270 - binary_accuracy: 0.9047\n",
      "Epoch 16: val_loss did not improve from 0.25969\n",
      "1710/1710 [==============================] - 1460s 854ms/step - loss: 0.2270 - binary_accuracy: 0.9047 - val_loss: 0.2698 - val_binary_accuracy: 0.8945 - lr: 0.0100\n",
      "Epoch 17/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2191 - binary_accuracy: 0.9075\n",
      "Epoch 17: val_loss did not improve from 0.25969\n",
      "1710/1710 [==============================] - 587s 344ms/step - loss: 0.2191 - binary_accuracy: 0.9075 - val_loss: 0.2929 - val_binary_accuracy: 0.8863 - lr: 0.0100\n",
      "Epoch 18/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2194 - binary_accuracy: 0.9096\n",
      "Epoch 18: val_loss did not improve from 0.25969\n",
      "1710/1710 [==============================] - 586s 343ms/step - loss: 0.2194 - binary_accuracy: 0.9096 - val_loss: 0.2735 - val_binary_accuracy: 0.8953 - lr: 0.0100\n",
      "Epoch 19/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2140 - binary_accuracy: 0.9113\n",
      "Epoch 19: val_loss did not improve from 0.25969\n",
      "1710/1710 [==============================] - 590s 345ms/step - loss: 0.2140 - binary_accuracy: 0.9113 - val_loss: 0.2688 - val_binary_accuracy: 0.9000 - lr: 0.0100\n",
      "Epoch 20/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.2089 - binary_accuracy: 0.9133\n",
      "Epoch 20: val_loss did not improve from 0.25969\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "1710/1710 [==============================] - 591s 346ms/step - loss: 0.2089 - binary_accuracy: 0.9133 - val_loss: 0.3081 - val_binary_accuracy: 0.9021 - lr: 0.0100\n",
      "Epoch 21/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1543 - binary_accuracy: 0.9375\n",
      "Epoch 21: val_loss did not improve from 0.25969\n",
      "1710/1710 [==============================] - 591s 346ms/step - loss: 0.1543 - binary_accuracy: 0.9375 - val_loss: 0.2629 - val_binary_accuracy: 0.9110 - lr: 1.0000e-03\n",
      "Epoch 22/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1459 - binary_accuracy: 0.9416\n",
      "Epoch 22: val_loss improved from 0.25969 to 0.25436, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\EfficientNetB7\\EfficientNetB7_adaptive_lr001.h5\n",
      "1710/1710 [==============================] - 596s 349ms/step - loss: 0.1459 - binary_accuracy: 0.9416 - val_loss: 0.2544 - val_binary_accuracy: 0.9123 - lr: 1.0000e-03\n",
      "Epoch 23/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1416 - binary_accuracy: 0.9432\n",
      "Epoch 23: val_loss did not improve from 0.25436\n",
      "1710/1710 [==============================] - 590s 345ms/step - loss: 0.1416 - binary_accuracy: 0.9432 - val_loss: 0.2553 - val_binary_accuracy: 0.9112 - lr: 1.0000e-03\n",
      "Epoch 24/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1381 - binary_accuracy: 0.9450\n",
      "Epoch 24: val_loss did not improve from 0.25436\n",
      "1710/1710 [==============================] - 591s 346ms/step - loss: 0.1381 - binary_accuracy: 0.9450 - val_loss: 0.2602 - val_binary_accuracy: 0.9129 - lr: 1.0000e-03\n",
      "Epoch 25/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1352 - binary_accuracy: 0.9455\n",
      "Epoch 25: val_loss did not improve from 0.25436\n",
      "1710/1710 [==============================] - 591s 345ms/step - loss: 0.1352 - binary_accuracy: 0.9455 - val_loss: 0.2724 - val_binary_accuracy: 0.9147 - lr: 1.0000e-03\n",
      "Epoch 26/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1334 - binary_accuracy: 0.9469\n",
      "Epoch 26: val_loss did not improve from 0.25436\n",
      "1710/1710 [==============================] - 591s 346ms/step - loss: 0.1334 - binary_accuracy: 0.9469 - val_loss: 0.2708 - val_binary_accuracy: 0.9139 - lr: 1.0000e-03\n",
      "Epoch 27/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1311 - binary_accuracy: 0.9478\n",
      "Epoch 27: val_loss did not improve from 0.25436\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "1710/1710 [==============================] - 591s 346ms/step - loss: 0.1311 - binary_accuracy: 0.9478 - val_loss: 0.2698 - val_binary_accuracy: 0.9144 - lr: 1.0000e-03\n",
      "Epoch 28/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1250 - binary_accuracy: 0.9502\n",
      "Epoch 28: val_loss did not improve from 0.25436\n",
      "1710/1710 [==============================] - 592s 346ms/step - loss: 0.1250 - binary_accuracy: 0.9502 - val_loss: 0.2616 - val_binary_accuracy: 0.9188 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1236 - binary_accuracy: 0.9509\n",
      "Epoch 29: val_loss did not improve from 0.25436\n",
      "1710/1710 [==============================] - 590s 345ms/step - loss: 0.1236 - binary_accuracy: 0.9509 - val_loss: 0.2613 - val_binary_accuracy: 0.9182 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "1710/1710 [==============================] - ETA: 0s - loss: 0.1231 - binary_accuracy: 0.9516\n",
      "Epoch 30: val_loss did not improve from 0.25436\n",
      "1710/1710 [==============================] - 591s 346ms/step - loss: 0.1231 - binary_accuracy: 0.9516 - val_loss: 0.2624 - val_binary_accuracy: 0.9186 - lr: 1.0000e-04\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_test_data() missing 1 required positional argument: 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m results_base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKananat\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_result\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlr scheduler exp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m models_base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKananat\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_result\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlr scheduler exp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_base_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_base_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 46\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(model_names, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs, batch_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m plot_training_history(history, model_name, results_dir)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Test model\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_test_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m test_model(model_name, test_data, models_dir)\n",
      "\u001b[1;31mTypeError\u001b[0m: load_test_data() missing 1 required positional argument: 'model_name'"
     ]
    }
   ],
   "source": [
    "model_to_train = ['EfficientNetB7', 'EfficientNetV2L', 'InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "\n",
    "train_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_5px\"\n",
    "test_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_5px_test\"\n",
    "\n",
    "results_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\logs\"\n",
    "models_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\models\"\n",
    "\n",
    "train_models(model_to_train, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=30, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dmodelGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
