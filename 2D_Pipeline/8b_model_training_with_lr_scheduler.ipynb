{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(tf.__version__, np.__version__)\n",
    "\n",
    "# Expected output 2.9.0, 1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect GPU and limit memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU found and successfully configured\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus != []:\n",
    "    print(gpus)\n",
    "    for gpu in gpus: \n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPU on this machine\")\n",
    "\n",
    "# Expected output [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "if USE_GPU == False:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found and successfully configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_function(model_name):\n",
    "    \"\"\"Get the appropriate preprocessing function for each model\"\"\"\n",
    "    preprocess_dict = {\n",
    "        'DenseNet201': tf.keras.applications.densenet.preprocess_input,\n",
    "        'EfficientNetB7': tf.keras.applications.efficientnet.preprocess_input,\n",
    "        'EfficientNetV2L': tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "        'InceptionResNetV2': tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "        'InceptionV3': tf.keras.applications.inception_v3.preprocess_input,\n",
    "        'MobileNetV3Large': tf.keras.applications.mobilenet_v3.preprocess_input,\n",
    "        'NASNetLarge': tf.keras.applications.nasnet.preprocess_input,\n",
    "        'ResNet152': tf.keras.applications.resnet.preprocess_input,\n",
    "        'ResNet152V2': tf.keras.applications.resnet_v2.preprocess_input,\n",
    "        'VGG19': tf.keras.applications.vgg19.preprocess_input,\n",
    "        'Xception': tf.keras.applications.xception.preprocess_input\n",
    "    }\n",
    "    return preprocess_dict.get(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, preprocess_fn):\n",
    "    \"\"\"Apply preprocessing to a dataset\"\"\"\n",
    "    def preprocess(x, y):\n",
    "        return preprocess_fn(x), y\n",
    "    \n",
    "    return data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(train_data_dir, model_name, train_data_ratio=0.8, batch_size=8):\n",
    "    print(f\"Loading training data from {train_data_dir}...\")\n",
    "\n",
    "    train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        train_data = preprocess_data(train_data, preprocess_fn)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_size = int(len(train_data)*train_data_ratio)\n",
    "    val_size = int(len(train_data)*(1-train_data_ratio))\n",
    "    train = train_data.take(train_size)\n",
    "    val = train_data.skip(train_size).take(val_size)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    train = train.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val = val.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "def load_test_data(test_data_dir, model_name, batch_size=8):\n",
    "    print(f\"Loading testing data from {test_data_dir}...\")\n",
    "\n",
    "    test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        test_data = preprocess_data(test_data, preprocess_fn)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    test_data = test_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = ['DesnseNet201']\n",
    "INPUT_SHAPE = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_model = ['DenseNet201', 'EfficientNetB7', 'EfficientNetV2L','InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'NASNetLarge', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "\n",
    "def get_base_model(model_name, input_shape):\n",
    "    model_dict = {\n",
    "        'DenseNet201': tf.keras.applications.DenseNet201,\n",
    "        'EfficientNetB7': tf.keras.applications.EfficientNetB7,\n",
    "        'EfficientNetV2L': tf.keras.applications.EfficientNetV2L,\n",
    "        'InceptionResNetV2': tf.keras.applications.InceptionResNetV2,\n",
    "        'InceptionV3': tf.keras.applications.InceptionV3,\n",
    "        'MobileNetV3Large': tf.keras.applications.MobileNetV3Large,\n",
    "        'NASNetLarge': tf.keras.applications.NASNetLarge,\n",
    "        'ResNet152': tf.keras.applications.ResNet152,\n",
    "        'ResNet152V2': tf.keras.applications.ResNet152V2,\n",
    "        'VGG19': tf.keras.applications.VGG19,\n",
    "        'Xception': tf.keras.applications.Xception,\n",
    "    }\n",
    "    \n",
    "    if model_name not in model_dict:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    return model_dict[model_name](input_shape=input_shape, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, input_shape):\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    print(\"Built model\")\n",
    "    pd.set_option('max_colwidth', None)\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "    pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "\n",
    "# input_shape = (224, 224, 3)\n",
    "# base_model = get_base_model(model_name, input_shape)\n",
    "# model = build_model(base_model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name, results_dir, models_dir, initial_lr=\"001\"):\n",
    "    log_file = os.path.join(results_dir, f\"{model_name}_adaptive_lr{initial_lr}.csv\")\n",
    "    \n",
    "    return [\n",
    "        CSVLogger(log_file),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(models_dir, f\"{model_name}_adaptive_lr{initial_lr}.h5\"),\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            save_freq='epoch',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',        # Monitors validation loss\n",
    "            factor=0.5,               # Multiplies learning rate by 0.5 (reduces by 50%)\n",
    "            patience=5,               # Waits 5 epochs before reducing\n",
    "            verbose=1,                # Prints message when reducing LR\n",
    "            min_delta=1e-3,          # Minimum change in loss to be considered an improvement\n",
    "            min_lr=1e-4              # Won't reduce LR below this value\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, results_dir, lr_suffix=\"001\"):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "    plt.title(f'{model_name} Loss (LR: 0.{lr_suffix})')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"{model_name}_loss_lr{lr_suffix}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, test_data, models_dir, results_dir):\n",
    "    \"\"\"\n",
    "    Test model and save metrics to CSV\n",
    "    \n",
    "    Parameters:\n",
    "        model_name: Name of the model\n",
    "        test_data: Test dataset\n",
    "        models_dir: Directory containing model files\n",
    "        results_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(os.path.join(models_dir, f\"{model_name}_adaptive_lr001.h5\"))\n",
    "    \n",
    "    # Initialize metrics\n",
    "    precision = Precision()\n",
    "    recall = Recall()  # Same as Sensitivity\n",
    "    accuracy = BinaryAccuracy()\n",
    "    \n",
    "    # Lists to store predictions and true labels for specificity calculation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Evaluate model on test data\n",
    "    for batch in test_data.as_numpy_iterator():\n",
    "        X, y = batch\n",
    "        yhat = model.predict(X,verbose = 0)\n",
    "        \n",
    "        # Update metrics\n",
    "        precision.update_state(y, yhat)\n",
    "        recall.update_state(y, yhat)\n",
    "        accuracy.update_state(y, yhat)\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend([1 if pred >= 0.5 else 0 for pred in yhat])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_value = float(precision.result())\n",
    "    recall_value = float(recall.result())  # Sensitivity\n",
    "    accuracy_value = float(accuracy.result())\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    f1_score = 2 * (precision_value * recall_value) / (precision_value + recall_value)\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    # True Negatives / (True Negatives + False Positives)\n",
    "    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n",
    "    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_value,\n",
    "        'Precision': precision_value,\n",
    "        'Recall (Sensitivity)': recall_value,\n",
    "        'Specificity': specificity,\n",
    "        'F1 Score': f1_score\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Test Results:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Model':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    csv_file = os.path.join(results_dir, 'model_metrics.csv')\n",
    "    \n",
    "    # Check if file exists to determine if we need to write headers\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metrics.keys())\n",
    "        \n",
    "        # Write headers if file doesn't exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerow(metrics)\n",
    "    \n",
    "    print(f\"\\nMetrics saved to: {csv_file}\")\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_names, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=100, batch_size=32):\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"Training {model_name}...\")\n",
    "\n",
    "        train, val = load_training_data(train_data_dir, model_name, train_data_ratio=0.8, batch_size=batch_size)\n",
    "\n",
    "        # Set up model-specific directories\n",
    "        results_dir = os.path.join(results_base_dir, model_name)\n",
    "        models_dir = os.path.join(models_base_dir, model_name)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Build model\n",
    "        input_shape = (224, 224, 3)\n",
    "        base_model = get_base_model(model_name, input_shape)\n",
    "        model = build_model(base_model, input_shape)\n",
    "\n",
    "        # Compile model with initial learning rate\n",
    "        initial_lr = 1e-2\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "            metrics=[keras.metrics.BinaryAccuracy()]\n",
    "        )\n",
    "\n",
    "        # Print model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Set up callbacks with adaptive learning rate and early stopping\n",
    "        callbacks = get_callbacks(model_name, results_dir, models_dir)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train,\n",
    "            validation_data=val,\n",
    "            epochs=max_epochs,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history, model_name, results_dir)\n",
    "\n",
    "        # Test model\n",
    "        test_data = load_test_data(test_data_dir, model_name, batch_size=1)\n",
    "        test_model(model_name, test_data, models_dir, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Xception...\n",
      "Loading training data from D:\\Kananat\\TF_TMJOA_jpg_x_5px...\n",
      "Found 34203 files belonging to 2 classes.\n",
      "Built model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " xception (Functional)       (None, 7, 7, 2048)        20861480  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,484,969\n",
      "Trainable params: 2,623,489\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.5681 - binary_accuracy: 0.7321\n",
      "Epoch 1: val_loss improved from inf to 0.49381, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 226s 65ms/step - loss: 0.5681 - binary_accuracy: 0.7321 - val_loss: 0.4938 - val_binary_accuracy: 0.7680 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4973 - binary_accuracy: 0.7673\n",
      "Epoch 2: val_loss did not improve from 0.49381\n",
      "3420/3420 [==============================] - 223s 65ms/step - loss: 0.4973 - binary_accuracy: 0.7673 - val_loss: 0.5015 - val_binary_accuracy: 0.7624 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4781 - binary_accuracy: 0.7806\n",
      "Epoch 3: val_loss did not improve from 0.49381\n",
      "3420/3420 [==============================] - 227s 66ms/step - loss: 0.4781 - binary_accuracy: 0.7806 - val_loss: 0.5081 - val_binary_accuracy: 0.7654 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4709 - binary_accuracy: 0.7854\n",
      "Epoch 4: val_loss improved from 0.49381 to 0.46339, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 230s 67ms/step - loss: 0.4709 - binary_accuracy: 0.7854 - val_loss: 0.4634 - val_binary_accuracy: 0.7876 - lr: 0.0100\n",
      "Epoch 5/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4643 - binary_accuracy: 0.7880\n",
      "Epoch 5: val_loss did not improve from 0.46339\n",
      "3420/3420 [==============================] - 229s 67ms/step - loss: 0.4643 - binary_accuracy: 0.7880 - val_loss: 0.4648 - val_binary_accuracy: 0.7887 - lr: 0.0100\n",
      "Epoch 6/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4552 - binary_accuracy: 0.7929\n",
      "Epoch 6: val_loss improved from 0.46339 to 0.45148, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 231s 68ms/step - loss: 0.4552 - binary_accuracy: 0.7929 - val_loss: 0.4515 - val_binary_accuracy: 0.7939 - lr: 0.0100\n",
      "Epoch 7/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4459 - binary_accuracy: 0.7972\n",
      "Epoch 7: val_loss improved from 0.45148 to 0.44899, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 230s 67ms/step - loss: 0.4459 - binary_accuracy: 0.7972 - val_loss: 0.4490 - val_binary_accuracy: 0.7956 - lr: 0.0100\n",
      "Epoch 8/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4533 - binary_accuracy: 0.7931\n",
      "Epoch 8: val_loss did not improve from 0.44899\n",
      "3420/3420 [==============================] - 228s 67ms/step - loss: 0.4533 - binary_accuracy: 0.7931 - val_loss: 0.4538 - val_binary_accuracy: 0.7921 - lr: 0.0100\n",
      "Epoch 9/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4412 - binary_accuracy: 0.8015\n",
      "Epoch 9: val_loss did not improve from 0.44899\n",
      "3420/3420 [==============================] - 228s 67ms/step - loss: 0.4412 - binary_accuracy: 0.8015 - val_loss: 0.4714 - val_binary_accuracy: 0.7705 - lr: 0.0100\n",
      "Epoch 10/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4357 - binary_accuracy: 0.8040\n",
      "Epoch 10: val_loss did not improve from 0.44899\n",
      "3420/3420 [==============================] - 230s 67ms/step - loss: 0.4357 - binary_accuracy: 0.8040 - val_loss: 0.4561 - val_binary_accuracy: 0.7873 - lr: 0.0100\n",
      "Epoch 11/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4308 - binary_accuracy: 0.8085\n",
      "Epoch 11: val_loss improved from 0.44899 to 0.43194, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 230s 67ms/step - loss: 0.4308 - binary_accuracy: 0.8085 - val_loss: 0.4319 - val_binary_accuracy: 0.8050 - lr: 0.0100\n",
      "Epoch 12/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4259 - binary_accuracy: 0.8107\n",
      "Epoch 12: val_loss improved from 0.43194 to 0.42853, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 230s 67ms/step - loss: 0.4259 - binary_accuracy: 0.8107 - val_loss: 0.4285 - val_binary_accuracy: 0.8070 - lr: 0.0100\n",
      "Epoch 13/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4260 - binary_accuracy: 0.8103\n",
      "Epoch 13: val_loss did not improve from 0.42853\n",
      "3420/3420 [==============================] - 230s 67ms/step - loss: 0.4260 - binary_accuracy: 0.8103 - val_loss: 0.4522 - val_binary_accuracy: 0.7899 - lr: 0.0100\n",
      "Epoch 14/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4161 - binary_accuracy: 0.8163\n",
      "Epoch 14: val_loss did not improve from 0.42853\n",
      "3420/3420 [==============================] - 284s 83ms/step - loss: 0.4161 - binary_accuracy: 0.8163 - val_loss: 0.4399 - val_binary_accuracy: 0.7933 - lr: 0.0100\n",
      "Epoch 15/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4163 - binary_accuracy: 0.8153\n",
      "Epoch 15: val_loss did not improve from 0.42853\n",
      "3420/3420 [==============================] - 306s 90ms/step - loss: 0.4163 - binary_accuracy: 0.8153 - val_loss: 0.4414 - val_binary_accuracy: 0.7965 - lr: 0.0100\n",
      "Epoch 16/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4220 - binary_accuracy: 0.8114\n",
      "Epoch 16: val_loss did not improve from 0.42853\n",
      "3420/3420 [==============================] - 305s 89ms/step - loss: 0.4220 - binary_accuracy: 0.8114 - val_loss: 0.4353 - val_binary_accuracy: 0.7988 - lr: 0.0100\n",
      "Epoch 17/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.4167 - binary_accuracy: 0.8162\n",
      "Epoch 17: val_loss did not improve from 0.42853\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "3420/3420 [==============================] - 295s 86ms/step - loss: 0.4167 - binary_accuracy: 0.8162 - val_loss: 0.4317 - val_binary_accuracy: 0.8060 - lr: 0.0100\n",
      "Epoch 18/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3873 - binary_accuracy: 0.8326\n",
      "Epoch 18: val_loss improved from 0.42853 to 0.42057, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 295s 86ms/step - loss: 0.3873 - binary_accuracy: 0.8326 - val_loss: 0.4206 - val_binary_accuracy: 0.8114 - lr: 0.0050\n",
      "Epoch 19/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3859 - binary_accuracy: 0.8342\n",
      "Epoch 19: val_loss did not improve from 0.42057\n",
      "3420/3420 [==============================] - 294s 86ms/step - loss: 0.3859 - binary_accuracy: 0.8342 - val_loss: 0.4396 - val_binary_accuracy: 0.7946 - lr: 0.0050\n",
      "Epoch 20/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3849 - binary_accuracy: 0.8333\n",
      "Epoch 20: val_loss improved from 0.42057 to 0.41579, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 295s 86ms/step - loss: 0.3849 - binary_accuracy: 0.8333 - val_loss: 0.4158 - val_binary_accuracy: 0.8165 - lr: 0.0050\n",
      "Epoch 21/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3814 - binary_accuracy: 0.8358\n",
      "Epoch 21: val_loss did not improve from 0.41579\n",
      "3420/3420 [==============================] - 294s 86ms/step - loss: 0.3814 - binary_accuracy: 0.8358 - val_loss: 0.4318 - val_binary_accuracy: 0.8051 - lr: 0.0050\n",
      "Epoch 22/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3821 - binary_accuracy: 0.8366\n",
      "Epoch 22: val_loss did not improve from 0.41579\n",
      "3420/3420 [==============================] - 294s 86ms/step - loss: 0.3821 - binary_accuracy: 0.8366 - val_loss: 0.4224 - val_binary_accuracy: 0.8127 - lr: 0.0050\n",
      "Epoch 23/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3791 - binary_accuracy: 0.8379\n",
      "Epoch 23: val_loss did not improve from 0.41579\n",
      "3420/3420 [==============================] - 294s 86ms/step - loss: 0.3791 - binary_accuracy: 0.8379 - val_loss: 0.4342 - val_binary_accuracy: 0.8070 - lr: 0.0050\n",
      "Epoch 24/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3757 - binary_accuracy: 0.8392\n",
      "Epoch 24: val_loss did not improve from 0.41579\n",
      "3420/3420 [==============================] - 294s 86ms/step - loss: 0.3757 - binary_accuracy: 0.8392 - val_loss: 0.4183 - val_binary_accuracy: 0.8124 - lr: 0.0050\n",
      "Epoch 25/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3772 - binary_accuracy: 0.8387\n",
      "Epoch 25: val_loss did not improve from 0.41579\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "3420/3420 [==============================] - 294s 86ms/step - loss: 0.3772 - binary_accuracy: 0.8387 - val_loss: 0.4165 - val_binary_accuracy: 0.8126 - lr: 0.0050\n",
      "Epoch 26/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3669 - binary_accuracy: 0.8438\n",
      "Epoch 26: val_loss did not improve from 0.41579\n",
      "3420/3420 [==============================] - 294s 86ms/step - loss: 0.3669 - binary_accuracy: 0.8438 - val_loss: 0.4161 - val_binary_accuracy: 0.8164 - lr: 0.0025\n",
      "Epoch 27/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3632 - binary_accuracy: 0.8461\n",
      "Epoch 27: val_loss improved from 0.41579 to 0.41340, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 295s 86ms/step - loss: 0.3632 - binary_accuracy: 0.8461 - val_loss: 0.4134 - val_binary_accuracy: 0.8196 - lr: 0.0025\n",
      "Epoch 28/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3627 - binary_accuracy: 0.8466\n",
      "Epoch 28: val_loss improved from 0.41340 to 0.41170, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 295s 86ms/step - loss: 0.3627 - binary_accuracy: 0.8466 - val_loss: 0.4117 - val_binary_accuracy: 0.8184 - lr: 0.0025\n",
      "Epoch 29/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3613 - binary_accuracy: 0.8472\n",
      "Epoch 29: val_loss improved from 0.41170 to 0.40955, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\Xception\\Xception_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 295s 86ms/step - loss: 0.3613 - binary_accuracy: 0.8472 - val_loss: 0.4096 - val_binary_accuracy: 0.8205 - lr: 0.0025\n",
      "Epoch 30/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.3603 - binary_accuracy: 0.8493\n",
      "Epoch 30: val_loss did not improve from 0.40955\n",
      "3420/3420 [==============================] - 294s 86ms/step - loss: 0.3603 - binary_accuracy: 0.8493 - val_loss: 0.4104 - val_binary_accuracy: 0.8189 - lr: 0.0025\n",
      "Loading testing data from D:\\Kananat\\TF_TMJOA_jpg_x_5px_test...\n",
      "Found 7205 files belonging to 2 classes.\n",
      "\n",
      "Xception Test Results:\n",
      "Accuracy: 0.7742\n",
      "Precision: 0.7858\n",
      "Recall (Sensitivity): 0.7387\n",
      "Specificity: 0.8080\n",
      "F1 Score: 0.7615\n",
      "\n",
      "Metrics saved to: D:\\Kananat\\_result\\lr scheduler exp\\logs\\Xception\\model_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# all_model = ['DenseNet201', 'EfficientNetB7', 'EfficientNetV2L','InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "# input shape error 'NASNetLarge' need input shape of exact dimensions (331, 331, 3)\n",
    "\n",
    "model_to_train =  ['Xception']\n",
    "\n",
    "train_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_5px\"\n",
    "test_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_5px_test\"\n",
    "\n",
    "results_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\logs\"\n",
    "models_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\models\"\n",
    "\n",
    "train_models(model_to_train, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=50, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'EfficientNetB7'\n",
    "# test_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_5px_test\"\n",
    "\n",
    "# models_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\models\"\n",
    "# models_dir = os.path.join(models_base_dir, model_name)\n",
    "\n",
    "# results_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\logs\"\n",
    "# results_dir = os.path.join(results_base_dir, model_name)\n",
    "\n",
    "# test_data = load_test_data(test_data_dir, model_name, batch_size=1)\n",
    "# test_model(model_name, test_data, models_dir, results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dmodelGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
