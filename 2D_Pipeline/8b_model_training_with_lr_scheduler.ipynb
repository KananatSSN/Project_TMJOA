{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(tf.__version__, np.__version__)\n",
    "\n",
    "# Expected output 2.9.0, 1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect GPU and limit memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus != []:\n",
    "    print(gpus)\n",
    "    for gpu in gpus: \n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPU on this machine\")\n",
    "\n",
    "# Expected output [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "if USE_GPU == False:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found and successfully configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_function(model_name):\n",
    "    \"\"\"Get the appropriate preprocessing function for each model\"\"\"\n",
    "    preprocess_dict = {\n",
    "        'DenseNet201': tf.keras.applications.densenet.preprocess_input,\n",
    "        'EfficientNetB7': tf.keras.applications.efficientnet.preprocess_input,\n",
    "        'EfficientNetV2L': tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "        'InceptionResNetV2': tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "        'InceptionV3': tf.keras.applications.inception_v3.preprocess_input,\n",
    "        'MobileNetV3Large': tf.keras.applications.mobilenet_v3.preprocess_input,\n",
    "        'NASNetLarge': tf.keras.applications.nasnet.preprocess_input,\n",
    "        'ResNet152': tf.keras.applications.resnet.preprocess_input,\n",
    "        'ResNet152V2': tf.keras.applications.resnet_v2.preprocess_input,\n",
    "        'VGG19': tf.keras.applications.vgg19.preprocess_input,\n",
    "        'Xception': tf.keras.applications.xception.preprocess_input\n",
    "    }\n",
    "    return preprocess_dict.get(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, preprocess_fn):\n",
    "    \"\"\"Apply preprocessing to a dataset\"\"\"\n",
    "    def preprocess(x, y):\n",
    "        return preprocess_fn(x), y\n",
    "    \n",
    "    return data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(train_data_dir, model_name, train_data_ratio=0.8, batch_size=8):\n",
    "    print(f\"Loading training data from {train_data_dir}...\")\n",
    "\n",
    "    train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        train_data = preprocess_data(train_data, preprocess_fn)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_size = int(len(train_data)*train_data_ratio)\n",
    "    val_size = int(len(train_data)*(1-train_data_ratio))\n",
    "    train = train_data.take(train_size)\n",
    "    val = train_data.skip(train_size).take(val_size)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    train = train.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val = val.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "def load_test_data(test_data_dir, model_name, batch_size=8):\n",
    "    print(f\"Loading testing data from {test_data_dir}...\")\n",
    "\n",
    "    test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        test_data = preprocess_data(test_data, preprocess_fn)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    test_data = test_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = ['DesnseNet201']\n",
    "INPUT_SHAPE = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_model = ['DenseNet201', 'EfficientNetB7', 'EfficientNetV2L','InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'NASNetLarge', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "\n",
    "def get_base_model(model_name, input_shape):\n",
    "    model_dict = {\n",
    "        'DenseNet201': tf.keras.applications.DenseNet201,\n",
    "        'EfficientNetB7': tf.keras.applications.EfficientNetB7,\n",
    "        'EfficientNetV2L': tf.keras.applications.EfficientNetV2L,\n",
    "        'InceptionResNetV2': tf.keras.applications.InceptionResNetV2,\n",
    "        'InceptionV3': tf.keras.applications.InceptionV3,\n",
    "        'MobileNetV3Large': tf.keras.applications.MobileNetV3Large,\n",
    "        'NASNetLarge': tf.keras.applications.NASNetLarge,\n",
    "        'ResNet152': tf.keras.applications.ResNet152,\n",
    "        'ResNet152V2': tf.keras.applications.ResNet152V2,\n",
    "        'VGG19': tf.keras.applications.VGG19,\n",
    "        'Xception': tf.keras.applications.Xception,\n",
    "    }\n",
    "    \n",
    "    if model_name not in model_dict:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    return model_dict[model_name](input_shape=input_shape, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, input_shape):\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    print(\"Built model\")\n",
    "    pd.set_option('max_colwidth', None)\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "    pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "\n",
    "# input_shape = (224, 224, 3)\n",
    "# base_model = get_base_model(model_name, input_shape)\n",
    "# model = build_model(base_model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name, results_dir, models_dir, initial_lr=\"001\"):\n",
    "    log_file = os.path.join(results_dir, f\"{model_name}_adaptive_lr{initial_lr}.csv\")\n",
    "    \n",
    "    return [\n",
    "        CSVLogger(log_file),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(models_dir, f\"{model_name}_adaptive_lr{initial_lr}.h5\"),\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            save_freq='epoch',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',        # Monitors validation loss\n",
    "            factor=0.1,               # Multiplies learning rate by 0.1 (reduces by 90%)\n",
    "            patience=5,               # Waits 3 epochs before reducing\n",
    "            verbose=1,                # Prints message when reducing LR\n",
    "            min_delta=1e-4,          # Minimum change in loss to be considered an improvement\n",
    "            min_lr=1e-6              # Won't reduce LR below this value\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, results_dir, lr_suffix=\"001\"):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "    plt.title(f'{model_name} Loss (LR: 0.{lr_suffix})')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"{model_name}_loss_lr{lr_suffix}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, test_data, models_dir):\n",
    "    model = tf.keras.models.load_model(os.path.join(models_dir, f\"{model_name}_bo20_lr0001.h5\"))\n",
    "    pre = Precision()\n",
    "    re = Recall()\n",
    "    acc = BinaryAccuracy()\n",
    "\n",
    "    for batch in test_data.as_numpy_iterator():\n",
    "        X, y = batch\n",
    "        yhat = model.predict(X)\n",
    "        pre.update_state(y, yhat)\n",
    "        re.update_state(y, yhat)\n",
    "        acc.update_state(y, yhat)\n",
    "\n",
    "    f1_score = 2 * (pre.result() * re.result()) / (pre.result() + re.result())\n",
    "    print(f\"{model_name} Test Results:\")\n",
    "    print(f\"Precision: {pre.result():.4f}\")\n",
    "    print(f\"Recall: {re.result():.4f}\")\n",
    "    print(f\"Accuracy: {acc.result():.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_names, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=100, batch_size=32):\n",
    "    \n",
    "    train, val = load_training_data(train_data_dir, train_data_ratio=0.8, batch_size=batch_size)\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"Training {model_name}...\")\n",
    "\n",
    "        # Set up model-specific directories\n",
    "        results_dir = os.path.join(results_base_dir, model_name)\n",
    "        models_dir = os.path.join(models_base_dir, model_name)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Build model\n",
    "        input_shape = (224, 224, 3)\n",
    "        base_model = get_base_model(model_name, input_shape)\n",
    "        model = build_model(base_model, input_shape)\n",
    "\n",
    "        # Compile model with initial learning rate\n",
    "        initial_lr = 1e-2\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "            metrics=[keras.metrics.BinaryAccuracy()]\n",
    "        )\n",
    "\n",
    "        # Print model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Set up callbacks with adaptive learning rate and early stopping\n",
    "        callbacks = get_callbacks(model_name, results_dir, models_dir)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train,\n",
    "            validation_data=val,\n",
    "            epochs=max_epochs,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history, model_name, results_dir)\n",
    "\n",
    "        # Test model\n",
    "        test_data = load_test_data(test_data_dir, batch_size=1)\n",
    "        test_model(model_name, test_data, models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = ['EfficientNetB7', 'EfficientNetV2L', 'InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "\n",
    "train_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_10px\"\n",
    "test_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_10px_test\"\n",
    "\n",
    "results_base_dir = r\"D:\\Kananat\\_result\\logs\"\n",
    "models_base_dir = r\"D:\\Kananat\\_result\\models\"\n",
    "\n",
    "train_models(model_to_train, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=100, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dmodelGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
