{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(tf.__version__, np.__version__)\n",
    "\n",
    "# Expected output 2.9.0, 1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect GPU and limit memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU found and successfully configured\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus != []:\n",
    "    print(gpus)\n",
    "    for gpu in gpus: \n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPU on this machine\")\n",
    "\n",
    "# Expected output [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "if USE_GPU == False:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found and successfully configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_function(model_name):\n",
    "    \"\"\"Get the appropriate preprocessing function for each model\"\"\"\n",
    "    preprocess_dict = {\n",
    "        'DenseNet201': tf.keras.applications.densenet.preprocess_input,\n",
    "        'EfficientNetB7': tf.keras.applications.efficientnet.preprocess_input,\n",
    "        'EfficientNetV2L': tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "        'InceptionResNetV2': tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "        'InceptionV3': tf.keras.applications.inception_v3.preprocess_input,\n",
    "        'MobileNetV3Large': tf.keras.applications.mobilenet_v3.preprocess_input,\n",
    "        'NASNetLarge': tf.keras.applications.nasnet.preprocess_input,\n",
    "        'ResNet152': tf.keras.applications.resnet.preprocess_input,\n",
    "        'ResNet152V2': tf.keras.applications.resnet_v2.preprocess_input,\n",
    "        'VGG19': tf.keras.applications.vgg19.preprocess_input,\n",
    "        'Xception': tf.keras.applications.xception.preprocess_input\n",
    "    }\n",
    "    return preprocess_dict.get(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, preprocess_fn):\n",
    "    \"\"\"Apply preprocessing to a dataset\"\"\"\n",
    "    def preprocess(x, y):\n",
    "        return preprocess_fn(x), y\n",
    "    \n",
    "    return data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(dataset_dir, model_name, train_data_ratio=0.8, batch_size=8):\n",
    "\n",
    "    print(f\"Loading training data from {dataset_dir}...\")\n",
    "\n",
    "    train_data_dir = os.path.join(dataset_dir, \"train\")\n",
    "\n",
    "    train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_data_dir = os.path.join(dataset_dir, \"val\")\n",
    "\n",
    "    val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        train_data = preprocess_data(train_data, preprocess_fn)\n",
    "        val_data = preprocess_data(val_data, preprocess_fn)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    train = train_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val = val_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "def load_test_data(test_data_dir, model_name, batch_size=8):\n",
    "    print(f\"Loading testing data from {test_data_dir}...\")\n",
    "\n",
    "    test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        test_data = preprocess_data(test_data, preprocess_fn)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    test_data = test_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = ['DesnseNet201']\n",
    "INPUT_SHAPE = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_model = ['DenseNet201', 'EfficientNetB7', 'EfficientNetV2L','InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'NASNetLarge', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "\n",
    "def get_base_model(model_name, input_shape):\n",
    "    model_dict = {\n",
    "        'DenseNet201': tf.keras.applications.DenseNet201,\n",
    "        'EfficientNetB7': tf.keras.applications.EfficientNetB7,\n",
    "        'EfficientNetV2L': tf.keras.applications.EfficientNetV2L,\n",
    "        'InceptionResNetV2': tf.keras.applications.InceptionResNetV2,\n",
    "        'InceptionV3': tf.keras.applications.InceptionV3,\n",
    "        'MobileNetV3Large': tf.keras.applications.MobileNetV3Large,\n",
    "        'NASNetLarge': tf.keras.applications.NASNetLarge,\n",
    "        'ResNet152': tf.keras.applications.ResNet152,\n",
    "        'ResNet152V2': tf.keras.applications.ResNet152V2,\n",
    "        'VGG19': tf.keras.applications.VGG19,\n",
    "        'Xception': tf.keras.applications.Xception,\n",
    "    }\n",
    "    \n",
    "    if model_name not in model_dict:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    return model_dict[model_name](input_shape=input_shape, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, input_shape):\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    print(\"Built model\")\n",
    "    pd.set_option('max_colwidth', None)\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "    pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "\n",
    "# input_shape = (224, 224, 3)\n",
    "# base_model = get_base_model(model_name, input_shape)\n",
    "# model = build_model(base_model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name, results_dir, models_dir, initial_lr=\"001\"):\n",
    "    log_file = os.path.join(results_dir, f\"{model_name}_adaptive_lr{initial_lr}.csv\")\n",
    "    \n",
    "    return [\n",
    "        CSVLogger(log_file),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(models_dir, f\"{model_name}_adaptive_lr{initial_lr}.h5\"),\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            save_freq='epoch',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',        # Monitors validation loss\n",
    "            factor=0.8,               # Multiplies learning rate by 0.8\n",
    "            patience=5,               # Waits 10 epochs before reducing\n",
    "            verbose=1,                # Prints message when reducing LR\n",
    "            min_delta=1e-3,          # Minimum change in loss to be considered an improvement\n",
    "            min_lr=1e-5              # Won't reduce LR below this value\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, results_dir, lr_suffix=\"001\"):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "    plt.title(f'{model_name} Loss (LR: 0.{lr_suffix})')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"{model_name}_loss_lr{lr_suffix}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, test_data, models_dir, results_dir):\n",
    "    \"\"\"\n",
    "    Test model and save metrics to CSV\n",
    "    \n",
    "    Parameters:\n",
    "        model_name: Name of the model\n",
    "        test_data: Test dataset\n",
    "        models_dir: Directory containing model files\n",
    "        results_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(os.path.join(models_dir, f\"{model_name}_adaptive_lr001.h5\"))\n",
    "    \n",
    "    # Initialize metrics\n",
    "    precision = Precision()\n",
    "    recall = Recall()  # Same as Sensitivity\n",
    "    accuracy = BinaryAccuracy()\n",
    "    \n",
    "    # Lists to store predictions and true labels for specificity calculation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Evaluate model on test data\n",
    "    for batch in test_data.as_numpy_iterator():\n",
    "        X, y = batch\n",
    "        yhat = model.predict(X,verbose = 0)\n",
    "        \n",
    "        # Update metrics\n",
    "        precision.update_state(y, yhat)\n",
    "        recall.update_state(y, yhat)\n",
    "        accuracy.update_state(y, yhat)\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend([1 if pred >= 0.5 else 0 for pred in yhat])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_value = float(precision.result())\n",
    "    recall_value = float(recall.result())  # Sensitivity\n",
    "    accuracy_value = float(accuracy.result())\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    f1_score = 2 * (precision_value * recall_value) / (precision_value + recall_value)\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    # True Negatives / (True Negatives + False Positives)\n",
    "    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n",
    "    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_value,\n",
    "        'Precision': precision_value,\n",
    "        'Recall (Sensitivity)': recall_value,\n",
    "        'Specificity': specificity,\n",
    "        'F1 Score': f1_score\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Test Results:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Model':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    \n",
    "    csv_file = os.path.join(os.path.dirname(results_dir), 'model_metrics.csv')\n",
    "    \n",
    "    # Check if file exists to determine if we need to write headers\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metrics.keys())\n",
    "        \n",
    "        # Write headers if file doesn't exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerow(metrics)\n",
    "    \n",
    "    print(f\"\\nMetrics saved to: {csv_file}\")\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_names, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=100, batch_size=32):\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"Training {model_name}...\")\n",
    "\n",
    "        train, val = load_training_data(train_data_dir, model_name, train_data_ratio=0.8, batch_size=batch_size)\n",
    "\n",
    "        # Set up model-specific directories\n",
    "        results_dir = os.path.join(results_base_dir, model_name)\n",
    "        models_dir = models_base_dir\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Build model\n",
    "        input_shape = (224, 224, 3)\n",
    "        base_model = get_base_model(model_name, input_shape)\n",
    "        model = build_model(base_model, input_shape)\n",
    "\n",
    "        # Compile model with initial learning rate\n",
    "        initial_lr = 1e-2\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "            metrics=[keras.metrics.BinaryAccuracy()]\n",
    "        )\n",
    "\n",
    "        # Print model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Set up callbacks with adaptive learning rate and early stopping\n",
    "        callbacks = get_callbacks(model_name, results_dir, models_dir)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train,\n",
    "            validation_data=val,\n",
    "            epochs=max_epochs,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history, model_name, results_dir)\n",
    "\n",
    "        # Test model\n",
    "        test_data = load_test_data(test_data_dir, model_name, batch_size=1)\n",
    "        test_model(model_name, test_data, models_dir, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DenseNet201...\n",
      "Loading training data from D:\\Kananat\\_dataset_2d...\n",
      "Found 15075 files belonging to 2 classes.\n",
      "Found 844 files belonging to 2 classes.\n",
      "Built model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " densenet201 (Functional)    (None, 7, 7, 1920)        18321984  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1920)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1967104   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,814,401\n",
      "Trainable params: 2,492,417\n",
      "Non-trainable params: 18,321,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1885/1885 [==============================] - ETA: 0s - loss: 0.5920 - binary_accuracy: 0.7291\n",
      "Epoch 1: val_loss improved from inf to 0.57378, saving model to D:\\Kananat\\_result\\probabilistic_encoding\\models\\DenseNet201_adaptive_lr001.h5\n",
      "1885/1885 [==============================] - 144s 71ms/step - loss: 0.5920 - binary_accuracy: 0.7291 - val_loss: 0.5738 - val_binary_accuracy: 0.6754 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "1884/1885 [============================>.] - ETA: 0s - loss: 0.4747 - binary_accuracy: 0.7742\n",
      "Epoch 2: val_loss improved from 0.57378 to 0.55162, saving model to D:\\Kananat\\_result\\probabilistic_encoding\\models\\DenseNet201_adaptive_lr001.h5\n",
      "1885/1885 [==============================] - 130s 69ms/step - loss: 0.4747 - binary_accuracy: 0.7741 - val_loss: 0.5516 - val_binary_accuracy: 0.6754 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "1884/1885 [============================>.] - ETA: 0s - loss: 0.4494 - binary_accuracy: 0.7881\n",
      "Epoch 3: val_loss did not improve from 0.55162\n",
      "1885/1885 [==============================] - 131s 69ms/step - loss: 0.4494 - binary_accuracy: 0.7881 - val_loss: 0.5935 - val_binary_accuracy: 0.6848 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "1884/1885 [============================>.] - ETA: 0s - loss: 0.4367 - binary_accuracy: 0.7947\n",
      "Epoch 4: val_loss did not improve from 0.55162\n",
      "1885/1885 [==============================] - 132s 70ms/step - loss: 0.4368 - binary_accuracy: 0.7946 - val_loss: 0.5908 - val_binary_accuracy: 0.6919 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "1885/1885 [==============================] - ETA: 0s - loss: 0.4251 - binary_accuracy: 0.7988\n",
      "Epoch 5: val_loss did not improve from 0.55162\n",
      "1885/1885 [==============================] - 135s 71ms/step - loss: 0.4251 - binary_accuracy: 0.7988 - val_loss: 0.5968 - val_binary_accuracy: 0.6884 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "1884/1885 [============================>.] - ETA: 0s - loss: 0.4155 - binary_accuracy: 0.8041\n",
      "Epoch 6: val_loss did not improve from 0.55162\n",
      "1885/1885 [==============================] - 128s 68ms/step - loss: 0.4155 - binary_accuracy: 0.8041 - val_loss: 0.5905 - val_binary_accuracy: 0.6836 - lr: 0.0100\n",
      "Epoch 7/200\n",
      " 210/1885 [==>...........................] - ETA: 1:50 - loss: 0.3821 - binary_accuracy: 0.8304"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m results_base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKananat\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_result\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprobabilistic_encoding\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m models_base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKananat\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_result\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprobabilistic_encoding\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_base_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_base_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(model_names, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs, batch_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m get_callbacks(model_name, results_dir, models_dir)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[0;32m     43\u001b[0m plot_training_history(history, model_name, results_dir)\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\kanan\\miniconda3\\envs\\2dmodelGPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all_model = ['DenseNet201', 'EfficientNetB7', 'EfficientNetV2L','InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "# input shape error 'NASNetLarge' need input shape of exact dimensions (331, 331, 3)\n",
    "\n",
    "model_to_train =  ['DenseNet201', 'EfficientNetB7', 'EfficientNetV2L','InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "\n",
    "train_data_dir = r\"D:\\Kananat\\_dataset_2d\"\n",
    "test_data_dir = r\"D:\\Kananat\\_dataset_2d\\test\"\n",
    "\n",
    "results_base_dir = r\"D:\\Kananat\\_result\\probabilistic_encoding\\logs\"\n",
    "models_base_dir = r\"D:\\Kananat\\_result\\probabilistic_encoding\\models\"\n",
    "\n",
    "train_models(model_to_train, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=200, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dmodelGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
