{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(tf.__version__, np.__version__)\n",
    "\n",
    "# Expected output 2.9.0, 1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect GPU and limit memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU found and successfully configured\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus != []:\n",
    "    print(gpus)\n",
    "    for gpu in gpus: \n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPU on this machine\")\n",
    "\n",
    "# Expected output [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "if USE_GPU == False:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found and successfully configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_function(model_name):\n",
    "    \"\"\"Get the appropriate preprocessing function for each model\"\"\"\n",
    "    preprocess_dict = {\n",
    "        'DenseNet201': tf.keras.applications.densenet.preprocess_input,\n",
    "        'EfficientNetB7': tf.keras.applications.efficientnet.preprocess_input,\n",
    "        'EfficientNetV2L': tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "        'InceptionResNetV2': tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "        'InceptionV3': tf.keras.applications.inception_v3.preprocess_input,\n",
    "        'MobileNetV3Large': tf.keras.applications.mobilenet_v3.preprocess_input,\n",
    "        'NASNetLarge': tf.keras.applications.nasnet.preprocess_input,\n",
    "        'ResNet152': tf.keras.applications.resnet.preprocess_input,\n",
    "        'ResNet152V2': tf.keras.applications.resnet_v2.preprocess_input,\n",
    "        'VGG19': tf.keras.applications.vgg19.preprocess_input,\n",
    "        'Xception': tf.keras.applications.xception.preprocess_input\n",
    "    }\n",
    "    return preprocess_dict.get(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, preprocess_fn):\n",
    "    \"\"\"Apply preprocessing to a dataset\"\"\"\n",
    "    def preprocess(x, y):\n",
    "        return preprocess_fn(x), y\n",
    "    \n",
    "    return data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(train_data_dir, model_name, train_data_ratio=0.8, batch_size=8):\n",
    "    print(f\"Loading training data from {train_data_dir}...\")\n",
    "\n",
    "    train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        train_data = preprocess_data(train_data, preprocess_fn)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_size = int(len(train_data)*train_data_ratio)\n",
    "    val_size = int(len(train_data)*(1-train_data_ratio))\n",
    "    train = train_data.take(train_size)\n",
    "    val = train_data.skip(train_size).take(val_size)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    train = train.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val = val.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "def load_test_data(test_data_dir, model_name, batch_size=8):\n",
    "    print(f\"Loading testing data from {test_data_dir}...\")\n",
    "\n",
    "    test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_data_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Get preprocessing function for the model\n",
    "    preprocess_fn = get_preprocessing_function(model_name)\n",
    "    if preprocess_fn is not None:\n",
    "        test_data = preprocess_data(test_data, preprocess_fn)\n",
    "\n",
    "    # Use caching and prefetching for better performance\n",
    "    test_data = test_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = ['DesnseNet201']\n",
    "INPUT_SHAPE = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_model = ['DenseNet201', 'EfficientNetB7', 'EfficientNetV2L','InceptionResNetV2', 'InceptionV3', 'MobileNetV3Large', 'NASNetLarge', 'ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "\n",
    "def get_base_model(model_name, input_shape):\n",
    "    model_dict = {\n",
    "        'DenseNet201': tf.keras.applications.DenseNet201,\n",
    "        'EfficientNetB7': tf.keras.applications.EfficientNetB7,\n",
    "        'EfficientNetV2L': tf.keras.applications.EfficientNetV2L,\n",
    "        'InceptionResNetV2': tf.keras.applications.InceptionResNetV2,\n",
    "        'InceptionV3': tf.keras.applications.InceptionV3,\n",
    "        'MobileNetV3Large': tf.keras.applications.MobileNetV3Large,\n",
    "        'NASNetLarge': tf.keras.applications.NASNetLarge,\n",
    "        'ResNet152': tf.keras.applications.ResNet152,\n",
    "        'ResNet152V2': tf.keras.applications.ResNet152V2,\n",
    "        'VGG19': tf.keras.applications.VGG19,\n",
    "        'Xception': tf.keras.applications.Xception,\n",
    "    }\n",
    "    \n",
    "    if model_name not in model_dict:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    return model_dict[model_name](input_shape=input_shape, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, input_shape):\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    print(\"Built model\")\n",
    "    pd.set_option('max_colwidth', None)\n",
    "    layers = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "    pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "\n",
    "# input_shape = (224, 224, 3)\n",
    "# base_model = get_base_model(model_name, input_shape)\n",
    "# model = build_model(base_model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name, results_dir, models_dir, initial_lr=\"001\"):\n",
    "    log_file = os.path.join(results_dir, f\"{model_name}_adaptive_lr{initial_lr}.csv\")\n",
    "    \n",
    "    return [\n",
    "        CSVLogger(log_file),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(models_dir, f\"{model_name}_adaptive_lr{initial_lr}.h5\"),\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            save_freq='epoch',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',        # Monitors validation loss\n",
    "            factor=0.1,               # Multiplies learning rate by 0.1 (reduces by 90%)\n",
    "            patience=5,               # Waits 3 epochs before reducing\n",
    "            verbose=1,                # Prints message when reducing LR\n",
    "            min_delta=1e-4,          # Minimum change in loss to be considered an improvement\n",
    "            min_lr=1e-4              # Won't reduce LR below this value\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, results_dir, lr_suffix=\"001\"):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "    plt.title(f'{model_name} Loss (LR: 0.{lr_suffix})')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"{model_name}_loss_lr{lr_suffix}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, test_data, models_dir, results_dir):\n",
    "    \"\"\"\n",
    "    Test model and save metrics to CSV\n",
    "    \n",
    "    Parameters:\n",
    "        model_name: Name of the model\n",
    "        test_data: Test dataset\n",
    "        models_dir: Directory containing model files\n",
    "        results_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(os.path.join(models_dir, f\"{model_name}_adaptive_lr001.h5\"))\n",
    "    \n",
    "    # Initialize metrics\n",
    "    precision = Precision()\n",
    "    recall = Recall()  # Same as Sensitivity\n",
    "    accuracy = BinaryAccuracy()\n",
    "    \n",
    "    # Lists to store predictions and true labels for specificity calculation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Evaluate model on test data\n",
    "    for batch in test_data.as_numpy_iterator():\n",
    "        X, y = batch\n",
    "        yhat = model.predict(X)\n",
    "        \n",
    "        # Update metrics\n",
    "        precision.update_state(y, yhat)\n",
    "        recall.update_state(y, yhat)\n",
    "        accuracy.update_state(y, yhat)\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend([1 if pred >= 0.5 else 0 for pred in yhat])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_value = float(precision.result())\n",
    "    recall_value = float(recall.result())  # Sensitivity\n",
    "    accuracy_value = float(accuracy.result())\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    f1_score = 2 * (precision_value * recall_value) / (precision_value + recall_value)\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    # True Negatives / (True Negatives + False Positives)\n",
    "    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n",
    "    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_value,\n",
    "        'Precision': precision_value,\n",
    "        'Recall (Sensitivity)': recall_value,\n",
    "        'Specificity': specificity,\n",
    "        'F1 Score': f1_score\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Test Results:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Model':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    csv_file = os.path.join(results_dir, 'model_metrics.csv')\n",
    "    \n",
    "    # Check if file exists to determine if we need to write headers\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metrics.keys())\n",
    "        \n",
    "        # Write headers if file doesn't exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerow(metrics)\n",
    "    \n",
    "    print(f\"\\nMetrics saved to: {csv_file}\")\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_names, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=100, batch_size=32):\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"Training {model_name}...\")\n",
    "\n",
    "        train, val = load_training_data(train_data_dir, model_name, train_data_ratio=0.8, batch_size=batch_size)\n",
    "\n",
    "        # Set up model-specific directories\n",
    "        results_dir = os.path.join(results_base_dir, model_name)\n",
    "        models_dir = os.path.join(models_base_dir, model_name)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Build model\n",
    "        input_shape = (224, 224, 3)\n",
    "        base_model = get_base_model(model_name, input_shape)\n",
    "        model = build_model(base_model, input_shape)\n",
    "\n",
    "        # Compile model with initial learning rate\n",
    "        initial_lr = 1e-2\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "            metrics=[keras.metrics.BinaryAccuracy()]\n",
    "        )\n",
    "\n",
    "        # Print model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Set up callbacks with adaptive learning rate and early stopping\n",
    "        callbacks = get_callbacks(model_name, results_dir, models_dir)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train,\n",
    "            validation_data=val,\n",
    "            epochs=max_epochs,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history, model_name, results_dir)\n",
    "\n",
    "        # Test model\n",
    "        test_data = load_test_data(test_data_dir, model_name, batch_size=1)\n",
    "        test_model(model_name, test_data, models_dir, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet152...\n",
      "Loading training data from D:\\Kananat\\TF_TMJOA_jpg_x_5px...\n",
      "Found 34203 files belonging to 2 classes.\n",
      "Built model\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " resnet152 (Functional)      (None, 7, 7, 2048)        58370944  \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,994,433\n",
      "Trainable params: 2,623,489\n",
      "Non-trainable params: 58,370,944\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.6360 - binary_accuracy: 0.7215\n",
      "Epoch 1: val_loss improved from inf to 0.67791, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\ResNet152\\ResNet152_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 734s 212ms/step - loss: 0.6360 - binary_accuracy: 0.7215 - val_loss: 0.6779 - val_binary_accuracy: 0.5314 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.6017 - binary_accuracy: 0.6748\n",
      "Epoch 2: val_loss improved from 0.67791 to 0.55126, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\ResNet152\\ResNet152_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 1270s 372ms/step - loss: 0.6017 - binary_accuracy: 0.6748 - val_loss: 0.5513 - val_binary_accuracy: 0.7349 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "3420/3420 [==============================] - ETA: 0s - loss: 0.5305 - binary_accuracy: 0.7548\n",
      "Epoch 3: val_loss improved from 0.55126 to 0.53544, saving model to D:\\Kananat\\_result\\lr scheduler exp\\models\\ResNet152\\ResNet152_adaptive_lr001.h5\n",
      "3420/3420 [==============================] - 1270s 371ms/step - loss: 0.5305 - binary_accuracy: 0.7548 - val_loss: 0.5354 - val_binary_accuracy: 0.7596 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "2711/3420 [======================>.......] - ETA: 3:13 - loss: 0.5213 - binary_accuracy: 0.7625"
     ]
    }
   ],
   "source": [
    "model_to_train =  ['ResNet152', 'ResNet152V2', 'VGG19', 'Xception']\n",
    "# input shape error 'NASNetLarge' need input shape of exact dimensions (331, 331, 3)\n",
    "train_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_5px\"\n",
    "test_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_5px_test\"\n",
    "\n",
    "results_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\logs\"\n",
    "models_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\models\"\n",
    "\n",
    "train_models(model_to_train, train_data_dir, test_data_dir, results_base_dir, models_base_dir, max_epochs=30, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'EfficientNetB7'\n",
    "# test_data_dir = r\"D:\\Kananat\\TF_TMJOA_jpg_x_5px_test\"\n",
    "\n",
    "# models_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\models\"\n",
    "# models_dir = os.path.join(models_base_dir, model_name)\n",
    "\n",
    "# results_base_dir = r\"D:\\Kananat\\_result\\lr scheduler exp\\logs\"\n",
    "# results_dir = os.path.join(results_base_dir, model_name)\n",
    "\n",
    "# test_data = load_test_data(test_data_dir, model_name, batch_size=1)\n",
    "# test_model(model_name, test_data, models_dir, results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dmodelGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
