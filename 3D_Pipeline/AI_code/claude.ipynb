{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c035dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Dependencies and Setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Medical imaging libraries\n",
    "try:\n",
    "    import torchio as tio\n",
    "    print(\"TorchIO available for medical augmentations\")\n",
    "except ImportError:\n",
    "    print(\"TorchIO not found. Install with: pip install torchio\")\n",
    "    tio = None\n",
    "\n",
    "try:\n",
    "    import monai\n",
    "    from monai.networks.nets import ResNet\n",
    "    from monai.transforms import (\n",
    "        Compose, LoadImage, ScaleIntensity, RandRotate, RandFlip,\n",
    "        RandGaussianNoise, RandBiasField, Rand3DElastic\n",
    "    )\n",
    "    print(\"MONAI available for medical networks\")\n",
    "except ImportError:\n",
    "    print(\"MONAI not found. Install with: pip install monai\")\n",
    "    monai = None\n",
    "\n",
    "# Set device and random seeds for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_path': r\"D:\\Kananat\\Data\\training_dataset\",  # Update this to your data path\n",
    "    'batch_size': 4,  # Small batch size for 3D volumes\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 100,\n",
    "    'early_stopping_patience': 20,\n",
    "    'num_folds': 5,\n",
    "    'input_size': (254, 254, 254),  # Adjust based on your data\n",
    "    'num_classes': 2,\n",
    "    'weight_decay': 5e-5,\n",
    "    'dropout_rate': 0.3,\n",
    "    'ensemble_size': 5\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Input size: {CONFIG['input_size']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Advanced Regularization Classes\n",
    "\n",
    "class DropBlock3D(nn.Module):\n",
    "    \"\"\"3D DropBlock for spatial regularization in medical volumes\"\"\"\n",
    "    def __init__(self, drop_rate=0.1, block_size=5):\n",
    "        super(DropBlock3D, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "            \n",
    "        # Calculate gamma (probability to set seed)\n",
    "        gamma = self.drop_rate / (self.block_size ** 3)\n",
    "        \n",
    "        # Generate mask\n",
    "        mask_shape = (x.shape[0], x.shape[1], \n",
    "                     x.shape[2] - self.block_size + 1,\n",
    "                     x.shape[3] - self.block_size + 1,\n",
    "                     x.shape[4] - self.block_size + 1)\n",
    "        \n",
    "        mask = torch.bernoulli(torch.full(mask_shape, gamma, device=x.device))\n",
    "        \n",
    "        # Expand mask to block size\n",
    "        mask = F.pad(mask, [self.block_size//2] * 6, value=0)\n",
    "        mask = F.max_pool3d(mask, kernel_size=self.block_size, \n",
    "                           stride=1, padding=self.block_size//2)\n",
    "        \n",
    "        # Apply mask\n",
    "        mask = 1 - mask\n",
    "        normalize_factor = mask.numel() / mask.sum()\n",
    "        \n",
    "        return x * mask * normalize_factor\n",
    "\n",
    "class SpatialDropout3D(nn.Module):\n",
    "    \"\"\"3D Spatial Dropout for feature map regularization\"\"\"\n",
    "    def __init__(self, p=0.2):\n",
    "        super(SpatialDropout3D, self).__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "            \n",
    "        # Generate noise for each channel\n",
    "        noise = torch.bernoulli(torch.full((x.size(0), x.size(1), 1, 1, 1), \n",
    "                                         1 - self.p, device=x.device))\n",
    "        return x * noise / (1 - self.p)\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"Stochastic depth for very deep 3D networks\"\"\"\n",
    "    def __init__(self, survival_prob=0.8):\n",
    "        super(StochasticDepth, self).__init__()\n",
    "        self.survival_prob = survival_prob\n",
    "        \n",
    "    def forward(self, x, residual):\n",
    "        if not self.training:\n",
    "            return x + residual\n",
    "            \n",
    "        if torch.rand(1).item() < self.survival_prob:\n",
    "            return x + residual\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Unified Focal Loss for class imbalance\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class LayerNorm3D(nn.Module):\n",
    "    \"\"\"3D Layer Normalization for small batch sizes\"\"\"\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super(LayerNorm3D, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, depth, height, width)\n",
    "        mean = x.mean(dim=(2, 3, 4), keepdim=True)\n",
    "        var = x.var(dim=(2, 3, 4), keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Reshape weight and bias for broadcasting\n",
    "        weight = self.weight.view(1, -1, 1, 1, 1)\n",
    "        bias = self.bias.view(1, -1, 1, 1, 1)\n",
    "        \n",
    "        return x_norm * weight + bias\n",
    "\n",
    "print(\"Advanced regularization classes defined successfully\")\n",
    "print(\"- DropBlock3D: Spatial regularization preserving 3D structure\")\n",
    "print(\"- SpatialDropout3D: Channel-wise dropout for feature maps\")\n",
    "print(\"- StochasticDepth: Random layer skipping for very deep networks\")\n",
    "print(\"- FocalLoss: Handles class imbalance in medical data\")\n",
    "print(\"- LayerNorm3D: Stable normalization for small batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: TMJ-Specific Augmentations\n",
    "\n",
    "class TMJAugmentations:\n",
    "    \"\"\"TMJ-specific augmentation pipeline preserving anatomical relationships\"\"\"\n",
    "    \n",
    "    def __init__(self, training=True):\n",
    "        self.training = training\n",
    "        \n",
    "        if tio is not None:\n",
    "            # TorchIO augmentations (preferred for medical imaging)\n",
    "            self.train_transforms = tio.Compose([\n",
    "                # Spatial transforms with conservative parameters\n",
    "                tio.RandomAffine(\n",
    "                    scales=(0.95, 1.05),  # Physiological scaling\n",
    "                    degrees=(-8, 8),      # Conservative rotation for TMJ\n",
    "                    translation=(-4, 4),   # 4mm max translation\n",
    "                    p=0.8\n",
    "                ),\n",
    "                \n",
    "                # Elastic deformation simulating natural variation\n",
    "                tio.RandomElasticDeformation(\n",
    "                    num_control_points=(5, 5, 5),\n",
    "                    max_displacement=(3, 3, 3),  # 3mm max displacement\n",
    "                    p=0.6\n",
    "                ),\n",
    "                \n",
    "                # Intensity augmentations for scanner variation\n",
    "                tio.RandomGamma(log_gamma=(-0.2, 0.2), p=0.5),\n",
    "                tio.RandomNoise(std=(0, 0.05), p=0.4),\n",
    "                tio.RandomBiasField(coefficients=0.3, p=0.3),\n",
    "                \n",
    "                # Flip with anatomical consideration\n",
    "                tio.RandomFlip(axes=(0,), p=0.3),  # Sagittal flip only\n",
    "                \n",
    "                # Normalize intensity\n",
    "                tio.RescaleIntensity(out_min_max=(0, 1))\n",
    "            ])\n",
    "            \n",
    "            self.val_transforms = tio.Compose([\n",
    "                tio.RescaleIntensity(out_min_max=(0, 1))\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            # Fallback to custom augmentations\n",
    "            print(\"Using custom augmentations (TorchIO not available)\")\n",
    "            \n",
    "    def __call__(self, volume):\n",
    "        if tio is not None:\n",
    "            # Convert to tensor if needed and ensure correct shape\n",
    "            if isinstance(volume, np.ndarray):\n",
    "                volume_tensor = torch.from_numpy(volume).float()\n",
    "            else:\n",
    "                volume_tensor = volume.float()\n",
    "            \n",
    "            # Ensure 4D tensor (1, D, H, W) for TorchIO\n",
    "            if volume_tensor.ndim == 3:\n",
    "                volume_tensor = volume_tensor.unsqueeze(0)  # Add channel dimension\n",
    "            elif volume_tensor.ndim == 5:\n",
    "                volume_tensor = volume_tensor.squeeze(0)    # Remove extra dimension\n",
    "            \n",
    "            # Create TorchIO Subject\n",
    "            subject = tio.Subject(\n",
    "                image=tio.ScalarImage(tensor=volume_tensor)\n",
    "            )\n",
    "            \n",
    "            if self.training:\n",
    "                transformed = self.train_transforms(subject)\n",
    "            else:\n",
    "                transformed = self.val_transforms(subject)\n",
    "                \n",
    "            # Return as numpy array to maintain consistency\n",
    "            result = transformed.image.data.squeeze().numpy()\n",
    "            \n",
    "            # Ensure result has channel dimension for consistency\n",
    "            if result.ndim == 3:\n",
    "                result = np.expand_dims(result, axis=0)\n",
    "                \n",
    "            return result\n",
    "        else:\n",
    "            return self.custom_augment(volume)\n",
    "    \n",
    "    def custom_augment(self, volume):\n",
    "        \"\"\"Fallback custom augmentations\"\"\"\n",
    "        if not self.training:\n",
    "            return torch.from_numpy(volume) if isinstance(volume, np.ndarray) else volume\n",
    "            \n",
    "        # Convert to tensor if needed\n",
    "        if isinstance(volume, np.ndarray):\n",
    "            volume = torch.from_numpy(volume)\n",
    "            \n",
    "        # Simple rotation (around center)\n",
    "        if torch.rand(1) < 0.5:\n",
    "            angle = torch.rand(1) * 16 - 8  # Â±8 degrees\n",
    "            # Simplified rotation implementation\n",
    "            pass\n",
    "            \n",
    "        # Add noise\n",
    "        if torch.rand(1) < 0.3:\n",
    "            noise = torch.randn_like(volume) * 0.05\n",
    "            volume = volume + noise\n",
    "            \n",
    "        # Normalize\n",
    "        volume = (volume - volume.min()) / (volume.max() - volume.min() + 1e-8)\n",
    "        \n",
    "        return volume\n",
    "\n",
    "class MixUp3D:\n",
    "    \"\"\"3D MixUp augmentation for medical volumes\"\"\"\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "            \n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size).to(x.device)\n",
    "        \n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "        y_a, y_b = y, y[index]\n",
    "        \n",
    "        return mixed_x, y_a, y_b, lam\n",
    "\n",
    "class CutMix3D:\n",
    "    \"\"\"3D CutMix augmentation for medical volumes\"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size).to(x.device)\n",
    "        \n",
    "        # Generate random bounding box\n",
    "        D, H, W = x.size(2), x.size(3), x.size(4)\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_d = int(D * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        \n",
    "        # Random center point\n",
    "        cd = np.random.randint(D)\n",
    "        ch = np.random.randint(H)\n",
    "        cw = np.random.randint(W)\n",
    "        \n",
    "        bbd1 = np.clip(cd - cut_d // 2, 0, D)\n",
    "        bbd2 = np.clip(cd + cut_d // 2, 0, D)\n",
    "        bbh1 = np.clip(ch - cut_h // 2, 0, H)\n",
    "        bbh2 = np.clip(ch + cut_h // 2, 0, H)\n",
    "        bbw1 = np.clip(cw - cut_w // 2, 0, W)\n",
    "        bbw2 = np.clip(cw + cut_w // 2, 0, W)\n",
    "        \n",
    "        x[:, :, bbd1:bbd2, bbh1:bbh2, bbw1:bbw2] = x[index, :, bbd1:bbd2, bbh1:bbh2, bbw1:bbw2]\n",
    "        \n",
    "        # Adjust lambda to exactly match pixel ratio\n",
    "        lam = 1 - ((bbd2 - bbd1) * (bbh2 - bbh1) * (bbw2 - bbw1) / (D * H * W))\n",
    "        \n",
    "        y_a, y_b = y, y[index]\n",
    "        return x, y_a, y_b, lam\n",
    "\n",
    "# Initialize augmentation objects\n",
    "tmj_augment_train = TMJAugmentations(training=True)\n",
    "tmj_augment_val = TMJAugmentations(training=False)\n",
    "mixup = MixUp3D(alpha=0.2)\n",
    "cutmix = CutMix3D(alpha=1.0)\n",
    "\n",
    "print(\"TMJ-specific augmentations initialized:\")\n",
    "print(\"- Conservative rotations (Â±8Â°) and translations (Â±4mm)\")\n",
    "print(\"- Elastic deformation with 3mm max displacement\")\n",
    "print(\"- Scanner variation simulation (gamma, noise, bias field)\")\n",
    "print(\"- 3D MixUp and CutMix for advanced augmentation\")\n",
    "print(\"- Anatomically-aware transformations preserving TMJ structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Dataset and DataLoader\n",
    "\n",
    "class TMJDataset(Dataset):\n",
    "    \"\"\"TMJ 3D volume dataset for pre-split train/val/test data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, split='train', transform=None, target_size=(64, 64, 64)):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.split = split  # 'train', 'val', or 'test'\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Load data paths and labels from the specified split\n",
    "        self.data_list = []\n",
    "        self.labels = []\n",
    "        self.patient_ids = []\n",
    "        \n",
    "        # Path to the specific split folder\n",
    "        split_path = self.data_path / split\n",
    "        if not split_path.exists():\n",
    "            raise ValueError(f\"Split folder {split_path} does not exist!\")\n",
    "        \n",
    "        # Load Class 0 files\n",
    "        class0_path = split_path / '0'\n",
    "        if class0_path.exists():\n",
    "            for file_path in class0_path.glob('*'):\n",
    "                if self._is_valid_medical_file(file_path):\n",
    "                    self.data_list.append(file_path)\n",
    "                    self.labels.append(0)\n",
    "                    # Extract patient ID from filename (modify as needed)\n",
    "                    patient_id = self._extract_patient_id(file_path.name)\n",
    "                    self.patient_ids.append(patient_id)\n",
    "        \n",
    "        # Load Class 1 files\n",
    "        class1_path = split_path / '1'\n",
    "        if class1_path.exists():\n",
    "            for file_path in class1_path.glob('*'):\n",
    "                if self._is_valid_medical_file(file_path):\n",
    "                    self.data_list.append(file_path)\n",
    "                    self.labels.append(1)\n",
    "                    patient_id = self._extract_patient_id(file_path.name)\n",
    "                    self.patient_ids.append(patient_id)\n",
    "        \n",
    "        print(f\"Loaded {split} split with {len(self.data_list)} samples:\")\n",
    "        print(f\"  Class 0: {sum(1 for l in self.labels if l == 0)} samples\")\n",
    "        print(f\"  Class 1: {sum(1 for l in self.labels if l == 1)} samples\")\n",
    "        \n",
    "        if len(self.data_list) == 0:\n",
    "            print(f\"Warning: No valid files found in {split_path}\")\n",
    "            print(\"Expected structure:\")\n",
    "            print(f\"  {data_path}/\")\n",
    "            print(f\"    â”œâ”€â”€ train/\")\n",
    "            print(f\"    â”‚   â”œâ”€â”€ 0/  (class 0 files)\")\n",
    "            print(f\"    â”‚   â””â”€â”€ 1/  (class 1 files)\")\n",
    "            print(f\"    â”œâ”€â”€ val/\")\n",
    "            print(f\"    â”‚   â”œâ”€â”€ 0/  (class 0 files)\")\n",
    "            print(f\"    â”‚   â””â”€â”€ 1/  (class 1 files)\")\n",
    "            print(f\"    â””â”€â”€ test/\")\n",
    "            print(f\"        â”œâ”€â”€ 0/  (class 0 files)\")\n",
    "            print(f\"        â””â”€â”€ 1/  (class 1 files)\")\n",
    "        \n",
    "    def _is_valid_medical_file(self, file_path):\n",
    "        \"\"\"Check if file is a valid medical image format\"\"\"\n",
    "        valid_extensions = ['.nii', '.nii.gz', '.npy', '.npz', '.dcm']\n",
    "        return any(str(file_path).lower().endswith(ext) for ext in valid_extensions)\n",
    "    \n",
    "    def _extract_patient_id(self, filename):\n",
    "        \"\"\"Extract patient ID from filename - modify based on your naming convention\"\"\"\n",
    "        # Example: 'patient_001_scan.nii.gz' -> 'patient_001'\n",
    "        # Modify this based on your actual filename pattern\n",
    "        return filename.split('_')[0] if '_' in filename else filename.split('.')[0]\n",
    "    \n",
    "    def _load_volume(self, file_path):\n",
    "        \"\"\"Load 3D volume from various medical formats\"\"\"\n",
    "        file_path = str(file_path)\n",
    "        \n",
    "        if file_path.endswith(('.nii', '.nii.gz')):\n",
    "            # NIfTI format\n",
    "            img = nib.load(file_path)\n",
    "            volume = img.get_fdata()\n",
    "        elif file_path.endswith('.npy'):\n",
    "            # NumPy format\n",
    "            volume = np.load(file_path)\n",
    "        elif file_path.endswith('.npz'):\n",
    "            # Compressed NumPy format\n",
    "            data = np.load(file_path)\n",
    "            volume = data['volume'] if 'volume' in data.files else data[data.files[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "        \n",
    "        return volume.astype(np.float32)\n",
    "    \n",
    "    def _preprocess_volume(self, volume):\n",
    "        \"\"\"Preprocess volume to target size and normalize\"\"\"\n",
    "        # Handle different input shapes\n",
    "        if volume.ndim == 4:\n",
    "            volume = volume[..., 0]  # Take first channel if 4D\n",
    "        \n",
    "        # Resize to target size\n",
    "        volume = self._resize_volume(volume, self.target_size)\n",
    "        \n",
    "        # Normalize intensity\n",
    "        volume = self._normalize_intensity(volume)\n",
    "        \n",
    "        # Add channel dimension\n",
    "        volume = np.expand_dims(volume, axis=0)\n",
    "        \n",
    "        return volume\n",
    "    \n",
    "    def _resize_volume(self, volume, target_size):\n",
    "        \"\"\"Resize volume to target size using interpolation\"\"\"\n",
    "        from scipy.ndimage import zoom\n",
    "        \n",
    "        current_size = volume.shape\n",
    "        zoom_factors = [target_size[i] / current_size[i] for i in range(3)]\n",
    "        \n",
    "        # Use order=1 (linear) for medical images to preserve intensities\n",
    "        resized_volume = zoom(volume, zoom_factors, order=1)\n",
    "        \n",
    "        return resized_volume\n",
    "    \n",
    "    def _normalize_intensity(self, volume):\n",
    "        \"\"\"Normalize intensity values\"\"\"\n",
    "        # Clip outliers (optional - adjust percentiles as needed)\n",
    "        p1, p99 = np.percentile(volume, [1, 99])\n",
    "        volume = np.clip(volume, p1, p99)\n",
    "        \n",
    "        # Min-max normalization\n",
    "        volume_min, volume_max = volume.min(), volume.max()\n",
    "        if volume_max > volume_min:\n",
    "            volume = (volume - volume_min) / (volume_max - volume_min)\n",
    "        \n",
    "        return volume\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load volume\n",
    "        volume = self._load_volume(self.data_list[idx])\n",
    "        label = self.labels[idx]\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        \n",
    "        # Preprocess (this adds channel dimension)\n",
    "        volume = self._preprocess_volume(volume)\n",
    "        \n",
    "        # Apply transforms (handle potential shape changes)\n",
    "        if self.transform:\n",
    "            volume = self.transform(volume)\n",
    "        \n",
    "        # Convert to tensor and ensure correct shape\n",
    "        if not isinstance(volume, torch.Tensor):\n",
    "            volume = torch.from_numpy(volume).float()\n",
    "        \n",
    "        # Ensure volume has correct shape: (C, D, H, W)\n",
    "        if volume.ndim == 3:\n",
    "            volume = volume.unsqueeze(0)  # Add channel dimension if missing\n",
    "        elif volume.ndim == 5:\n",
    "            volume = volume.squeeze(0)    # Remove extra dimension if present\n",
    "        \n",
    "        return volume, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "def create_dataloaders(data_path, config, use_balanced_sampling=True):\n",
    "    \"\"\"Create train, validation, and test dataloaders for pre-split data\"\"\"\n",
    "    \n",
    "    # Create datasets for each split\n",
    "    train_dataset = TMJDataset(\n",
    "        data_path=data_path,\n",
    "        split='train',\n",
    "        transform=tmj_augment_train,\n",
    "        target_size=config['input_size']\n",
    "    )\n",
    "    \n",
    "    val_dataset = TMJDataset(\n",
    "        data_path=data_path,\n",
    "        split='val',\n",
    "        transform=tmj_augment_val,\n",
    "        target_size=config['input_size']\n",
    "    )\n",
    "    \n",
    "    test_dataset = TMJDataset(\n",
    "        data_path=data_path,\n",
    "        split='test',\n",
    "        transform=tmj_augment_val,\n",
    "        target_size=config['input_size']\n",
    "    )\n",
    "    \n",
    "    # Create balanced sampler for training if requested\n",
    "    train_sampler = None\n",
    "    if use_balanced_sampling and len(train_dataset) > 0:\n",
    "        train_sampler = get_balanced_sampler(train_dataset.labels)\n",
    "        print(\"Using balanced sampling for training data\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        sampler=train_sampler,\n",
    "        shuffle=(train_sampler is None),  # Don't shuffle if using sampler\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=True  # Drop last incomplete batch for stable training\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataLoaders created:\")\n",
    "    print(f\"  Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "    print(f\"  Val:   {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
    "    print(f\"  Test:  {len(test_dataset)} samples, {len(test_loader)} batches\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def get_balanced_sampler(labels):\n",
    "    \"\"\"Create balanced sampler for handling class imbalance\"\"\"\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "    \n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    return sampler\n",
    "\n",
    "def validate_data_structure(data_path):\n",
    "    \"\"\"Validate the expected data structure\"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    required_splits = ['train', 'val', 'test']\n",
    "    required_classes = ['0', '1']\n",
    "    \n",
    "    print(f\"Validating data structure at {data_path}...\")\n",
    "    \n",
    "    all_valid = True\n",
    "    for split in required_splits:\n",
    "        split_path = data_path / split\n",
    "        if not split_path.exists():\n",
    "            print(f\"âŒ Missing split folder: {split_path}\")\n",
    "            all_valid = False\n",
    "            continue\n",
    "            \n",
    "        for class_label in required_classes:\n",
    "            class_path = split_path / class_label\n",
    "            if not class_path.exists():\n",
    "                print(f\"âŒ Missing class folder: {class_path}\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                # Count files in this class\n",
    "                files = list(class_path.glob('*'))\n",
    "                valid_files = [f for f in files if any(str(f).lower().endswith(ext) \n",
    "                              for ext in ['.nii', '.nii.gz', '.npy', '.npz', '.dcm'])]\n",
    "                print(f\"âœ… {split}/{class_label}: {len(valid_files)} valid files\")\n",
    "    \n",
    "    if all_valid:\n",
    "        print(\"âœ… Data structure validation passed!\")\n",
    "    else:\n",
    "        print(\"âŒ Data structure validation failed!\")\n",
    "        print(\"\\nExpected structure:\")\n",
    "        print(\"data_path/\")\n",
    "        print(\"â”œâ”€â”€ train/\")\n",
    "        print(\"â”‚   â”œâ”€â”€ 0/  (class 0 training files)\")\n",
    "        print(\"â”‚   â””â”€â”€ 1/  (class 1 training files)\")\n",
    "        print(\"â”œâ”€â”€ val/\")\n",
    "        print(\"â”‚   â”œâ”€â”€ 0/  (class 0 validation files)\")\n",
    "        print(\"â”‚   â””â”€â”€ 1/  (class 1 validation files)\")\n",
    "        print(\"â””â”€â”€ test/\")\n",
    "        print(\"    â”œâ”€â”€ 0/  (class 0 test files)\")\n",
    "        print(\"    â””â”€â”€ 1/  (class 1 test files)\")\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "# Test dataset loading with new structure\n",
    "print(\"Testing dataset loading with pre-split data...\")\n",
    "try:\n",
    "    # Validate data structure first\n",
    "    if validate_data_structure(CONFIG['data_path']):\n",
    "        # Create all dataloaders\n",
    "        train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_dataloaders(\n",
    "            CONFIG['data_path'], \n",
    "            CONFIG, \n",
    "            use_balanced_sampling=True\n",
    "        )\n",
    "        \n",
    "        # Test loading a sample from each split\n",
    "        if len(train_dataset) > 0:\n",
    "            sample_volume, sample_label, sample_patient = train_dataset[0]\n",
    "            print(f\"\\nTrain sample shape: {sample_volume.shape}\")\n",
    "            print(f\"Train sample label: {sample_label}\")\n",
    "            print(f\"Train sample patient ID: {sample_patient}\")\n",
    "        \n",
    "        print(\"âœ… Dataset loading successful!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ Please fix data structure before proceeding\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating dataset: {e}\")\n",
    "    print(\"Please check your data path and file formats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528719fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: 3D ResNet Architecture with MedicalNet-style Pre-training\n",
    "\n",
    "class BasicBlock3D(nn.Module):\n",
    "    \"\"\"3D BasicBlock for ResNet\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, \n",
    "                 dropout_rate=0.1, use_dropblock=True):\n",
    "        super(BasicBlock3D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                              padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=1,\n",
    "                              padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Advanced regularization\n",
    "        self.dropout = SpatialDropout3D(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.dropblock = DropBlock3D(drop_rate=0.1, block_size=5) if use_dropblock else None\n",
    "        self.stochastic_depth = StochasticDepth(survival_prob=0.8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        if self.dropblock:\n",
    "            out = self.dropblock(out)\n",
    "            \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        # Apply stochastic depth\n",
    "        out = self.stochastic_depth(out, residual)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Bottleneck3D(nn.Module):\n",
    "    \"\"\"3D Bottleneck block for deeper ResNets\"\"\"\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
    "                 dropout_rate=0.1, use_dropblock=True):\n",
    "        super(Bottleneck3D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride,\n",
    "                              padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Advanced regularization\n",
    "        self.dropout = SpatialDropout3D(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.dropblock = DropBlock3D(drop_rate=0.1, block_size=5) if use_dropblock else None\n",
    "        self.stochastic_depth = StochasticDepth(survival_prob=0.8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        if self.dropblock:\n",
    "            out = self.dropblock(out)\n",
    "            \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        out = self.stochastic_depth(out, residual)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet3D(nn.Module):\n",
    "    \"\"\"3D ResNet for TMJ classification with medical-specific modifications\"\"\"\n",
    "    \n",
    "    def __init__(self, block, layers, num_classes=2, input_channels=1,\n",
    "                 dropout_rate=0.3, use_dropblock=True, use_layer_norm=False):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        \n",
    "        self.inplanes = 64\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_dropblock = use_dropblock\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        \n",
    "        # Initial convolution (larger kernel for 3D medical data)\n",
    "        self.conv1 = nn.Conv3d(input_channels, 64, kernel_size=7, stride=2, \n",
    "                              padding=3, bias=False)\n",
    "        \n",
    "        if use_layer_norm:\n",
    "            self.bn1 = LayerNorm3D(64)\n",
    "        else:\n",
    "            self.bn1 = nn.BatchNorm3d(64)\n",
    "            \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # ResNet layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        \n",
    "        # Classifier with dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if self.use_layer_norm:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(self.inplanes, planes * block.expansion,\n",
    "                             kernel_size=1, stride=stride, bias=False),\n",
    "                    LayerNorm3D(planes * block.expansion),\n",
    "                )\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(self.inplanes, planes * block.expansion,\n",
    "                             kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm3d(planes * block.expansion),\n",
    "                )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample,\n",
    "                           self.dropout_rate, self.use_dropblock))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        \n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, dropout_rate=self.dropout_rate,\n",
    "                               use_dropblock=self.use_dropblock))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using medical imaging best practices\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm3d, LayerNorm3D)):\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if hasattr(m, 'bias'):\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # ResNet layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features for ensemble or visualization\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        features = torch.flatten(x, 1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "def create_medical_resnet3d(arch='resnet18', num_classes=2, pretrained_path=None):\n",
    "    \"\"\"Create 3D ResNet optimized for medical imaging\"\"\"\n",
    "    \n",
    "    if arch == 'resnet18':\n",
    "        model = ResNet3D(BasicBlock3D, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    elif arch == 'resnet34':\n",
    "        model = ResNet3D(BasicBlock3D, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    elif arch == 'resnet50':\n",
    "        model = ResNet3D(Bottleneck3D, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    elif arch == 'resnet101':\n",
    "        model = ResNet3D(Bottleneck3D, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported architecture: {arch}\")\n",
    "    \n",
    "    # Load MedicalNet pre-trained weights if available\n",
    "    if pretrained_path and os.path.exists(pretrained_path):\n",
    "        print(f\"Loading pre-trained weights from {pretrained_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "            \n",
    "            # Handle different checkpoint formats\n",
    "            if 'state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['state_dict']\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "            \n",
    "            # Remove 'module.' prefix if present\n",
    "            new_state_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                if k.startswith('module.'):\n",
    "                    new_state_dict[k[7:]] = v\n",
    "                else:\n",
    "                    new_state_dict[k] = v\n",
    "            \n",
    "            # Load weights, ignoring classifier\n",
    "            model_dict = model.state_dict()\n",
    "            filtered_dict = {k: v for k, v in new_state_dict.items() \n",
    "                           if k in model_dict and v.size() == model_dict[k].size()}\n",
    "            \n",
    "            model_dict.update(filtered_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "            \n",
    "            print(f\"Loaded {len(filtered_dict)} pre-trained layers\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pre-trained weights: {e}\")\n",
    "            print(\"Training from scratch...\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "print(\"Creating 3D ResNet model...\")\n",
    "model = create_medical_resnet3d(\n",
    "    arch='resnet18',  # Start with ResNet-18 for smaller datasets\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    pretrained_path=None  # Add path to MedicalNet weights if available\n",
    ")\n",
    "\n",
    "print(f\"Model created: {model.__class__.__name__}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Test model with sample input\n",
    "try:\n",
    "    test_input = torch.randn(1, 1, *CONFIG['input_size'])\n",
    "    with torch.no_grad():\n",
    "        test_output = model(test_input)\n",
    "    print(f\"Model test successful - Output shape: {test_output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model test failed: {e}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Training and Validation Functions\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=20, min_delta=0.001, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Loss function for MixUp\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# Block 6: Training and Validation Functions with Enhanced Logging\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class CSVLogger:\n",
    "    \"\"\"CSV logger for training metrics\"\"\"\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "        self.fieldnames = ['epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc', 'val_auc', 'lr', 'best_val_acc']\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "        \n",
    "        # Initialize CSV file with headers\n",
    "        with open(self.log_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n",
    "            writer.writeheader()\n",
    "    \n",
    "    def log_epoch(self, epoch, train_loss, train_acc, val_loss, val_acc, val_auc, lr, best_val_acc):\n",
    "        \"\"\"Log metrics for one epoch\"\"\"\n",
    "        with open(self.log_path, 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n",
    "            writer.writerow({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': f'{train_loss:.6f}',\n",
    "                'train_acc': f'{train_acc:.6f}',\n",
    "                'val_loss': f'{val_loss:.6f}',\n",
    "                'val_acc': f'{val_acc:.6f}',\n",
    "                'val_auc': f'{val_auc:.6f}',\n",
    "                'lr': f'{lr:.8f}',\n",
    "                'best_val_acc': f'{best_val_acc:.6f}'\n",
    "            })\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Enhanced model checkpoint saving\"\"\"\n",
    "    def __init__(self, save_dir, model_name='model'):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        self.model_name = model_name\n",
    "        self.best_val_acc = -1.0\n",
    "        \n",
    "    def save_checkpoint(self, model, optimizer, scheduler, epoch, metrics, is_best=False, is_last=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics': metrics,\n",
    "            'best_val_acc': self.best_val_acc\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            best_path = self.save_dir / f'{self.model_name}_best.pt'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"ðŸ’¾ Saved best model: {best_path}\")\n",
    "            \n",
    "        if is_last:\n",
    "            last_path = self.save_dir / f'{self.model_name}_last.pt'\n",
    "            torch.save(checkpoint, last_path)\n",
    "            print(f\"ðŸ’¾ Saved last model: {last_path}\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=20, min_delta=0.001, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Loss function for MixUp\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch, use_mixup=True, use_cutmix=True):\n",
    "    \"\"\"Train for one epoch with progress bar and memory optimization\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch:3d} [Train]', \n",
    "                leave=False, ncols=100, ascii=True)\n",
    "    \n",
    "    for batch_idx, (inputs, targets, _) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Clear cache every few batches to prevent memory buildup\n",
    "        if batch_idx % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Random choice between normal training, MixUp, and CutMix\n",
    "        r = np.random.rand(1)\n",
    "        if use_mixup and r < 0.3:\n",
    "            # MixUp augmentation\n",
    "            inputs, targets_a, targets_b, lam = mixup(inputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "        elif use_cutmix and r < 0.6:\n",
    "            # CutMix augmentation\n",
    "            inputs, targets_a, targets_b, lam = cutmix(inputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "        else:\n",
    "            # Normal training\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        if r >= 0.6:  # Only count predictions for normal training\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == targets.data)\n",
    "            total_samples += targets.size(0)\n",
    "        else:\n",
    "            total_samples += targets.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_loss = running_loss / ((batch_idx + 1) * dataloader.batch_size)\n",
    "        current_acc = (running_corrects.double() / total_samples).item() if total_samples > 0 else 0.0\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{current_loss:.4f}',\n",
    "            'Acc': f'{current_acc:.4f}',\n",
    "            'GPU': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB' if torch.cuda.is_available() else 'CPU'\n",
    "        })\n",
    "        \n",
    "        # Delete intermediate variables to free memory\n",
    "        del outputs, loss\n",
    "        if 'targets_a' in locals():\n",
    "            del targets_a, targets_b\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if batch_idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    # Final cache clear\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device, epoch):\n",
    "    \"\"\"Validate for one epoch with progress bar\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch:3d} [Val]  ', \n",
    "                leave=False, ncols=100, ascii=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, _) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == targets.data)\n",
    "            \n",
    "            # Store predictions for metrics\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_loss = running_loss / ((batch_idx + 1) * dataloader.batch_size)\n",
    "            current_acc = (running_corrects.double() / ((batch_idx + 1) * dataloader.batch_size)).item()\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{current_loss:.4f}',\n",
    "                'Acc': f'{current_acc:.4f}'\n",
    "            })\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    \n",
    "    # Calculate AUC if binary classification\n",
    "    try:\n",
    "        if len(np.unique(all_labels)) == 2:\n",
    "            auc = roc_auc_score(all_labels, [p[1] for p in all_probs])\n",
    "        else:\n",
    "            auc = 0.0\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item(), auc, all_preds, all_labels, all_probs\n",
    "\n",
    "def print_epoch_results(epoch, num_epochs, train_loss, train_acc, val_loss, val_acc, val_auc, \n",
    "                       lr, best_val_acc, time_elapsed):\n",
    "    \"\"\"Print detailed epoch results\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EPOCH {epoch+1:3d}/{num_epochs} RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"â±ï¸  Time: {time_elapsed:.1f}s\")\n",
    "    print(f\"ðŸ“š Learning Rate: {lr:.8f}\")\n",
    "    print(f\"\")\n",
    "    print(f\"ðŸ‹ï¸  TRAINING   â†’ Loss: {train_loss:.6f} | Acc: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"âœ… VALIDATION â†’ Loss: {val_loss:.6f} | Acc: {val_acc:.4f} ({val_acc*100:.2f}%) | AUC: {val_auc:.4f}\")\n",
    "    print(f\"ðŸ† BEST VAL   â†’ Acc: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ðŸ”§ GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.max_memory_allocated()/1024**3:.2f}GB peak\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "def train_model_with_validation(model, train_loader, val_loader, criterion, optimizer, \n",
    "                               scheduler, num_epochs, device, fold=None, save_dir=None):\n",
    "    \"\"\"Complete training loop with enhanced logging and checkpointing\"\"\"\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Setup logging and checkpointing\n",
    "    if save_dir is None:\n",
    "        save_dir = Path(\"tmj_results\") / \"checkpoints\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Model name for saving\n",
    "    model_name = f\"tmj_model_fold{fold}\" if fold else \"tmj_model\"\n",
    "    \n",
    "    # Initialize loggers\n",
    "    csv_logger = CSVLogger(save_dir / f\"{model_name}_training_log.csv\")\n",
    "    checkpoint_manager = ModelCheckpoint(save_dir, model_name)\n",
    "    early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'])\n",
    "    \n",
    "    # Initialize tracking\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs, val_aucs = [], [], []\n",
    "    \n",
    "    # Best model tracking\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting Training {'for ' + str(fold) if fold else ''}\")\n",
    "    print(f\"ðŸ“ Logs and models will be saved to: {save_dir}\")\n",
    "    print(f\"ðŸ“Š CSV log: {model_name}_training_log.csv\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc, val_auc, val_preds, val_labels, val_probs = validate_epoch(\n",
    "            model, val_loader, criterion, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update best accuracy\n",
    "        is_best = val_acc > best_val_acc\n",
    "        if is_best:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print detailed results\n",
    "        print_epoch_results(epoch, num_epochs, train_loss, train_acc, val_loss, val_acc, \n",
    "                          val_auc, current_lr, best_val_acc, epoch_time)\n",
    "        \n",
    "        # Log to CSV\n",
    "        csv_logger.log_epoch(epoch, train_loss, train_acc, val_loss, val_acc, val_auc, \n",
    "                           current_lr, best_val_acc)\n",
    "        \n",
    "        # Save checkpoints\n",
    "        metrics = {\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_auc': val_auc\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model, optimizer, scheduler, epoch, metrics, \n",
    "            is_best=is_best, is_last=(epoch == num_epochs - 1)\n",
    "        )\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f'\\nðŸ›‘ Early stopping triggered at epoch {epoch+1}')\n",
    "            print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # Final validation metrics\n",
    "    final_val_loss, final_val_acc, final_val_auc, final_preds, final_labels, final_probs = validate_epoch(\n",
    "        model, val_loader, criterion, device, num_epochs\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(final_labels, final_preds)\n",
    "    \n",
    "    print(f\"\\nðŸ TRAINING COMPLETED!\")\n",
    "    print(f\"ðŸ“Š Final Results:\")\n",
    "    print(f\"   Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"   Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"   Final Validation AUC: {final_val_auc:.4f}\")\n",
    "    print(f\"ðŸ“ All logs and models saved to: {save_dir}\")\n",
    "    \n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'val_aucs': val_aucs,\n",
    "        'final_val_acc': final_val_acc,\n",
    "        'final_val_auc': final_val_auc,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'confusion_matrix': cm,\n",
    "        'val_predictions': final_preds,\n",
    "        'val_labels': final_labels,\n",
    "        'val_probabilities': final_probs,\n",
    "        'save_dir': save_dir,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def create_optimizer_and_scheduler(model, train_loader):\n",
    "    \"\"\"Create optimizer and learning rate scheduler\"\"\"\n",
    "    \n",
    "    # AdamW optimizer with weight decay\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Cosine annealing with warm restarts\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=CONFIG['num_epochs'] // 4,  # Restart every 1/4 of total epochs\n",
    "        eta_min=CONFIG['learning_rate'] * 0.01\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "print(\"Enhanced training functions with comprehensive logging:\")\n",
    "print(\"- ðŸ“Š CSV logging for all metrics\")\n",
    "print(\"- ðŸ’¾ Automatic best and last model saving\")\n",
    "print(\"- ðŸ“ˆ Real-time progress bars for batches\")\n",
    "print(\"- ðŸ–¨ï¸  Detailed epoch results printing\")\n",
    "print(\"- ðŸ”§ GPU memory monitoring\")\n",
    "print(\"- â±ï¸  Timing information\")\n",
    "print(\"- ðŸ›‘ Enhanced early stopping\")\n",
    "print(\"- ðŸ“ Organized checkpoint management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Training and Evaluation Functions for Pre-Split Data\n",
    "\n",
    "def train_single_model(train_loader, val_loader, config):\n",
    "    \"\"\"Train a single model using pre-split data\"\"\"\n",
    "    \n",
    "    print(\"Training single model on pre-split data...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_medical_resnet3d(\n",
    "        arch='resnet18',\n",
    "        num_classes=config['num_classes']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create optimizer and scheduler\n",
    "    optimizer, scheduler = create_optimizer_and_scheduler(model, train_loader)\n",
    "    \n",
    "    # Create loss function (Focal Loss for imbalance)\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, history = train_model_with_validation(\n",
    "        model, train_loader, val_loader, criterion, optimizer,\n",
    "        scheduler, config['num_epochs'], device\n",
    "    )\n",
    "    \n",
    "    print(f\"Single model training completed!\")\n",
    "    print(f\"  Final Val Accuracy: {history['final_val_acc']:.4f}\")\n",
    "    print(f\"  Final Val AUC: {history['final_val_auc']:.4f}\")\n",
    "    \n",
    "    return trained_model, history\n",
    "\n",
    "def train_ensemble_models(train_loader, val_loader, config, num_models=5):\n",
    "    \"\"\"Train ensemble of models with different initializations using pre-split data\"\"\"\n",
    "    \n",
    "    print(f\"Training ensemble of {num_models} models on pre-split data...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ensemble_models = []\n",
    "    ensemble_results = []\n",
    "    \n",
    "    for model_idx in range(num_models):\n",
    "        print(f\"\\nTraining Ensemble Model {model_idx + 1}/{num_models}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Set different random seed for each model\n",
    "        set_seed(42 + model_idx * 10)\n",
    "        \n",
    "        # Create model with different initialization\n",
    "        model = create_medical_resnet3d(\n",
    "            arch='resnet18',\n",
    "            num_classes=config['num_classes']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Create optimizer and scheduler\n",
    "        optimizer, scheduler = create_optimizer_and_scheduler(model, train_loader)\n",
    "        \n",
    "        # Create loss function\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model, history = train_model_with_validation(\n",
    "            model, train_loader, val_loader, criterion, optimizer,\n",
    "            scheduler, config['num_epochs'], device, fold=f\"Model_{model_idx+1}\"\n",
    "        )\n",
    "        \n",
    "        # Store model and results\n",
    "        ensemble_models.append(trained_model.state_dict().copy())\n",
    "        ensemble_results.append(history)\n",
    "        \n",
    "        print(f\"Model {model_idx + 1} - Val Accuracy: {history['final_val_acc']:.4f}\")\n",
    "    \n",
    "    # Calculate ensemble statistics\n",
    "    val_accs = [result['final_val_acc'] for result in ensemble_results]\n",
    "    val_aucs = [result['final_val_auc'] for result in ensemble_results]\n",
    "    \n",
    "    print(f\"\\nEnsemble Training Results:\")\n",
    "    print(f\"Individual accuracies: {[f'{acc:.4f}' for acc in val_accs]}\")\n",
    "    print(f\"Individual AUCs: {[f'{auc:.4f}' for auc in val_aucs]}\")\n",
    "    print(f\"Mean accuracy: {np.mean(val_accs):.4f} Â± {np.std(val_accs):.4f}\")\n",
    "    print(f\"Mean AUC: {np.mean(val_aucs):.4f} Â± {np.std(val_aucs):.4f}\")\n",
    "    \n",
    "    return ensemble_models, ensemble_results\n",
    "\n",
    "def evaluate_ensemble(ensemble_models, test_loader, device):\n",
    "    \"\"\"Evaluate ensemble of models on test set\"\"\"\n",
    "    \n",
    "    print(\"Evaluating ensemble on test set...\")\n",
    "    \n",
    "    # Create a model architecture for loading weights\n",
    "    base_model = create_medical_resnet3d(\n",
    "        arch='resnet18',\n",
    "        num_classes=CONFIG['num_classes']\n",
    "    ).to(device)\n",
    "    \n",
    "    all_ensemble_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for model_idx, model_weights in enumerate(ensemble_models):\n",
    "        print(f\"Getting predictions from model {model_idx + 1}/{len(ensemble_models)}...\")\n",
    "        \n",
    "        base_model.load_state_dict(model_weights)\n",
    "        base_model.eval()\n",
    "        \n",
    "        model_probs = []\n",
    "        labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, _ in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = base_model(inputs)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                model_probs.extend(probs.cpu().numpy())\n",
    "                if model_idx == 0:  # Only store labels once\n",
    "                    labels.extend(targets.cpu().numpy())\n",
    "        \n",
    "        all_ensemble_probs.append(model_probs)\n",
    "        if model_idx == 0:\n",
    "            all_labels = labels\n",
    "    \n",
    "    # Average ensemble predictions\n",
    "    ensemble_probs = np.mean(all_ensemble_probs, axis=0)\n",
    "    ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ensemble_acc = accuracy_score(all_labels, ensemble_preds)\n",
    "    try:\n",
    "        ensemble_auc = roc_auc_score(all_labels, ensemble_probs[:, 1]) if ensemble_probs.shape[1] > 1 else 0.0\n",
    "    except:\n",
    "        ensemble_auc = 0.0\n",
    "    \n",
    "    # Individual model accuracies\n",
    "    individual_accs = []\n",
    "    individual_aucs = []\n",
    "    for model_probs in all_ensemble_probs:\n",
    "        model_preds = np.argmax(model_probs, axis=1)\n",
    "        model_acc = accuracy_score(all_labels, model_preds)\n",
    "        individual_accs.append(model_acc)\n",
    "        \n",
    "        try:\n",
    "            model_auc = roc_auc_score(all_labels, np.array(model_probs)[:, 1]) if len(model_probs[0]) > 1 else 0.0\n",
    "            individual_aucs.append(model_auc)\n",
    "        except:\n",
    "            individual_aucs.append(0.0)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ ENSEMBLE EVALUATION RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Individual model accuracies: {[f'{acc:.4f}' for acc in individual_accs]}\")\n",
    "    print(f\"Individual model AUCs: {[f'{auc:.4f}' for auc in individual_aucs]}\")\n",
    "    print(f\"Ensemble accuracy: {ensemble_acc:.4f}\")\n",
    "    print(f\"Ensemble AUC: {ensemble_auc:.4f}\")\n",
    "    print(f\"Best individual accuracy: {max(individual_accs):.4f}\")\n",
    "    print(f\"Improvement over best individual: {ensemble_acc - max(individual_accs):+.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, ensemble_preds)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return {\n",
    "        'ensemble_accuracy': ensemble_acc,\n",
    "        'ensemble_auc': ensemble_auc,\n",
    "        'individual_accuracies': individual_accs,\n",
    "        'individual_aucs': individual_aucs,\n",
    "        'ensemble_predictions': ensemble_preds,\n",
    "        'ensemble_probabilities': ensemble_probs,\n",
    "        'labels': all_labels,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def evaluate_single_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate a single model on test set\"\"\"\n",
    "    \n",
    "    print(\"Evaluating single model on test set...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, _ in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, [p[1] for p in all_probs]) if len(all_probs[0]) > 1 else 0.0\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ SINGLE MODEL TEST RESULTS:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test AUC: {auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'predictions': all_preds,\n",
    "        'probabilities': all_probs,\n",
    "        'labels': all_labels,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_training_history(histories, title_prefix=\"\", save_path=None):\n",
    "    \"\"\"Plot training curves from training history\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    for i, history in enumerate(histories):\n",
    "        # Training and validation loss\n",
    "        axes[0, 0].plot(history['train_losses'], label=f'Model {i+1} Train', alpha=0.7)\n",
    "        axes[0, 1].plot(history['val_losses'], label=f'Model {i+1} Val', alpha=0.7)\n",
    "        \n",
    "        # Training and validation accuracy\n",
    "        axes[1, 0].plot(history['train_accs'], label=f'Model {i+1} Train', alpha=0.7)\n",
    "        axes[1, 1].plot(history['val_accs'], label=f'Model {i+1} Val', alpha=0.7)\n",
    "    \n",
    "    axes[0, 0].set_title(f'{title_prefix}Training Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].set_title(f'{title_prefix}Validation Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].set_title(f'{title_prefix}Training Accuracy')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].set_title(f'{title_prefix}Validation Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Training curves saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=['Healthy', 'Osteoarthritis'], title='Confusion Matrix', save_path=None):\n",
    "    \"\"\"Plot confusion matrix with proper formatting\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def comprehensive_model_evaluation(model_or_models, test_loader, device, is_ensemble=False):\n",
    "    \"\"\"Comprehensive evaluation with visualizations\"\"\"\n",
    "    \n",
    "    if is_ensemble:\n",
    "        results = evaluate_ensemble(model_or_models, test_loader, device)\n",
    "        \n",
    "        # Plot confusion matrix for ensemble\n",
    "        plot_confusion_matrix(\n",
    "            results['confusion_matrix'], \n",
    "            title='Ensemble Model - Test Set Confusion Matrix'\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        results = evaluate_single_model(model_or_models, test_loader, device)\n",
    "        \n",
    "        # Plot confusion matrix for single model\n",
    "        plot_confusion_matrix(\n",
    "            results['confusion_matrix'], \n",
    "            title='Single Model - Test Set Confusion Matrix'\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Training and evaluation functions updated for pre-split data:\")\n",
    "print(\"- train_single_model(): Train one model on pre-split data\")\n",
    "print(\"- train_ensemble_models(): Train multiple models with different seeds\")\n",
    "print(\"- evaluate_ensemble(): Comprehensive ensemble evaluation\")\n",
    "print(\"- evaluate_single_model(): Single model evaluation\") \n",
    "print(\"- comprehensive_model_evaluation(): Evaluation with visualizations\")\n",
    "print(\"- Enhanced plotting functions with save options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cce4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Main Training Execution for Pre-Split Data\n",
    "\n",
    "def main_training_pipeline():\n",
    "    \"\"\"Main pipeline for TMJ classification model training with pre-split data\"\"\"\n",
    "    \n",
    "    print(\"Starting TMJ 3D CNN Training Pipeline with Pre-Split Data\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Validate data structure and load datasets\n",
    "    print(\"\\n1. Validating data structure and loading datasets...\")\n",
    "    try:\n",
    "        if not validate_data_structure(CONFIG['data_path']):\n",
    "            print(\"âŒ Data structure validation failed. Please fix and try again.\")\n",
    "            return None\n",
    "        \n",
    "        # Create all dataloaders\n",
    "        train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_dataloaders(\n",
    "            CONFIG['data_path'], \n",
    "            CONFIG, \n",
    "            use_balanced_sampling=True\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… All datasets loaded successfully!\")\n",
    "        \n",
    "        # Display dataset statistics\n",
    "        print(f\"\\nDataset Statistics:\")\n",
    "        print(f\"  Training:   {len(train_dataset)} samples\")\n",
    "        print(f\"  Validation: {len(val_dataset)} samples\") \n",
    "        print(f\"  Test:       {len(test_dataset)} samples\")\n",
    "        print(f\"  Total:      {len(train_dataset) + len(val_dataset) + len(test_dataset)} samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading datasets: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Choose training strategy\n",
    "    print(\"\\n2. Training Strategy Selection:\")\n",
    "    print(\"   a) Single model training (fastest, good for initial experiments)\")\n",
    "    print(\"   b) Ensemble training (best performance, multiple models)\")\n",
    "    print(\"   c) Both single and ensemble (comprehensive comparison)\")\n",
    "    \n",
    "    strategy = input(\"Choose strategy (a/b/c) [default: a]: \").lower().strip()\n",
    "    if strategy == '':\n",
    "        strategy = 'a'\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if strategy == 'a':\n",
    "        # Single model training\n",
    "        print(\"\\n3. Training Single Model...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        model, history = train_single_model(train_loader, val_loader, CONFIG)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"\\n4. Evaluating on Test Set...\")\n",
    "        test_results = comprehensive_model_evaluation(\n",
    "            model, test_loader, device, is_ensemble=False\n",
    "        )\n",
    "        \n",
    "        # Plot training curves\n",
    "        plot_training_history([history], \"Single Model \")\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'strategy': 'single_model',\n",
    "            'model': model.state_dict(),\n",
    "            'history': history,\n",
    "            'test_results': test_results\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ FINAL RESULTS - Single Model:\")\n",
    "        print(f\"  Validation Accuracy: {history['final_val_acc']:.4f}\")\n",
    "        print(f\"  Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "        print(f\"  Test AUC: {test_results['auc']:.4f}\")\n",
    "        \n",
    "    elif strategy == 'b':\n",
    "        # Ensemble training\n",
    "        print(\"\\n3. Training Ensemble Models...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        ensemble_models, ensemble_histories = train_ensemble_models(\n",
    "            train_loader, val_loader, CONFIG, num_models=CONFIG['ensemble_size']\n",
    "        )\n",
    "        \n",
    "        # Evaluate ensemble on test set\n",
    "        print(\"\\n4. Evaluating Ensemble on Test Set...\")\n",
    "        test_results = comprehensive_model_evaluation(\n",
    "            ensemble_models, test_loader, device, is_ensemble=True\n",
    "        )\n",
    "        \n",
    "        # Plot training curves\n",
    "        plot_training_history(ensemble_histories, \"Ensemble \")\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'strategy': 'ensemble',\n",
    "            'models': ensemble_models,\n",
    "            'histories': ensemble_histories,\n",
    "            'test_results': test_results\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ FINAL RESULTS - Ensemble:\")\n",
    "        val_accs = [h['final_val_acc'] for h in ensemble_histories]\n",
    "        print(f\"  Validation Accuracy: {np.mean(val_accs):.4f} Â± {np.std(val_accs):.4f}\")\n",
    "        print(f\"  Test Accuracy: {test_results['ensemble_accuracy']:.4f}\")\n",
    "        print(f\"  Test AUC: {test_results['ensemble_auc']:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # Both single and ensemble\n",
    "        print(\"\\n3. Training Both Single Model and Ensemble...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Train single model\n",
    "        print(\"\\n3a. Training Single Model...\")\n",
    "        single_model, single_history = train_single_model(train_loader, val_loader, CONFIG)\n",
    "        \n",
    "        # Train ensemble\n",
    "        print(\"\\n3b. Training Ensemble Models...\")\n",
    "        ensemble_models, ensemble_histories = train_ensemble_models(\n",
    "            train_loader, val_loader, CONFIG, num_models=CONFIG['ensemble_size']\n",
    "        )\n",
    "        \n",
    "        # Evaluate both on test set\n",
    "        print(\"\\n4. Evaluating Both Approaches on Test Set...\")\n",
    "        \n",
    "        print(\"\\n4a. Single Model Test Results:\")\n",
    "        single_test_results = comprehensive_model_evaluation(\n",
    "            single_model, test_loader, device, is_ensemble=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n4b. Ensemble Test Results:\")\n",
    "        ensemble_test_results = comprehensive_model_evaluation(\n",
    "            ensemble_models, test_loader, device, is_ensemble=True\n",
    "        )\n",
    "        \n",
    "        # Plot training curves\n",
    "        plot_training_history([single_history], \"Single Model \")\n",
    "        plot_training_history(ensemble_histories, \"Ensemble \")\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'strategy': 'both',\n",
    "            'single_model': single_model.state_dict(),\n",
    "            'single_history': single_history,\n",
    "            'single_test_results': single_test_results,\n",
    "            'ensemble_models': ensemble_models,\n",
    "            'ensemble_histories': ensemble_histories,\n",
    "            'ensemble_test_results': ensemble_test_results\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ FINAL COMPARISON:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Single Model:\")\n",
    "        print(f\"  Test Accuracy: {single_test_results['accuracy']:.4f}\")\n",
    "        print(f\"  Test AUC: {single_test_results['auc']:.4f}\")\n",
    "        print(f\"Ensemble Model:\")\n",
    "        print(f\"  Test Accuracy: {ensemble_test_results['ensemble_accuracy']:.4f}\")\n",
    "        print(f\"  Test AUC: {ensemble_test_results['ensemble_auc']:.4f}\")\n",
    "        print(f\"Ensemble Improvement: {ensemble_test_results['ensemble_accuracy'] - single_test_results['accuracy']:+.4f}\")\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    print(f\"\\n5. Saving Results...\")\n",
    "    save_results(results, strategy)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, strategy_name):\n",
    "    \"\"\"Save training results and models for pre-split data\"\"\"\n",
    "    \n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = Path(\"tmj_results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save training results\n",
    "    results_file = results_dir / f\"{strategy_name}_results_{timestamp}.pkl\"\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'results': results,\n",
    "            'config': CONFIG,\n",
    "            'strategy': strategy_name,\n",
    "            'timestamp': timestamp\n",
    "        }, f)\n",
    "    \n",
    "    # Save model weights\n",
    "    if strategy_name == 'single_model':\n",
    "        models_file = results_dir / f\"single_model_{timestamp}.pt\"\n",
    "        torch.save(results['model'], models_file)\n",
    "    elif strategy_name == 'ensemble':\n",
    "        models_file = results_dir / f\"ensemble_models_{timestamp}.pt\"\n",
    "        torch.save(results['models'], models_file)\n",
    "    else:  # both\n",
    "        single_file = results_dir / f\"single_model_{timestamp}.pt\"\n",
    "        ensemble_file = results_dir / f\"ensemble_models_{timestamp}.pt\"\n",
    "        torch.save(results['single_model'], single_file)\n",
    "        torch.save(results['ensemble_models'], ensemble_file)\n",
    "        models_file = f\"{single_file.name} & {ensemble_file.name}\"\n",
    "    \n",
    "    # Save detailed report\n",
    "    report_file = results_dir / f\"{strategy_name}_report_{timestamp}.txt\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(f\"TMJ 3D CNN Training Report - {strategy_name.upper()}\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Strategy: {strategy_name}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Configuration:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for key, value in CONFIG.items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        if strategy_name == \"single_model\":\n",
    "            f.write(\"Single Model Results:\\n\")\n",
    "            f.write(\"-\" * 25 + \"\\n\")\n",
    "            f.write(f\"  Validation Accuracy: {results['history']['final_val_acc']:.4f}\\n\")\n",
    "            f.write(f\"  Validation AUC: {results['history']['final_val_auc']:.4f}\\n\")\n",
    "            f.write(f\"  Test Accuracy: {results['test_results']['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Test AUC: {results['test_results']['auc']:.4f}\\n\")\n",
    "            \n",
    "        elif strategy_name == \"ensemble\":\n",
    "            val_accs = [h['final_val_acc'] for h in results['histories']]\n",
    "            val_aucs = [h['final_val_auc'] for h in results['histories']]\n",
    "            f.write(\"Ensemble Results:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"  Validation Accuracy: {np.mean(val_accs):.4f} Â± {np.std(val_accs):.4f}\\n\")\n",
    "            f.write(f\"  Validation AUC: {np.mean(val_aucs):.4f} Â± {np.std(val_aucs):.4f}\\n\")\n",
    "            f.write(f\"  Test Accuracy: {results['test_results']['ensemble_accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Test AUC: {results['test_results']['ensemble_auc']:.4f}\\n\")\n",
    "            f.write(f\"  Individual Test Accuracies: {[f'{acc:.4f}' for acc in results['test_results']['individual_accuracies']]}\\n\")\n",
    "            \n",
    "        else:  # both\n",
    "            f.write(\"Comparison Results:\\n\")\n",
    "            f.write(\"-\" * 22 + \"\\n\")\n",
    "            f.write(\"Single Model:\\n\")\n",
    "            f.write(f\"  Test Accuracy: {results['single_test_results']['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Test AUC: {results['single_test_results']['auc']:.4f}\\n\")\n",
    "            f.write(\"Ensemble Model:\\n\")\n",
    "            f.write(f\"  Test Accuracy: {results['ensemble_test_results']['ensemble_accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Test AUC: {results['ensemble_test_results']['ensemble_auc']:.4f}\\n\")\n",
    "            improvement = results['ensemble_test_results']['ensemble_accuracy'] - results['single_test_results']['accuracy']\n",
    "            f.write(f\"  Ensemble Improvement: {improvement:+.4f}\\n\")\n",
    "    \n",
    "    print(f\"âœ… Results saved to: {results_dir}\")\n",
    "    print(f\"  - Training results: {results_file.name}\")\n",
    "    print(f\"  - Model weights: {models_file}\")\n",
    "    print(f\"  - Summary report: {report_file.name}\")\n",
    "\n",
    "def load_and_inference(model_path, test_loader, model_type='single'):\n",
    "    \"\"\"Load trained model and run inference on test data\"\"\"\n",
    "    \n",
    "    print(f\"Loading {model_type} model for inference...\")\n",
    "    \n",
    "    if model_type == 'ensemble':\n",
    "        # Load ensemble models\n",
    "        ensemble_models = torch.load(model_path, map_location=device)\n",
    "        print(f\"Loaded ensemble of {len(ensemble_models)} models\")\n",
    "        \n",
    "        # Evaluate ensemble\n",
    "        results = evaluate_ensemble(ensemble_models, test_loader, device)\n",
    "        return results\n",
    "        \n",
    "    else:\n",
    "        # Load single model\n",
    "        model_weights = torch.load(model_path, map_location=device)\n",
    "        print(\"Loaded single model\")\n",
    "        \n",
    "        # Create model architecture and load weights\n",
    "        model = create_medical_resnet3d(\n",
    "            arch='resnet18',\n",
    "            num_classes=CONFIG['num_classes']\n",
    "        ).to(device)\n",
    "        model.load_state_dict(model_weights)\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_single_model(model, test_loader, device)\n",
    "        return results\n",
    "\n",
    "def quick_train():\n",
    "    \"\"\"Quick training function for testing/debugging\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Quick Training Mode (reduced epochs for testing)\")\n",
    "    \n",
    "    # Temporarily reduce epochs for quick testing\n",
    "    original_epochs = CONFIG['num_epochs']\n",
    "    CONFIG['num_epochs'] = 5\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        train_loader, val_loader, test_loader, _, _, _ = create_dataloaders(\n",
    "            CONFIG['data_path'], CONFIG, use_balanced_sampling=True\n",
    "        )\n",
    "        \n",
    "        # Train single model quickly\n",
    "        model, history = train_single_model(train_loader, val_loader, CONFIG)\n",
    "        \n",
    "        # Quick evaluation\n",
    "        test_results = evaluate_single_model(model, test_loader, device)\n",
    "        \n",
    "        print(f\"âœ… Quick training completed!\")\n",
    "        print(f\"  Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "        \n",
    "        return model, history, test_results\n",
    "        \n",
    "    finally:\n",
    "        # Restore original epochs\n",
    "        CONFIG['num_epochs'] = original_epochs\n",
    "\n",
    "def validate_training_setup():\n",
    "    \"\"\"Validate that everything is set up correctly for training\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” Validating Training Setup...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    checks_passed = 0\n",
    "    total_checks = 6\n",
    "    \n",
    "    # Check 1: Data structure\n",
    "    if validate_data_structure(CONFIG['data_path']):\n",
    "        print(\"âœ… Data structure validation passed\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(\"âŒ Data structure validation failed\")\n",
    "    \n",
    "    # Check 2: GPU availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… GPU available: {torch.cuda.get_device_name()}\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(\"âš ï¸  GPU not available, will use CPU (slower)\")\n",
    "        checks_passed += 1\n",
    "    \n",
    "    # Check 3: Try loading a small batch\n",
    "    try:\n",
    "        train_loader, _, _, _, _, _ = create_dataloaders(CONFIG['data_path'], CONFIG)\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        print(f\"âœ… Data loading works, batch shape: {sample_batch[0].shape}\")\n",
    "        checks_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Data loading failed: {e}\")\n",
    "    \n",
    "    # Check 4: Model creation\n",
    "    try:\n",
    "        test_model = create_medical_resnet3d(arch='resnet18', num_classes=CONFIG['num_classes'])\n",
    "        print(f\"âœ… Model creation successful\")\n",
    "        checks_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model creation failed: {e}\")\n",
    "    \n",
    "    # Check 5: Forward pass test\n",
    "    try:\n",
    "        test_input = torch.randn(1, 1, *CONFIG['input_size'])\n",
    "        with torch.no_grad():\n",
    "            test_output = test_model(test_input)\n",
    "        print(f\"âœ… Model forward pass successful, output shape: {test_output.shape}\")\n",
    "        checks_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model forward pass failed: {e}\")\n",
    "    \n",
    "    # Check 6: Directory permissions\n",
    "    try:\n",
    "        results_dir = Path(\"tmj_results\")\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "        test_file = results_dir / \"test.txt\"\n",
    "        test_file.write_text(\"test\")\n",
    "        test_file.unlink()\n",
    "        print(\"âœ… Results directory writable\")\n",
    "        checks_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Cannot write to results directory: {e}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Setup Validation: {checks_passed}/{total_checks} checks passed\")\n",
    "    \n",
    "    if checks_passed == total_checks:\n",
    "        print(\"ðŸŽ‰ All checks passed! Ready to train.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âš ï¸  Some checks failed. Please fix issues before training.\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if data path exists\n",
    "    if not os.path.exists(CONFIG['data_path']):\n",
    "        print(f\"âŒ Error: Data path {CONFIG['data_path']} does not exist!\")\n",
    "        print(\"Please update CONFIG['data_path'] to point to your TMJ dataset\")\n",
    "        print(\"Expected structure:\")\n",
    "        print(\"  tmj_data/\")\n",
    "        print(\"    â”œâ”€â”€ train/\")\n",
    "        print(\"    â”‚   â”œâ”€â”€ 0/  (class 0 files)\")\n",
    "        print(\"    â”‚   â””â”€â”€ 1/  (class 1 files)\")\n",
    "        print(\"    â”œâ”€â”€ val/\")\n",
    "        print(\"    â”‚   â”œâ”€â”€ 0/  (class 0 files)\")\n",
    "        print(\"    â”‚   â””â”€â”€ 1/  (class 1 files)\")\n",
    "        print(\"    â””â”€â”€ test/\")\n",
    "        print(\"        â”œâ”€â”€ 0/  (class 0 files)\")\n",
    "        print(\"        â””â”€â”€ 1/  (class 1 files)\")\n",
    "    else:\n",
    "        # Validate setup\n",
    "        if validate_training_setup():\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ðŸš€ READY TO START TRAINING!\")\n",
    "            print(\"=\"*50)\n",
    "            print(\"Run: main_training_pipeline()\")\n",
    "            print(\"Or for quick test: quick_train()\")\n",
    "            print(\"=\"*50)\n",
    "        else:\n",
    "            print(\"\\nâŒ Please fix the issues above before proceeding.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TMJ 3D CNN TRAINING SCRIPT - PRE-SPLIT DATA VERSION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"â€¢ main_training_pipeline() - Complete training pipeline\")\n",
    "print(\"â€¢ quick_train() - Fast training for testing (5 epochs)\")\n",
    "print(\"â€¢ validate_training_setup() - Check if everything is ready\")\n",
    "print(\"â€¢ load_and_inference(model_path, test_loader) - Load and test models\")\n",
    "print(\"\\nUpdate CONFIG['data_path'] then run main_training_pipeline()!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_training_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dmodel_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
