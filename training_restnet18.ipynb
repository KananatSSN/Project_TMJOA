{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cWEX4sIwnMfi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWEX4sIwnMfi",
        "outputId": "20094b43-2eaf-4be2-ca38-939988ab967e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q9ZlCsh4nP_B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ZlCsh4nP_B",
        "outputId": "829c185f-9b5c-4715-a86d-2bdc2db51f91"
      },
      "outputs": [],
      "source": [
        "pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ab5190",
      "metadata": {
        "id": "86ab5190"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import List, Callable\n",
        "from scipy.ndimage import map_coordinates, gaussian_filter\n",
        "\n",
        "# MONAI imports\n",
        "from monai.transforms import (\n",
        "    RandAdjustContrast,\n",
        "    RandGaussianNoise,\n",
        "    RandGaussianSmooth,\n",
        "    RandScaleIntensity,\n",
        "    RandShiftIntensity,\n",
        "    RandGibbsNoise,\n",
        "    RandKSpaceSpikeNoise,\n",
        "    RandRicianNoise,\n",
        "    RandHistogramShift,\n",
        "    RandCoarseDropout,\n",
        "    RandCoarseShuffle,\n",
        ")\n",
        "\n",
        "\n",
        "class CBCTRandAugment:\n",
        "    \"\"\"\n",
        "    RandAugment implementation for preprocessed CBCT volumes with MONAI augmentations.\n",
        "    Input: torch.Tensor of shape [1, n, n, n], dtype=float32, range=[0,1]\n",
        "    Works with any cubic volume size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int = 2, m: int = 6, use_monai: bool = True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n: Number of transformations to apply sequentially\n",
        "            m: Magnitude of transformations (0-10 scale)\n",
        "            use_monai: Whether to include MONAI transformations\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.use_monai = use_monai\n",
        "\n",
        "        # Original custom operations\n",
        "        self.operations: List[Callable] = [\n",
        "            self._random_rotation,\n",
        "            self._random_flip,\n",
        "            self._random_translation,\n",
        "            self._random_noise,\n",
        "            self._random_gamma,\n",
        "            self._random_contrast,\n",
        "            self._random_gaussian_blur,\n",
        "            self._elastic_deformation,\n",
        "            self._random_zoom,\n",
        "            self._identity,\n",
        "        ]\n",
        "\n",
        "        # Add MONAI operations if enabled\n",
        "        if use_monai:\n",
        "            self._init_monai_transforms()\n",
        "            self.operations.extend([\n",
        "                self._monai_adjust_contrast,\n",
        "                self._monai_gaussian_noise,\n",
        "                self._monai_gaussian_smooth,\n",
        "                self._monai_scale_intensity,\n",
        "                self._monai_shift_intensity,\n",
        "                self._monai_gibbs_noise,\n",
        "                self._monai_kspace_spike,\n",
        "                self._monai_rician_noise,\n",
        "                self._monai_histogram_shift,\n",
        "                self._monai_coarse_dropout,\n",
        "                self._monai_coarse_shuffle,\n",
        "            ])\n",
        "\n",
        "    def _init_monai_transforms(self):\n",
        "        \"\"\"Initialize MONAI transforms with magnitude-based probabilities\"\"\"\n",
        "        # These will be recreated with scaled parameters in each method\n",
        "        pass\n",
        "\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply RandAugment to preprocessed CBCT volume\n",
        "\n",
        "        Args:\n",
        "            image: Tensor of shape [1, n, n, n], range [0,1]\n",
        "        Returns:\n",
        "            Augmented tensor of same shape and range\n",
        "        \"\"\"\n",
        "        # Validate input\n",
        "        assert len(image.shape) == 4, f\"Expected 4D tensor, got {len(image.shape)}D\"\n",
        "        assert image.shape[0] == 1, f\"Expected channel dimension = 1, got {image.shape[0]}\"\n",
        "        assert image.dtype == torch.float32, f\"Expected float32, got {image.dtype}\"\n",
        "\n",
        "        # Select N random operations\n",
        "        selected_ops = np.random.choice(self.operations, self.n, replace=False)\n",
        "\n",
        "        # Apply operations sequentially\n",
        "        for op in selected_ops:\n",
        "            image = op(image)\n",
        "            # Ensure values stay in [0,1] range\n",
        "            image = torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _scale_magnitude(self, max_val: float) -> float:\n",
        "        \"\"\"Scale magnitude from [0,10] to [0, max_val]\"\"\"\n",
        "        random_multiplier = random.randint(1, self.m)\n",
        "        return (random_multiplier / 10.0) * max_val\n",
        "\n",
        "    def _get_prob(self) -> float:\n",
        "        \"\"\"Get probability based on magnitude\"\"\"\n",
        "        return min(0.9, self.m / 10.0)\n",
        "\n",
        "    # ==================== MONAI-based Transformations ====================\n",
        "\n",
        "    def _monai_adjust_contrast(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI contrast adjustment with magnitude-scaled gamma\"\"\"\n",
        "        max_gamma = (0.5, 1.5)  # Contrast range\n",
        "        gamma_range = self._scale_magnitude(0.5)\n",
        "        gamma = (1.0 - gamma_range, 1.0 + gamma_range)\n",
        "\n",
        "        transform = RandAdjustContrast(prob=self._get_prob(), gamma=gamma)\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_gaussian_noise(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI Gaussian noise with magnitude-scaled standard deviation\"\"\"\n",
        "        max_std = 0.15\n",
        "        std = self._scale_magnitude(max_std)\n",
        "\n",
        "        transform = RandGaussianNoise(prob=self._get_prob(), mean=0.0, std=std)\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_gaussian_smooth(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI Gaussian smoothing with magnitude-scaled sigma\"\"\"\n",
        "        max_sigma = (0.5, 2.0)\n",
        "        sigma_scale = self._scale_magnitude(1.0)\n",
        "        sigma_min = 0.25 * sigma_scale\n",
        "        sigma_max = max_sigma[1] * sigma_scale\n",
        "\n",
        "        transform = RandGaussianSmooth(\n",
        "            prob=self._get_prob(),\n",
        "            sigma_x=(sigma_min, sigma_max),\n",
        "            sigma_y=(sigma_min, sigma_max),\n",
        "            sigma_z=(sigma_min, sigma_max)\n",
        "        )\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_scale_intensity(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI intensity scaling with magnitude-based factors\"\"\"\n",
        "        max_factor = 0.3\n",
        "        factor = self._scale_magnitude(max_factor)\n",
        "        factors = (1.0 - factor, 1.0 + factor)\n",
        "\n",
        "        transform = RandScaleIntensity(prob=self._get_prob(), factors=factors)\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_shift_intensity(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI intensity shift with magnitude-based offset\"\"\"\n",
        "        max_offset = 0.15\n",
        "        offset = self._scale_magnitude(max_offset)\n",
        "\n",
        "        transform = RandShiftIntensity(prob=self._get_prob(), offsets=(-offset, offset))\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_gibbs_noise(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI Gibbs ringing artifact simulation\"\"\"\n",
        "        # Scale alpha based on magnitude (0.0 to 1.0)\n",
        "        max_alpha = 0.8\n",
        "        alpha = self._scale_magnitude(max_alpha)\n",
        "\n",
        "        transform = RandGibbsNoise(prob=self._get_prob(), alpha=(0.0, alpha))\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_kspace_spike(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI k-space spike noise (MRI artifact simulation)\"\"\"\n",
        "        # Useful for simulating motion or hardware artifacts\n",
        "        max_intensity = (0.5, 1.5)\n",
        "        intensity_scale = self._scale_magnitude(1.0)\n",
        "\n",
        "        transform = RandKSpaceSpikeNoise(\n",
        "            prob=self._get_prob(),\n",
        "            intensity_range=(0.1 * intensity_scale, max_intensity[1] * intensity_scale)\n",
        "        )\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_rician_noise(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI Rician noise (common in MRI)\"\"\"\n",
        "        max_std = 0.15\n",
        "        std = self._scale_magnitude(max_std)\n",
        "\n",
        "        transform = RandRicianNoise(prob=self._get_prob(), mean=0.0, std=std)\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_histogram_shift(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI histogram shifting for intensity distribution changes\"\"\"\n",
        "        max_num_points = 15\n",
        "        num_points = max(3, int(self._scale_magnitude(max_num_points)))\n",
        "\n",
        "        transform = RandHistogramShift(prob=self._get_prob(), num_control_points=num_points)\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_coarse_dropout(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI coarse dropout - randomly drop out cubic regions\"\"\"\n",
        "        # Scale number and size of holes based on magnitude\n",
        "        max_holes = 8\n",
        "        num_holes = max(1, int(self._scale_magnitude(max_holes)))\n",
        "\n",
        "        # Hole size as fraction of volume\n",
        "        max_spatial_size = 0.15  # 15% of dimension\n",
        "        spatial_size = self._scale_magnitude(max_spatial_size)\n",
        "        hole_size = int(spatial_size * image.shape[1])  # Assume cubic volume\n",
        "\n",
        "        transform = RandCoarseDropout(\n",
        "            prob=self._get_prob(),\n",
        "            holes=num_holes,\n",
        "            spatial_size=hole_size,\n",
        "            fill_value=0\n",
        "        )\n",
        "        return transform(image)\n",
        "\n",
        "    def _monai_coarse_shuffle(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"MONAI coarse shuffle - shuffle cubic regions\"\"\"\n",
        "        # Scale number and size of regions based on magnitude\n",
        "        max_holes = 6\n",
        "        num_holes = max(1, int(self._scale_magnitude(max_holes)))\n",
        "\n",
        "        max_spatial_size = 0.12\n",
        "        spatial_size = self._scale_magnitude(max_spatial_size)\n",
        "        hole_size = int(spatial_size * image.shape[1])\n",
        "\n",
        "        transform = RandCoarseShuffle(\n",
        "            prob=self._get_prob(),\n",
        "            holes=num_holes,\n",
        "            spatial_size=hole_size\n",
        "        )\n",
        "        return transform(image)\n",
        "\n",
        "    # ==================== Original Custom Transformations ====================\n",
        "\n",
        "    def _identity(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Identity transformation\"\"\"\n",
        "        return image\n",
        "\n",
        "    def _random_rotation(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Random 3D rotation with magnitude-scaled angle\"\"\"\n",
        "        max_angle = 30\n",
        "        angle = self._scale_magnitude(max_angle)\n",
        "\n",
        "        axis = np.random.randint(0, 3)\n",
        "        angle_rad = np.random.uniform(-angle, angle) * np.pi / 180\n",
        "\n",
        "        if axis == 0:\n",
        "            dims = [2, 3]\n",
        "        elif axis == 1:\n",
        "            dims = [1, 3]\n",
        "        else:\n",
        "            dims = [1, 2]\n",
        "\n",
        "        if abs(angle) < 5:\n",
        "            return image\n",
        "        elif abs(angle) % 90 < 5:\n",
        "            k = int(round(angle / 90)) % 4\n",
        "            return torch.rot90(image, k=k, dims=dims)\n",
        "        else:\n",
        "            return self._apply_affine_rotation(image, angle_rad, axis)\n",
        "\n",
        "    def _apply_affine_rotation(self, image: torch.Tensor, angle: float, axis: int) -> torch.Tensor:\n",
        "        \"\"\"Apply arbitrary angle rotation using affine transformation\"\"\"\n",
        "        img_np = image.squeeze(0).numpy()\n",
        "        cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
        "        center = np.array(img_np.shape) / 2\n",
        "\n",
        "        if axis == 0:\n",
        "            matrix = np.array([[1, 0, 0], [0, cos_a, -sin_a], [0, sin_a, cos_a]])\n",
        "        elif axis == 1:\n",
        "            matrix = np.array([[cos_a, 0, sin_a], [0, 1, 0], [-sin_a, 0, cos_a]])\n",
        "        else:\n",
        "            matrix = np.array([[cos_a, -sin_a, 0], [sin_a, cos_a, 0], [0, 0, 1]])\n",
        "\n",
        "        coords = np.mgrid[0:img_np.shape[0], 0:img_np.shape[1], 0:img_np.shape[2]]\n",
        "        coords = coords.reshape(3, -1)\n",
        "        coords = coords - center.reshape(3, 1)\n",
        "        rotated_coords = matrix @ coords\n",
        "        rotated_coords = rotated_coords + center.reshape(3, 1)\n",
        "\n",
        "        rotated_img = map_coordinates(img_np, rotated_coords.reshape(3, *img_np.shape),\n",
        "                                    order=1, mode='constant', cval=0.0)\n",
        "\n",
        "        return torch.tensor(rotated_img, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    def _random_flip(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Random flip along one or more axes\"\"\"\n",
        "        axes_to_flip = []\n",
        "        for dim in [1, 2, 3]:\n",
        "            if torch.rand(1) < 0.5:\n",
        "                axes_to_flip.append(dim)\n",
        "\n",
        "        for dim in axes_to_flip:\n",
        "            image = torch.flip(image, dims=[dim])\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _random_translation(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Random translation with magnitude-scaled displacement\"\"\"\n",
        "        max_translation = 0.2\n",
        "        translation = self._scale_magnitude(max_translation)\n",
        "\n",
        "        shifts = []\n",
        "        for dim_size in image.shape[1:]:\n",
        "            max_shift = int(translation * dim_size)\n",
        "            shift = np.random.randint(-max_shift, max_shift + 1)\n",
        "            shifts.append(shift)\n",
        "\n",
        "        for i, shift in enumerate(shifts):\n",
        "            if shift != 0:\n",
        "                image = torch.roll(image, shifts=shift, dims=i+1)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _random_noise(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add Gaussian noise with magnitude-scaled standard deviation\"\"\"\n",
        "        max_std = 0.1\n",
        "        noise_std = self._scale_magnitude(max_std)\n",
        "\n",
        "        if noise_std > 0:\n",
        "            noise = torch.randn_like(image) * noise_std\n",
        "            image = image + noise\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _random_gamma(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Gamma correction with magnitude-scaled gamma value\"\"\"\n",
        "        max_gamma_change = 0.4\n",
        "        gamma_change = self._scale_magnitude(max_gamma_change)\n",
        "        gamma = 1.0 + np.random.uniform(-gamma_change/2, gamma_change/2)\n",
        "\n",
        "        image = torch.pow(image, gamma)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _random_contrast(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Adjust contrast with magnitude-scaled factor\"\"\"\n",
        "        max_contrast_change = 0.4\n",
        "        contrast_change = self._scale_magnitude(max_contrast_change)\n",
        "        contrast_factor = 1.0 + np.random.uniform(-contrast_change/2, contrast_change/2)\n",
        "\n",
        "        mean_val = torch.mean(image)\n",
        "        image = (image - mean_val) * contrast_factor + mean_val\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _random_gaussian_blur(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply Gaussian blur with magnitude-scaled sigma\"\"\"\n",
        "        max_sigma = 2.0\n",
        "        sigma = self._scale_magnitude(max_sigma)\n",
        "\n",
        "        if sigma > 0.1:\n",
        "            img_np = image.squeeze(0).numpy()\n",
        "            blurred = gaussian_filter(img_np, sigma=sigma)\n",
        "            image = torch.tensor(blurred, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _elastic_deformation(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply elastic deformation with magnitude-scaled displacement\"\"\"\n",
        "        max_displacement = 10\n",
        "        displacement = self._scale_magnitude(max_displacement)\n",
        "\n",
        "        if displacement < 1:\n",
        "            return image\n",
        "\n",
        "        shape = image.shape[1:]\n",
        "        dx = np.random.uniform(-displacement, displacement, shape)\n",
        "        dy = np.random.uniform(-displacement, displacement, shape)\n",
        "        dz = np.random.uniform(-displacement, displacement, shape)\n",
        "\n",
        "        sigma = displacement / 3\n",
        "        dx = gaussian_filter(dx, sigma=sigma)\n",
        "        dy = gaussian_filter(dy, sigma=sigma)\n",
        "        dz = gaussian_filter(dz, sigma=sigma)\n",
        "\n",
        "        coords = np.mgrid[0:shape[0], 0:shape[1], 0:shape[2]].astype(np.float32)\n",
        "        coords[0] += dx\n",
        "        coords[1] += dy\n",
        "        coords[2] += dz\n",
        "\n",
        "        img_np = image.squeeze(0).numpy()\n",
        "        deformed = map_coordinates(img_np, coords, order=1, mode='reflect')\n",
        "\n",
        "        return torch.tensor(deformed, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    def _random_zoom(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Zoom the image and randomly crop to keep the same dimension\"\"\"\n",
        "        # Define zoom range based on magnitude\n",
        "        # Zoom > 1.0 means zoom in (makes objects larger)\n",
        "        # Zoom < 1.0 means zoom out (makes objects smaller)\n",
        "        max_zoom = 0.3  # Â±30% zoom\n",
        "        zoom_range = self._scale_magnitude(max_zoom)\n",
        "        zoom_factor = 1.0 + np.random.uniform(-zoom_range, zoom_range)\n",
        "\n",
        "        # Clamp zoom to reasonable range (0.7 to 1.5)\n",
        "        zoom_factor = np.clip(zoom_factor, 0.7, 1.5)\n",
        "\n",
        "        img_np = image.squeeze(0).numpy()\n",
        "        original_shape = img_np.shape\n",
        "\n",
        "        # Calculate new size after zoom\n",
        "        new_shape = tuple([int(s * zoom_factor) for s in original_shape])\n",
        "\n",
        "        # Create coordinate grid for the new size\n",
        "        coords = np.array(np.meshgrid(\n",
        "            np.linspace(0, original_shape[0] - 1, new_shape[0]),\n",
        "            np.linspace(0, original_shape[1] - 1, new_shape[1]),\n",
        "            np.linspace(0, original_shape[2] - 1, new_shape[2]),\n",
        "            indexing='ij'\n",
        "        ))\n",
        "\n",
        "        # Apply zoom through interpolation\n",
        "        zoomed = map_coordinates(img_np, coords, order=3, mode='constant', cval=0.0)\n",
        "\n",
        "        if zoom_factor > 1.0:\n",
        "            # Zoomed in - need to crop\n",
        "            # Random crop to original size\n",
        "            crop_start = []\n",
        "            for i in range(3):\n",
        "                max_start = new_shape[i] - original_shape[i]\n",
        "                if max_start > 0:\n",
        "                    start = np.random.randint(0, max_start + 1)\n",
        "                else:\n",
        "                    start = 0\n",
        "                crop_start.append(start)\n",
        "\n",
        "            cropped = zoomed[\n",
        "                crop_start[0]:crop_start[0] + original_shape[0],\n",
        "                crop_start[1]:crop_start[1] + original_shape[1],\n",
        "                crop_start[2]:crop_start[2] + original_shape[2]\n",
        "            ]\n",
        "\n",
        "            return torch.tensor(cropped, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        else:\n",
        "            # Zoomed out - need to pad\n",
        "            # Random placement of smaller volume in original size canvas\n",
        "            padded = np.zeros(original_shape, dtype=np.float32)\n",
        "\n",
        "            pad_start = []\n",
        "            for i in range(3):\n",
        "                max_start = original_shape[i] - new_shape[i]\n",
        "                if max_start > 0:\n",
        "                    start = np.random.randint(0, max_start + 1)\n",
        "                else:\n",
        "                    start = 0\n",
        "                pad_start.append(start)\n",
        "\n",
        "            padded[\n",
        "                pad_start[0]:pad_start[0] + new_shape[0],\n",
        "                pad_start[1]:pad_start[1] + new_shape[1],\n",
        "                pad_start[2]:pad_start[2] + new_shape[2]\n",
        "            ] = zoomed\n",
        "\n",
        "            return torch.tensor(padded, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Create augmenter with MONAI transforms\n",
        "#     augmenter = CBCTRandAugment(n=3, m=7, use_monai=True)\n",
        "\n",
        "#     # Create dummy CBCT volume\n",
        "#     dummy_volume = torch.rand(1, 128, 128, 128, dtype=torch.float32)\n",
        "\n",
        "#     # Apply augmentation\n",
        "#     augmented = augmenter(dummy_volume)\n",
        "\n",
        "#     print(f\"Original shape: {dummy_volume.shape}\")\n",
        "#     print(f\"Augmented shape: {augmented.shape}\")\n",
        "#     print(f\"Value range: [{augmented.min():.3f}, {augmented.max():.3f}]\")\n",
        "#     print(f\"Total operations available: {len(augmenter.operations)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YhoYbz55dIEw",
      "metadata": {
        "id": "YhoYbz55dIEw"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Modified 3D ResNet for larger input size and binary classification\n",
        "class ModifiedWideResNet3D(nn.Module):\n",
        "    \"\"\"Modified WideResNet3D for larger input and binary classification\"\"\"\n",
        "\n",
        "    def __init__(self, width=2, depth=[2, 2, 2, 2], num_classes=2, dropout_rate=0.3):\n",
        "        super(ModifiedWideResNet3D, self).__init__()\n",
        "\n",
        "        nChannels = [16*width, 16*width, 32*width, 64*width, 128*width]\n",
        "\n",
        "        # Initial convolution - downsample immediately for large inputs\n",
        "        self.conv1 = nn.Conv3d(1, nChannels[0], kernel_size=7, stride=2,\n",
        "                              padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(nChannels[0])\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual blocks with progressive downsampling\n",
        "        self.block1 = self._make_layer(nChannels[0], nChannels[1], depth[0], stride=1, dropout_rate=dropout_rate)\n",
        "        self.block2 = self._make_layer(nChannels[1], nChannels[2], depth[1], stride=2, dropout_rate=dropout_rate)\n",
        "        self.block3 = self._make_layer(nChannels[2], nChannels[3], depth[2], stride=2, dropout_rate=dropout_rate)\n",
        "        self.block4 = self._make_layer(nChannels[3], nChannels[4], depth[3], stride=2, dropout_rate=dropout_rate)\n",
        "\n",
        "        # Final layers\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(nChannels[4], num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride, dropout_rate):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock3D(in_channels, out_channels, stride, dropout_rate))\n",
        "\n",
        "        for i in range(1, num_blocks):\n",
        "            layers.append(BasicBlock3D(out_channels, out_channels, 1, dropout_rate))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.01)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlock3D(nn.Module):\n",
        "    \"\"\"Basic 3D residual block\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.0):\n",
        "        super(BasicBlock3D, self).__init__()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm3d(in_channels)\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3,\n",
        "                              stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3,\n",
        "                              stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout3d(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv3d(in_channels, out_channels, kernel_size=1,\n",
        "                         stride=stride, bias=False),\n",
        "                nn.BatchNorm3d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.leaky_relu(self.bn1(x), negative_slope=0.01)\n",
        "        out = self.conv1(out)\n",
        "        out = F.leaky_relu(self.bn2(out), negative_slope=0.01)\n",
        "\n",
        "        if self.dropout:\n",
        "            out = self.dropout(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out += self.shortcut(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090b535d",
      "metadata": {
        "id": "090b535d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nibabel as nib\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "# from augmentation_3d import CBCTRandAugment\n",
        "\n",
        "class NiftiDataset(Dataset):\n",
        "    \"\"\"Custom dataset for loading .nii.gz files\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, split='train', transform=None, target_size=(64, 64, 64)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: csv_file = r\"d:\\Kananat\\Data\\Last0\\labels.csv\"\n",
        "            transform: Optional transform to be applied on a sample\n",
        "            target_size: Target size to resize images to (depth, height, width)\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "        self.samples = []\n",
        "\n",
        "        # # First, collect samples for each class separately\n",
        "        # class_0_samples = []\n",
        "        # class_1_samples = []\n",
        "\n",
        "        # # Load file paths and labels\n",
        "        # for class_label in ['0', '1']:\n",
        "        #     class_dir = os.path.join(data_dir, class_label)\n",
        "        #     if os.path.exists(class_dir):\n",
        "        #         for filename in os.listdir(class_dir):\n",
        "        #             if filename.endswith('.nii.gz') or filename.endswith('.nii'):\n",
        "        #                 filepath = os.path.join(class_dir, filename)\n",
        "        #                 if class_label == '0':\n",
        "        #                     class_0_samples.append((filepath, 0))\n",
        "        #                 else:\n",
        "        #                     class_1_samples.append((filepath, 1))\n",
        "\n",
        "        df = pd.read_csv(data_dir)\n",
        "        df = df[df['split'] == split]\n",
        "\n",
        "        ID = df.values.tolist()\n",
        "\n",
        "        for id in ID:\n",
        "\n",
        "            data_path = Path(data_dir).parent / f\"{id[0]}_preprocessed.nii.gz\"\n",
        "            labels = id[1:7]\n",
        "\n",
        "            if not data_path.exists():\n",
        "                print(f\"Missing file: {data_path}\")\n",
        "                continue\n",
        "\n",
        "            self.samples.append((str(data_path), labels))\n",
        "\n",
        "        # self.samples = class_0_samples + class_1_samples\n",
        "        random.shuffle(self.samples)\n",
        "\n",
        "        print(f\"Total samples: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filepath, label = self.samples[idx]\n",
        "\n",
        "        label = torch.FloatTensor(label)\n",
        "\n",
        "        # Load NIfTI image\n",
        "        nii_img = nib.load(filepath)\n",
        "        image = nii_img.get_fdata()\n",
        "\n",
        "        # Handle different input dimensions\n",
        "        if len(image.shape) == 4:\n",
        "            # If 4D, take the first volume\n",
        "            image = image[:, :, :, 0]\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        image = self._normalize_image(image)\n",
        "\n",
        "        # Resize if needed\n",
        "        if image.shape != self.target_size:\n",
        "            image = self._resize_image(image, self.target_size)\n",
        "\n",
        "        # Convert to tensor and add channel dimension\n",
        "        image = torch.from_numpy(image).float().unsqueeze(0)  # Shape: (1, D, H, W)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def _normalize_image(self, image):\n",
        "        \"\"\"Normalize image to [0, 1] range\"\"\"\n",
        "        # Remove NaN and infinity values\n",
        "        # image = np.nan_to_num(image, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        # # Normalize to [0, 1]\n",
        "        # min_val = np.min(image)\n",
        "        # max_val = np.max(image)\n",
        "        # if max_val > min_val:\n",
        "        #     image = (image - min_val) / (max_val - min_val)\n",
        "        # else:\n",
        "        #     image = np.zeros_like(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _resize_image(self, image, target_size):\n",
        "        \"\"\"Resize image using trilinear interpolation\"\"\"\n",
        "        # # Convert to tensor for interpolation\n",
        "        # image_tensor = torch.from_numpy(image).float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # # Resize using trilinear interpolation\n",
        "        # resized = F.interpolate(\n",
        "        #     image_tensor,\n",
        "        #     size=target_size,\n",
        "        #     mode='trilinear',\n",
        "        #     align_corners=False\n",
        "        # )\n",
        "\n",
        "        # return resized.squeeze(0).squeeze(0).numpy()\n",
        "        return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yyubXKRLpnMe",
      "metadata": {
        "id": "yyubXKRLpnMe"
      },
      "outputs": [],
      "source": [
        "class MultiLabelTrainer:\n",
        "    \"\"\"Training class for multilabel classification\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, num_classes, save_dir='checkpoints', mixed_precision=False, threshold=0.5):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes  # Add number of classes\n",
        "        self.threshold = threshold  # Threshold for converting logits to predictions\n",
        "        self.save_dir = save_dir\n",
        "        self.mixed_precision = mixed_precision\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        # Change: Store different metrics for multilabel\n",
        "        self.train_f1_scores = []\n",
        "        self.val_f1_scores = []\n",
        "        self.train_precision_scores = []\n",
        "        self.val_precision_scores = []\n",
        "        self.train_recall_scores = []\n",
        "        self.val_recall_scores = []\n",
        "        self.learning_rates = []\n",
        "\n",
        "        # Initialize mixed precision training (unchanged)\n",
        "        if self.mixed_precision and device.type == 'cuda':\n",
        "            self.scaler = torch.amp.GradScaler('cuda')\n",
        "            print(\"Mixed precision training enabled\")\n",
        "        else:\n",
        "            self.scaler = None\n",
        "            if self.mixed_precision:\n",
        "                print(\"Mixed precision requested but CUDA not available\")\n",
        "\n",
        "        # Initialize CSV file with multilabel metrics\n",
        "        self.csv_path = os.path.join(save_dir, 'training_metrics.csv')\n",
        "        self.init_csv()\n",
        "\n",
        "    def init_csv(self):\n",
        "        \"\"\"Initialize CSV file with headers for multilabel metrics\"\"\"\n",
        "        df = pd.DataFrame(columns=['epoch', 'train_loss', 'train_f1', 'train_precision', 'train_recall',\n",
        "                                  'val_loss', 'val_f1', 'val_precision', 'val_recall', 'lr'])\n",
        "        df.to_csv(self.csv_path, index=False)\n",
        "\n",
        "    def save_metrics_to_csv(self, epoch, train_loss, train_f1, train_prec, train_rec,\n",
        "                           val_loss, val_f1, val_prec, val_rec, lr):\n",
        "        \"\"\"Save metrics for current epoch to CSV\"\"\"\n",
        "        new_row = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_f1': train_f1,\n",
        "            'train_precision': train_prec,\n",
        "            'train_recall': train_rec,\n",
        "            'val_loss': val_loss,\n",
        "            'val_f1': val_f1,\n",
        "            'val_precision': val_prec,\n",
        "            'val_recall': val_rec,\n",
        "            'lr': lr\n",
        "        }\n",
        "        df = pd.DataFrame([new_row])\n",
        "        df.to_csv(self.csv_path, mode='a', header=False, index=False)\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=50, lr=0.001, weight_decay=1e-4):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        \n",
        "        # CHANGE: Use BCEWithLogitsLoss for multilabel\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                        factor=0.8, patience=5)\n",
        "\n",
        "        best_val_f1 = 0.0  # Changed from best_val_acc to F1\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "            print('-' * 50)\n",
        "\n",
        "            # Training phase\n",
        "            train_metrics = self._train_epoch(train_loader, criterion, optimizer)\n",
        "\n",
        "            # Validation phase\n",
        "            val_metrics = self._validate_epoch(val_loader, criterion)\n",
        "\n",
        "            # Get current learning rate\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            # Update learning rate\n",
        "            old_lr = optimizer.param_groups[0]['lr']\n",
        "            scheduler.step(val_metrics['loss'])\n",
        "            new_lr = optimizer.param_groups[0]['lr']\n",
        "            if new_lr != old_lr:\n",
        "                print(f'Learning rate reduced from {old_lr:.6f} to {new_lr:.6f}')\n",
        "\n",
        "            # Save metrics\n",
        "            self.train_losses.append(train_metrics['loss'])\n",
        "            self.val_losses.append(val_metrics['loss'])\n",
        "            self.train_f1_scores.append(train_metrics['f1'])\n",
        "            self.val_f1_scores.append(val_metrics['f1'])\n",
        "            self.train_precision_scores.append(train_metrics['precision'])\n",
        "            self.val_precision_scores.append(val_metrics['precision'])\n",
        "            self.train_recall_scores.append(train_metrics['recall'])\n",
        "            self.val_recall_scores.append(val_metrics['recall'])\n",
        "            self.learning_rates.append(current_lr)\n",
        "\n",
        "            # Save metrics to CSV\n",
        "            self.save_metrics_to_csv(\n",
        "                epoch, train_metrics['loss'], train_metrics['f1'], train_metrics['precision'], train_metrics['recall'],\n",
        "                val_metrics['loss'], val_metrics['f1'], val_metrics['precision'], val_metrics['recall'], current_lr\n",
        "            )\n",
        "\n",
        "            print(f'Train Loss: {train_metrics[\"loss\"]:.4f}, F1: {train_metrics[\"f1\"]:.4f}, '\n",
        "                  f'Precision: {train_metrics[\"precision\"]:.4f}, Recall: {train_metrics[\"recall\"]:.4f}')\n",
        "            print(f'Val Loss: {val_metrics[\"loss\"]:.4f}, F1: {val_metrics[\"f1\"]:.4f}, '\n",
        "                  f'Precision: {val_metrics[\"precision\"]:.4f}, Recall: {val_metrics[\"recall\"]:.4f}')\n",
        "            print(f'Learning Rate: {current_lr:.6f}')\n",
        "\n",
        "            # Save best model based on F1 score\n",
        "            if val_metrics['f1'] >= best_val_f1:\n",
        "                best_val_f1 = val_metrics['f1']\n",
        "                self.save_checkpoint(epoch, val_metrics['f1'], 'best_model_F1.pth')\n",
        "                print(f'New best model saved with Val F1: {val_metrics[\"f1\"]:.4f}')\n",
        "\n",
        "            if val_metrics['loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['loss']\n",
        "                self.save_checkpoint(epoch, val_metrics['f1'], 'best_model_Loss.pth')\n",
        "                print(f'New best model saved with Val Loss: {val_metrics[\"loss\"]:.4f}')\n",
        "\n",
        "            # Save regular checkpoint\n",
        "            if (epoch + 1) % 25 == 0:\n",
        "                self.save_checkpoint(epoch, val_metrics['f1'], f'checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "        print(f'\\nTraining completed. Best validation F1: {best_val_f1:.4f}')\n",
        "        print(f'Training metrics saved to: {self.csv_path}')\n",
        "        return best_val_f1\n",
        "\n",
        "    def _train_epoch(self, train_loader, criterion, optimizer):\n",
        "        \"\"\"Train for one epoch - modified for multilabel\"\"\"\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
        "            \n",
        "            # IMPORTANT: Ensure target is float for BCEWithLogitsLoss\n",
        "            target = target.float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            if self.scaler is not None:\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    output = self.model(data)\n",
        "                    loss = criterion(output, target)\n",
        "\n",
        "                # Mixed precision backward pass\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                # Standard precision\n",
        "                output = self.model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            # CHANGE: Convert logits to predictions using sigmoid and threshold\n",
        "            with torch.no_grad():\n",
        "                predictions = (torch.sigmoid(output) > self.threshold).float()\n",
        "                all_predictions.append(predictions.cpu().numpy())\n",
        "                all_targets.append(target.cpu().numpy())\n",
        "\n",
        "            # Calculate batch metrics\n",
        "            batch_f1 = self._calculate_batch_metrics(predictions, target)\n",
        "            \n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
        "                'F1': f'{batch_f1:.3f}',\n",
        "                'GPU_Mem': f'{torch.cuda.memory_allocated()/1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'\n",
        "            })\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        all_predictions = np.vstack(all_predictions)\n",
        "        all_targets = np.vstack(all_targets)\n",
        "        metrics = self._calculate_multilabel_metrics(all_predictions, all_targets)\n",
        "        metrics['loss'] = running_loss / len(train_loader)\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def _validate_epoch(self, val_loader, criterion):\n",
        "        \"\"\"Validate for one epoch - modified for multilabel\"\"\"\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_loader, desc='Validation')\n",
        "\n",
        "            for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
        "                target = target.float()\n",
        "\n",
        "                # Mixed precision inference\n",
        "                if self.scaler is not None:\n",
        "                    with torch.amp.autocast('cuda'):\n",
        "                        output = self.model(data)\n",
        "                        loss = criterion(output, target)\n",
        "                else:\n",
        "                    output = self.model(data)\n",
        "                    loss = criterion(output, target)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                \n",
        "                # Convert logits to predictions\n",
        "                predictions = (torch.sigmoid(output) > self.threshold).float()\n",
        "                all_predictions.append(predictions.cpu().numpy())\n",
        "                all_targets.append(target.cpu().numpy())\n",
        "                \n",
        "                # Calculate batch metrics\n",
        "                batch_f1 = self._calculate_batch_metrics(predictions, target)\n",
        "\n",
        "                progress_bar.set_postfix({\n",
        "                    'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
        "                    'F1': f'{batch_f1:.3f}',\n",
        "                    'GPU_Mem': f'{torch.cuda.memory_allocated()/1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'\n",
        "                })\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        all_predictions = np.vstack(all_predictions)\n",
        "        all_targets = np.vstack(all_targets)\n",
        "        metrics = self._calculate_multilabel_metrics(all_predictions, all_targets)\n",
        "        metrics['loss'] = running_loss / len(val_loader)\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def _calculate_batch_metrics(self, predictions, targets):\n",
        "        \"\"\"Calculate F1 score for a batch\"\"\"\n",
        "        from sklearn.metrics import f1_score\n",
        "        pred_np = predictions.cpu().numpy() if torch.is_tensor(predictions) else predictions\n",
        "        target_np = targets.cpu().numpy() if torch.is_tensor(targets) else targets\n",
        "        return f1_score(target_np, pred_np, average='macro', zero_division=0)\n",
        "\n",
        "    def _calculate_multilabel_metrics(self, predictions, targets):\n",
        "        \"\"\"Calculate comprehensive multilabel metrics\"\"\"\n",
        "        from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss\n",
        "        \n",
        "        metrics = {\n",
        "            'f1': f1_score(targets, predictions, average='macro', zero_division=0),\n",
        "            'precision': precision_score(targets, predictions, average='macro', zero_division=0),\n",
        "            'recall': recall_score(targets, predictions, average='macro', zero_division=0),\n",
        "            'f1_micro': f1_score(targets, predictions, average='micro', zero_division=0),\n",
        "            'f1_samples': f1_score(targets, predictions, average='samples', zero_division=0),\n",
        "            'hamming_loss': hamming_loss(targets, predictions)\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        \"\"\"Evaluate on test set - modified for multilabel\"\"\"\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(test_loader, desc='Testing'):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                target = target.float()\n",
        "                \n",
        "                output = self.model(data)\n",
        "                probs = torch.sigmoid(output)\n",
        "                predictions = (probs > self.threshold).float()\n",
        "\n",
        "                all_preds.append(predictions.cpu().numpy())\n",
        "                all_targets.append(target.cpu().numpy())\n",
        "                all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "        all_preds = np.vstack(all_preds)\n",
        "        all_targets = np.vstack(all_targets)\n",
        "        all_probs = np.vstack(all_probs)\n",
        "\n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "        \n",
        "        metrics = self._calculate_multilabel_metrics(all_preds, all_targets)\n",
        "        \n",
        "        print(f'\\nTest Metrics:')\n",
        "        print(f'F1 Score (macro): {metrics[\"f1\"]:.4f}')\n",
        "        print(f'F1 Score (micro): {metrics[\"f1_micro\"]:.4f}')\n",
        "        print(f'F1 Score (samples): {metrics[\"f1_samples\"]:.4f}')\n",
        "        print(f'Precision (macro): {metrics[\"precision\"]:.4f}')\n",
        "        print(f'Recall (macro): {metrics[\"recall\"]:.4f}')\n",
        "        print(f'Hamming Loss: {metrics[\"hamming_loss\"]:.4f}')\n",
        "        \n",
        "        # Per-class metrics\n",
        "        report = classification_report(all_targets, all_preds, target_names=[f'Class_{i}' for i in range(self.num_classes)])\n",
        "        print('\\nPer-Class Classification Report:')\n",
        "        print(report)\n",
        "        \n",
        "        # Confusion matrices for each class\n",
        "        cm = multilabel_confusion_matrix(all_targets, all_preds)\n",
        "        \n",
        "        return metrics, report, cm, all_probs\n",
        "\n",
        "    def plot_training_curves(self):\n",
        "        \"\"\"Plot training and validation curves - modified for multilabel metrics\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        epochs = range(1, len(self.train_losses) + 1)\n",
        "\n",
        "        # Loss curves\n",
        "        axes[0].plot(epochs, self.train_losses, label='Train Loss')\n",
        "        axes[0].plot(epochs, self.val_losses, label='Val Loss')\n",
        "        axes[0].set_title('Training and Validation Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True)\n",
        "\n",
        "        # F1 curves\n",
        "        axes[1].plot(epochs, self.train_f1_scores, label='Train F1')\n",
        "        axes[1].plot(epochs, self.val_f1_scores, label='Val F1')\n",
        "        axes[1].set_title('F1 Score')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('F1 Score')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True)\n",
        "\n",
        "        # Precision curves\n",
        "        axes[2].plot(epochs, self.train_precision_scores, label='Train Precision')\n",
        "        axes[2].plot(epochs, self.val_precision_scores, label='Val Precision')\n",
        "        axes[2].set_title('Precision')\n",
        "        axes[2].set_xlabel('Epoch')\n",
        "        axes[2].set_ylabel('Precision')\n",
        "        axes[2].legend()\n",
        "        axes[2].grid(True)\n",
        "\n",
        "        # Recall curves\n",
        "        axes[3].plot(epochs, self.train_recall_scores, label='Train Recall')\n",
        "        axes[3].plot(epochs, self.val_recall_scores, label='Val Recall')\n",
        "        axes[3].set_title('Recall')\n",
        "        axes[3].set_xlabel('Epoch')\n",
        "        axes[3].set_ylabel('Recall')\n",
        "        axes[3].legend()\n",
        "        axes[3].grid(True)\n",
        "\n",
        "        # Learning rate curve\n",
        "        axes[4].plot(epochs, self.learning_rates, label='Learning Rate', color='orange')\n",
        "        axes[4].set_title('Learning Rate Schedule')\n",
        "        axes[4].set_xlabel('Epoch')\n",
        "        axes[4].set_ylabel('Learning Rate')\n",
        "        axes[4].set_yscale('log')\n",
        "        axes[4].legend()\n",
        "        axes[4].grid(True)\n",
        "\n",
        "        # Hide the last subplot\n",
        "        axes[5].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.save_dir, 'training_curves.png'))\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qWdqZt0-dfcD",
      "metadata": {
        "id": "qWdqZt0-dfcD"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'data_dir': r\"d:\\Kananat\\Data\\Last0\\3_Preprocessed\\labels.csv\",  # Update this path\n",
        "        'save_dir': r'/content/drive/MyDrive/TMJOA_data/3D_model/hopefully_last/ResNet/OA_noMONAI',\n",
        "        'target_size': (256, 256, 256),      # Resize from 255x255x255 to 64x64x64\n",
        "        'batch_size': 8,                  # Small batch size due to large images\n",
        "        'num_epochs': 200,\n",
        "        'learning_rate': 0.001,\n",
        "        'weight_decay': 1e-4,\n",
        "        'num_workers': 0,                 # Set to 0 for Windows compatibility\n",
        "        'mixed_precision': True,          # Enable mixed precision training\n",
        "    }\n",
        "\n",
        "    # Create model\n",
        "    model = ModifiedWideResNet3D(\n",
        "        width=2,\n",
        "        depth=[2, 2, 2, 2],\n",
        "        num_classes=5,\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "    # ########\n",
        "    # Continue training\n",
        "\n",
        "    # checkpoint = torch.load('/content/best_model.pth', map_location='cuda')\n",
        "    # model = ModifiedWideResNet3D(\n",
        "    #     input_size=config['target_size'],\n",
        "    #     width=2,\n",
        "    #     num_classes=2,\n",
        "    #     dropout_rate=0.1\n",
        "    # )\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # model = model.to(device)\n",
        "    # model.train()\n",
        "    # #########\n",
        "\n",
        "    # Device configuration with optimization\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "        print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
        "        print(f'CUDA Version: {torch.version.cuda}')\n",
        "\n",
        "        # Enable cudnn benchmarking for consistent input sizes\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    else:\n",
        "        print('CUDA not available, using CPU')\n",
        "        config['mixed_precision'] = False\n",
        "\n",
        "    # Data transforms\n",
        "    train_transform = CBCTRandAugment(n=2, m=6, use_monai=True)\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = NiftiDataset(config['data_dir'], 'train',\n",
        "        transform=train_transform,\n",
        "        target_size=config['target_size']\n",
        "    )\n",
        "\n",
        "    val_dataset = NiftiDataset(config['data_dir'], 'val',\n",
        "        transform=None,\n",
        "        target_size=config['target_size']\n",
        "    )\n",
        "\n",
        "    test_dataset = NiftiDataset(config['data_dir'], 'test',\n",
        "        transform=None,\n",
        "        target_size=config['target_size']\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=config['num_workers'],\n",
        "        persistent_workers=True if torch.cuda.is_available() else False,\n",
        "        pin_memory=True if torch.cuda.is_available() else False,\n",
        "        prefetch_factor=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=config['num_workers'],\n",
        "        persistent_workers=True if torch.cuda.is_available() else False,\n",
        "        pin_memory=True if torch.cuda.is_available() else False,\n",
        "        prefetch_factor=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=config['num_workers'],\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Train samples: {len(train_dataset)}\")\n",
        "    print(f\"Val samples: {len(val_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "    # Print model info\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model created with {total_params:,} parameters\")\n",
        "\n",
        "\n",
        "    trainer = MultiLabelTrainer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    save_dir=config['save_dir'],\n",
        "    mixed_precision=config['mixed_precision'],\n",
        "    num_classes=10,  # number of labels\n",
        "    threshold=0.5  # can be tuned\n",
        ")\n",
        "\n",
        "    # Train model\n",
        "    print(\"Starting training...\")\n",
        "    best_val_acc = trainer.train(\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        num_epochs=config['num_epochs'],\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Plot training curves\n",
        "    trainer.plot_training_curves()\n",
        "\n",
        "    # Load best model and evaluate\n",
        "    print(\"Loading best model for final evaluation...\")\n",
        "    trainer.load_checkpoint('best_model_Loss.pth')\n",
        "    test_acc, test_report, test_cm = trainer.evaluate(test_loader)\n",
        "\n",
        "    # Save final results\n",
        "    results = {\n",
        "        'best_val_accuracy': best_val_acc,\n",
        "        'test_accuracy': test_acc,\n",
        "        'config': config,\n",
        "        'model_parameters': total_params,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(trainer.save_dir, 'results.json'), 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"Training completed! Final test accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JcMCmEkHGCS8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JcMCmEkHGCS8",
        "outputId": "815fdd45-9a4f-4f3b-814f-444976f26563"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "2dmodelGPU",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
