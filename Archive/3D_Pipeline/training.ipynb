{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a288b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs:\", len(tf.config.list_physical_devices('GPU'))>0)\n",
    "print(\"CUDA available:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from classification_models_3D.kkeras import Classifiers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.layers import Dropout, Dense, GlobalAveragePooling3D\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97568988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_PATH = r\"/tmjoa_3d/Data/TMJOA/training_dataset\"  # Update this path\n",
    "TRAIN_DIR = os.path.join(DATA_PATH, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_PATH, \"val\")  # or \"validation\" if that's your folder name\n",
    "TEST_DIR = os.path.join(DATA_PATH, \"test\")\n",
    "\n",
    "# Model configuration\n",
    "BACKBONE = 'resnet18'  # You can change this to any model from the library\n",
    "INPUT_SHAPE = (224, 224, 224, 1)  # Single channel for your .nii.gz files\n",
    "NUM_CLASSES = 2\n",
    "USE_WEIGHTS = None  # No pre-trained weights - training from scratch\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 8  # Adjust based on your GPU memory\n",
    "LEARNING_RATE = 0.001  # Slightly higher learning rate for training from scratch\n",
    "PATIENCE = 100  # Increased patience since training from scratch takes longer\n",
    "EPOCHS = 500  # More epochs needed when training from scratch\n",
    "\n",
    "# Data augmentation (optional but recommended when training from scratch)\n",
    "ENABLE_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nii_file(file_path):\n",
    "    \"\"\"Load .nii.gz file and return numpy array\"\"\"\n",
    "    try:\n",
    "        img = nib.load(file_path)\n",
    "        data = img.get_fdata()\n",
    "        # print(f\"Loaded {file_path}: shape {data.shape}\")  # Debug print\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_volume(volume):\n",
    "    \"\"\"Normalize volume to [0, 1] range\"\"\"\n",
    "    volume = volume.astype(np.float32)\n",
    "    # Clip extreme values (optional)\n",
    "    volume = np.clip(volume, np.percentile(volume, 1), np.percentile(volume, 99))\n",
    "    # Normalize to [0, 1]\n",
    "    volume = (volume - volume.min()) / (volume.max() - volume.min() + 1e-8)\n",
    "    return volume\n",
    "\n",
    "def preprocess_volume(volume):\n",
    "    \"\"\"Apply preprocessing steps to volume\"\"\"\n",
    "    if volume is None:\n",
    "        print(\"Warning: Volume is None\")\n",
    "        return np.zeros((224, 224, 224, 1), dtype=np.float32)\n",
    "    \n",
    "    # Print debug info\n",
    "    #print(f\"Input volume shape: {volume.shape}\")\n",
    "    \n",
    "    # Ensure volume is 3D\n",
    "    if len(volume.shape) == 4:\n",
    "        # If 4D, take the first volume (assuming time series or multi-channel)\n",
    "        volume = volume[:, :, :, 0]\n",
    "    elif len(volume.shape) != 3:\n",
    "        print(f\"Warning: Unexpected volume shape {volume.shape}\")\n",
    "        return np.zeros((224, 224, 224, 1), dtype=np.float32)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if volume.shape != (224, 224, 224):\n",
    "        from scipy import ndimage\n",
    "        target_shape = (224, 224, 224)\n",
    "        zoom_factors = [t/s for t, s in zip(target_shape, volume.shape)]\n",
    "        volume = ndimage.zoom(volume, zoom_factors, order=1)\n",
    "        #print(f\"Resized volume to: {volume.shape}\")\n",
    "    \n",
    "    # Normalize\n",
    "    volume = normalize_volume(volume)\n",
    "    \n",
    "    # Add single channel dimension\n",
    "    volume = np.expand_dims(volume, axis=-1)\n",
    "    #print(f\"Final volume shape: {volume.shape}\")\n",
    "    \n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiiDataset:\n",
    "    def __init__(self, data_dir, classes, preprocess_fn=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = classes\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self._load_file_list()\n",
    "    \n",
    "    def _load_file_list(self):\n",
    "        \"\"\"Load list of files and their labels\"\"\"\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for file in os.listdir(class_dir):\n",
    "                    if file.endswith('.nii') or file.endswith('.nii.gz'):\n",
    "                        self.files.append(os.path.join(class_dir, file))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "        print(f\"Found {len(self.files)} files in {len(self.classes)} classes\")\n",
    "        for idx, class_name in enumerate(self.classes):\n",
    "            count = sum(1 for label in self.labels if label == idx)\n",
    "            print(f\"  Class {class_name}: {count} files\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load volume\n",
    "        volume = load_nii_file(file_path)\n",
    "        if volume is None:\n",
    "            # Return dummy data if loading fails\n",
    "            volume = np.zeros((224, 224, 224, 1))\n",
    "        else:\n",
    "            # Preprocess volume\n",
    "            if self.preprocess_fn:\n",
    "                volume = self.preprocess_fn(volume)\n",
    "        \n",
    "        return volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_volume(volume):\n",
    "    \"\"\"Apply random augmentations to volume - simplified version\"\"\"\n",
    "    # Make sure volume is 3D\n",
    "    if len(volume.shape) != 3:\n",
    "        print(f\"Warning: Expected 3D volume for augmentation, got shape {volume.shape}\")\n",
    "        return volume\n",
    "    \n",
    "    # Random flip\n",
    "    if np.random.random() > 0.5:\n",
    "        axis = np.random.choice(3)\n",
    "        volume = np.flip(volume, axis=axis)\n",
    "    \n",
    "    # Random brightness adjustment\n",
    "    if np.random.random() > 0.5:\n",
    "        brightness = np.random.uniform(0.8, 1.2)\n",
    "        volume = volume * brightness\n",
    "        volume = np.clip(volume, 0, 1)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    if np.random.random() > 0.5:\n",
    "        noise_std = np.random.uniform(0.0, 0.01)\n",
    "        noise = np.random.normal(0, noise_std, volume.shape)\n",
    "        volume = volume + noise\n",
    "        volume = np.clip(volume, 0, 1)\n",
    "    \n",
    "    return volume\n",
    "\n",
    "print(\"Augmentation functions updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your class names\n",
    "CLASS_NAMES = [\"0\", \"1\"]  # Update these to match your dataset structure\n",
    "\n",
    "# Create datasets (just for getting file lists, not loading data)\n",
    "print(\"Creating file lists...\")\n",
    "train_dataset = NiiDataset(TRAIN_DIR, CLASS_NAMES, preprocess_fn=None)\n",
    "val_dataset = NiiDataset(VAL_DIR, CLASS_NAMES, preprocess_fn=None)\n",
    "test_dataset = NiiDataset(TEST_DIR, CLASS_NAMES, preprocess_fn=None)\n",
    "\n",
    "# Test loading a single volume to check shapes\n",
    "print(\"\\nTesting volume loading...\")\n",
    "if len(train_dataset) > 0:\n",
    "    test_volume, test_label = train_dataset[0]\n",
    "    print(f\"Raw volume shape: {test_volume.shape}\")\n",
    "    processed_volume = preprocess_volume(test_volume)\n",
    "    print(f\"Processed volume shape: {processed_volume.shape}\")\n",
    "    print(f\"Expected shape: {INPUT_SHAPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37923bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical, Sequence\n",
    "\n",
    "class NiiDataGenerator(Sequence):\n",
    "    def __init__(self, dataset, batch_size, preprocess_fn, preprocess_input_fn, \n",
    "                 num_classes, shuffle=True, augment=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.preprocess_input_fn = preprocess_input_fn\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.indices = np.arange(len(self.dataset))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataset) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Initialize batch arrays\n",
    "        X = np.zeros((len(batch_indices), *INPUT_SHAPE), dtype=np.float32)\n",
    "        y = np.zeros((len(batch_indices), self.num_classes), dtype=np.float32)\n",
    "        \n",
    "        # Load and preprocess each sample in batch\n",
    "        for i, idx in enumerate(batch_indices):\n",
    "            try:\n",
    "                # Load volume and label\n",
    "                volume, label = self.dataset[idx]\n",
    "                \n",
    "                # Apply preprocessing\n",
    "                if self.preprocess_fn:\n",
    "                    volume = self.preprocess_fn(volume)\n",
    "                \n",
    "                # Ensure volume has correct shape\n",
    "                if volume.shape != INPUT_SHAPE:\n",
    "                    print(f\"Warning: Volume shape {volume.shape} doesn't match expected {INPUT_SHAPE}\")\n",
    "                    volume = np.zeros(INPUT_SHAPE, dtype=np.float32)\n",
    "                \n",
    "                # Apply augmentation if enabled (only during training)\n",
    "                if self.augment and ENABLE_AUGMENTATION:\n",
    "                    # Remove channel dimension for augmentation\n",
    "                    vol_for_aug = volume[:, :, :, 0]\n",
    "                    vol_for_aug = augment_volume(vol_for_aug)\n",
    "                    # Add channel dimension back\n",
    "                    volume = np.expand_dims(vol_for_aug, axis=-1)\n",
    "                \n",
    "                # Apply model preprocessing - handle single volume\n",
    "                volume_expanded = np.expand_dims(volume, axis=0)\n",
    "                volume_processed = self.preprocess_input_fn(volume_expanded)[0]\n",
    "                \n",
    "                # Store in batch\n",
    "                X[i] = volume_processed\n",
    "                y[i] = to_categorical(label, self.num_classes)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                # Use zeros for failed samples\n",
    "                X[i] = np.zeros(INPUT_SHAPE, dtype=np.float32)\n",
    "                y[i] = to_categorical(0, self.num_classes)  # Default to class 0\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "print(\"Data generator class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model and preprocessing function\n",
    "modelPoint, preprocess_input = Classifiers.get(BACKBONE)\n",
    "\n",
    "# Create base model without pre-trained weights\n",
    "base_model = modelPoint(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    include_top=False,\n",
    "    weights=None,  # No pre-trained weights\n",
    ")\n",
    "\n",
    "# Add custom head for your classification task\n",
    "x = base_model.layers[-1].output\n",
    "x = GlobalAveragePooling3D()(x)\n",
    "\n",
    "# Add more dropout and regularization since we're training from scratch\n",
    "x = Dropout(0.6)(x)  # Higher dropout\n",
    "x = Dense(256, activation='relu', kernel_regularizer='l2')(x)  # Added L2 regularization\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(128, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(NUM_CLASSES, activation='softmax' if NUM_CLASSES > 2 else 'sigmoid')(x)\n",
    "\n",
    "# Create final model\n",
    "model = Model(inputs=base_model.inputs, outputs=x)\n",
    "\n",
    "# Compile model with different optimizer settings for training from scratch\n",
    "from keras.optimizers import Adam\n",
    "optimizer = Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07\n",
    ")\n",
    "\n",
    "loss = 'categorical_crossentropy' if NUM_CLASSES > 2 else 'binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25613bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "print(\"Creating data generators...\")\n",
    "\n",
    "# Training generator with augmentation\n",
    "train_generator = NiiDataGenerator(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    preprocess_fn=preprocess_volume,\n",
    "    preprocess_input_fn=preprocess_input,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    shuffle=True,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "# Validation generator without augmentation\n",
    "val_generator = NiiDataGenerator(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    preprocess_fn=preprocess_volume,\n",
    "    preprocess_input_fn=preprocess_input,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Test generator without augmentation\n",
    "test_generator = NiiDataGenerator(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    preprocess_fn=preprocess_volume,\n",
    "    preprocess_input_fn=preprocess_input,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "print(f\"Created generators:\")\n",
    "print(f\"  Train batches: {len(train_generator)}\")\n",
    "print(f\"  Validation batches: {len(val_generator)}\")\n",
    "print(f\"  Test batches: {len(test_generator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks optimized for training from scratch\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # More aggressive learning rate reduction for training from scratch\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,  # More aggressive reduction\n",
    "        patience=PATIENCE//3,  # Reduce patience\n",
    "        min_lr=1e-8,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    CSVLogger('training_log.csv'),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=PATIENCE,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True,\n",
    "        mode='min'\n",
    "    ),\n",
    "    # Add learning rate scheduler for better training from scratch\n",
    "    tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: LEARNING_RATE * (0.95 ** epoch),\n",
    "        verbose=0\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using generators\n",
    "print(\"Starting training with data generators...\")\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Training completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"Checking first batch...\")\n",
    "    # Try to get first batch to debug\n",
    "    try:\n",
    "        X_batch, y_batch = train_generator[0]\n",
    "        print(f\"Batch shapes: X={X_batch.shape}, y={y_batch.shape}\")\n",
    "    except Exception as batch_error:\n",
    "        print(f\"Batch error: {batch_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a28a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df59786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_weights('best_model.keras')\n",
    "\n",
    "# Evaluate on test set using generator\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get predictions for all test samples\n",
    "print(\"Getting predictions...\")\n",
    "y_pred = model.predict(test_generator, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels (need to iterate through test generator)\n",
    "y_true = []\n",
    "test_generator.shuffle = False  # Ensure consistent order\n",
    "for i in range(len(test_generator)):\n",
    "    _, batch_y = test_generator[i]\n",
    "    y_true.extend(np.argmax(batch_y, axis=1))\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Trim predictions to match true labels (in case of incomplete last batch)\n",
    "y_pred_classes = y_pred_classes[:len(y_true)]\n",
    "\n",
    "# Generate classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929268f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Confusion matrix for test set\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Test Set Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(CLASS_NAMES))\n",
    "plt.xticks(tick_marks, CLASS_NAMES, rotation=45)\n",
    "plt.yticks(tick_marks, CLASS_NAMES)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save('final_3d_model.keras')\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"Training history saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_volume(model, file_path):\n",
    "    \"\"\"Predict class for a single .nii.gz file\"\"\"\n",
    "    # Load and preprocess volume\n",
    "    volume = load_nii_file(file_path)\n",
    "    if volume is None:\n",
    "        return None\n",
    "    \n",
    "    volume = preprocess_volume(volume)\n",
    "    volume = preprocess_input(np.expand_dims(volume, axis=0))\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(volume, verbose=0)\n",
    "    pred_class = np.argmax(pred, axis=1)[0]\n",
    "    confidence = pred[0][pred_class]\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': CLASS_NAMES[pred_class],\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {name: prob for name, prob in zip(CLASS_NAMES, pred[0])}\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = predict_single_volume(model, \"/tmjoa_3d/data/5_adjustedBG/63-700385 R_adjustedBG.nii.gz\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f56af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the best model (recommended)\n",
    "model = load_model('best_model.keras')\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(model.summary())\n",
    "\n",
    "# Or load the final model\n",
    "model = load_model('final_3d_model.keras')\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
