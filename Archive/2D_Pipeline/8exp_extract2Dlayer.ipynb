{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def file_to_ndarray(filepath):\n",
    "    # Check the file extension\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "    #print(file_extension)\n",
    "    \n",
    "    try:\n",
    "        if file_extension in ['.nii', '.nii.gz', '.gz']:  # Handle gzipped or regular NIfTI files\n",
    "            # Load the NIfTI file\n",
    "            nii_img = nib.load(filepath)\n",
    "            # Convert to ndarray\n",
    "            data = nii_img.get_fdata()\n",
    "            #print(f\"Loaded NIfTI file: {filepath}\")\n",
    "        else:\n",
    "            print(\"Unsupported file format.\")\n",
    "            return None\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\47-4881 L 2014.nii.gz'\n",
    "\n",
    "voxel = file_to_ndarray(filepath)\n",
    "print(voxel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_array_slice(array3d, axis=0, slice_num=0):\n",
    "    \"\"\"\n",
    "    Display a 2D slice from a 3D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    array3d : numpy.ndarray\n",
    "        Input 3D array\n",
    "    axis : int\n",
    "        Axis along which to take the slice (0, 1, or 2)\n",
    "    slice_num : int\n",
    "        Index of the slice to display\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(array3d, np.ndarray) or array3d.ndim != 3:\n",
    "        raise ValueError(\"Input must be a 3D numpy array\")\n",
    "    \n",
    "    if axis not in [0, 1, 2]:\n",
    "        raise ValueError(\"Axis must be 0, 1, or 2\")\n",
    "    \n",
    "    # Get the maximum valid slice number for the chosen axis\n",
    "    max_slice = array3d.shape[axis] - 1\n",
    "    if slice_num > max_slice:\n",
    "        raise ValueError(f\"Slice number must be between 0 and {max_slice} for axis {axis}\")\n",
    "    \n",
    "    # Extract the slice based on the axis\n",
    "    if axis == 0:\n",
    "        slice_2d = array3d[slice_num, :, :]\n",
    "        title = f\"Slice {slice_num} along axis 0 (front to back)\"\n",
    "    elif axis == 1:\n",
    "        slice_2d = array3d[:, slice_num, :]\n",
    "        title = f\"Slice {slice_num} along axis 1 (top to bottom)\"\n",
    "    else:  # axis == 2\n",
    "        slice_2d = array3d[:, :, slice_num]\n",
    "        title = f\"Slice {slice_num} along axis 2 (left to right)\"\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(slice_2d, cmap='viridis')\n",
    "    plt.colorbar(label='Value')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display different slices\n",
    "display_array_slice(voxel, axis=0, slice_num=112)  # Show third slice along axis 0\n",
    "display_array_slice(voxel, axis=1, slice_num=112)  # Show fourth slice along axis 1\n",
    "display_array_slice(voxel, axis=2, slice_num=112)  # Show fifth slice along axis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(ndarray):\n",
    "    \n",
    "    flat_array = ndarray.flatten()\n",
    "\n",
    "    # Define the bin edges from -4000 to 4000 with a bin size of 10\n",
    "    bins = np.arange(-4000, 4001, 10)  # 2001 to include the endpoint 2000 in the last bin\n",
    "\n",
    "    # Compute histogram\n",
    "    histogram_values, bin_edges = np.histogram(flat_array, bins=bins)\n",
    "\n",
    "    # Convert histogram values to list\n",
    "    #histogram_list = histogram_values.tolist()\n",
    "\n",
    "    return histogram_values, bin_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_list, bin_edges = compute_histogram(voxel)\n",
    "print(histogram_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_histogram_peaks_normal(arr, bin_edges, variance1=1.0, variance2=1.0, height=None, distance=1, prominence=None):\n",
    "    \"\"\"\n",
    "    Plot histogram with local peaks and normal distributions centered at the two highest peaks\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : numpy.ndarray\n",
    "        1D input array of histogram heights\n",
    "    bin_edges : numpy.ndarray\n",
    "        Array of bin edges (should be len(arr) + 1)\n",
    "    variance1 : float\n",
    "        Variance for the normal distribution at the highest peak\n",
    "    variance2 : float\n",
    "        Variance for the normal distribution at the second highest peak\n",
    "    height : float or None\n",
    "        Minimum height of peaks\n",
    "    distance : int\n",
    "        Minimum horizontal distance between peaks\n",
    "    prominence : float or None\n",
    "        Minimum prominence of peaks\n",
    "    \"\"\"\n",
    "    if len(bin_edges) != len(arr) + 1:\n",
    "        raise ValueError(\"bin_edges should have length equal to arr length + 1\")\n",
    "        \n",
    "    # Find local peaks\n",
    "    peaks, properties = find_peaks(arr, height=height, distance=distance, prominence=prominence)\n",
    "    peak_heights = arr[peaks]\n",
    "    \n",
    "    # Sort peaks by height\n",
    "    peak_order = np.argsort(peak_heights)[::-1]\n",
    "    peaks_sorted = peaks[peak_order]\n",
    "    heights_sorted = peak_heights[peak_order]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot histogram bars\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    plt.bar(bin_centers, arr, width=bin_edges[1]-bin_edges[0], \n",
    "            alpha=0.5, color='blue', label='Histogram')\n",
    "    \n",
    "    # Plot peaks\n",
    "    peak_x_positions = bin_centers[peaks_sorted]\n",
    "    plt.scatter(peak_x_positions, heights_sorted, \n",
    "                c='red', s=100, label='Local Peaks')\n",
    "    \n",
    "    # Add normal distributions for top two peaks\n",
    "    colors = ['g', 'm']  # green for first peak, magenta for second\n",
    "    styles = ['--', ':']  # dashed for first peak, dotted for second\n",
    "    variances = [variance1, variance2]\n",
    "    \n",
    "    for i in range(min(2, len(peaks_sorted))):\n",
    "        peak_center = bin_centers[peaks_sorted[i]]\n",
    "        peak_height = heights_sorted[i]\n",
    "        variance = variances[i]\n",
    "        \n",
    "        # Create x range centered around the peak\n",
    "        x_normal = np.linspace(peak_center - 4*np.sqrt(variance), \n",
    "                             peak_center + 4*np.sqrt(variance), \n",
    "                             200)\n",
    "        \n",
    "        # Calculate normal distribution\n",
    "        y_normal = norm.pdf(x_normal, peak_center, np.sqrt(variance))\n",
    "        \n",
    "        # Scale the normal distribution to match peak height\n",
    "        y_normal = y_normal * (peak_height / np.max(y_normal))\n",
    "        \n",
    "        # Plot normal distribution\n",
    "        plt.plot(x_normal, y_normal, \n",
    "                color=colors[i], \n",
    "                linestyle=styles[i],\n",
    "                label=f'Normal at Peak {i+1} (σ²={variance:.1f})', \n",
    "                linewidth=2)\n",
    "        \n",
    "        # Add peak labels\n",
    "        plt.annotate(f'Peak {i+1}', \n",
    "                    (peak_center, peak_height),\n",
    "                    xytext=(5, 5),\n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('Bin Center')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram with Local Peaks and Normal Distributions')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return peaks_sorted, heights_sorted, peak_x_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_val, bin_edges = compute_histogram(voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_y = histogram_val[1:]\n",
    "input_x = bin_edges[1:]\n",
    "\n",
    "# Plot with different variances for each peak\n",
    "peaks, heights, peak_centers = plot_histogram_peaks_normal(\n",
    "    input_y[300:600],\n",
    "    input_x[300:601],\n",
    "    variance1=10000,    # Variance for highest peak\n",
    "    variance2=22000,    # Variance for second peak\n",
    "    height=0,      # No minimum height\n",
    "    distance=20,       # Minimum 5 bins between peaks\n",
    "    prominence=2000     # Minimum prominence\n",
    ")\n",
    "\n",
    "print(\"Peak bin centers:\", peak_centers)\n",
    "print(\"Peak heights:\", heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def rescale_by_probability(image, target_mean, variance):\n",
    "    \"\"\"\n",
    "    Rescale pixel values based on their probability of being the pixel of interest\n",
    "    under a normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : numpy.ndarray\n",
    "        Input grayscale image array\n",
    "    target_mean : float\n",
    "        The target pixel value (mean of the normal distribution)\n",
    "    variance : float\n",
    "        Variance of the normal distribution\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Rescaled image where each pixel value represents the probability\n",
    "        of that pixel being the pixel of interest\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    rescaled = image.copy().astype(float)\n",
    "    \n",
    "    # Calculate probability for each pixel value\n",
    "    probabilities = norm.pdf(rescaled, target_mean, np.sqrt(variance))\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    probabilities = (probabilities - probabilities.min()) / (probabilities.max() - probabilities.min())\n",
    "    \n",
    "    # Optional: Convert to uint8 for visualization (0-255)\n",
    "    rescaled = (probabilities * 255).astype(np.uint8)\n",
    "    \n",
    "    return rescaled\n",
    "\n",
    "def visualize_rescaling(original, rescaled, target_mean, variance):\n",
    "    \"\"\"\n",
    "    Visualize original and rescaled images side by side\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image\n",
    "    im1 = ax1.imshow(original, cmap='gray')\n",
    "    ax1.set_title('Original Image')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Rescaled image\n",
    "    im2 = ax2.imshow(rescaled, cmap='gray')\n",
    "    ax2.set_title(f'Probability Map\\n(mean={target_mean}, variance={variance})')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_2d = voxel[112, :, :]\n",
    "\n",
    "# Target pixel value of 100 with some variance\n",
    "target_mean = 355\n",
    "variance = 10000  # Wide variance to show the effect\n",
    "\n",
    "# Rescale the image\n",
    "rescaled_image_air = rescale_by_probability(slice_2d, target_mean, variance)\n",
    "\n",
    "# Visualize results\n",
    "visualize_rescaling(slice_2d, rescaled_image_air, target_mean, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_2d = voxel[112, :, :]\n",
    "\n",
    "# Target pixel value of 100 with some variance\n",
    "target_mean = 935\n",
    "variance = 22000  # Wide variance to show the effect\n",
    "\n",
    "# Rescale the image\n",
    "rescaled_image_bone = rescale_by_probability(slice_2d, target_mean, variance)\n",
    "\n",
    "# Visualize results\n",
    "visualize_rescaling(slice_2d, rescaled_image_bone, target_mean, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rescale_range(array, a, b):\n",
    "    \"\"\"\n",
    "    Rescale values in range [a,b] to [0,255] based on their position in the range.\n",
    "    Values outside [a,b] become 0.\n",
    "    \n",
    "    Parameters:\n",
    "    array: np.ndarray - Input array\n",
    "    a: float - Lower bound of the range\n",
    "    b: float - Upper bound of the range\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = array.copy()\n",
    "    \n",
    "    # Set values outside [a,b] to 0\n",
    "    result[result < a] = 0\n",
    "    result[result > b] = 0\n",
    "    \n",
    "    # Find values within the range\n",
    "    mask = (result >= a) & (result <= b)\n",
    "    \n",
    "    # Linear rescaling of values in range [a,b] to [0,255]\n",
    "    result[mask] = ((result[mask] - a) / (b - a)) * 255\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# array = your_array\n",
    "# rescaled = rescale_range(array, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_2d = voxel[112, :, :]\n",
    "a = 355\n",
    "b = 935\n",
    "adjust_const = 0.25\n",
    "adjust = int((b-a)*adjust_const)\n",
    "print(b-a, adjust)\n",
    "\n",
    "area_of_uncertainty = rescale_range(slice_2d, a+adjust, b-adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rescaling(slice_2d, area_of_uncertainty, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have three 2D arrays of the same shape\n",
    "# Convert them to uint8 if they aren't already\n",
    "red = rescaled_image_air.astype(np.uint8)\n",
    "green = area_of_uncertainty.astype(np.uint8)\n",
    "blue = rescaled_image_bone.astype(np.uint8)\n",
    "\n",
    "# Combine into RGB\n",
    "rgb_image = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(rgb_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(source_folder, destination_base, train_ratio=0.7, val_ratio=0.2):\n",
    "   for split in ['train', 'val', 'test']:\n",
    "       os.makedirs(os.path.join(destination_base, split), exist_ok=True)\n",
    "   \n",
    "   files = [f for f in os.listdir(source_folder) if f.endswith('.nii.gz')]\n",
    "   random.shuffle(files)\n",
    "   \n",
    "   n_files = len(files)\n",
    "   n_train = int(n_files * train_ratio)\n",
    "   n_val = int(n_files * val_ratio)\n",
    "   \n",
    "   train_files = files[:n_train]\n",
    "   val_files = files[n_train:n_train + n_val]\n",
    "   test_files = files[n_train + n_val:]\n",
    "   \n",
    "   for files, split in [(train_files, 'train'), \n",
    "                       (val_files, 'val'), \n",
    "                       (test_files, 'test')]:\n",
    "       for f in files:\n",
    "           shutil.copy2(os.path.join(source_folder, f),\n",
    "                       os.path.join(destination_base, split, f))\n",
    "\n",
    "# Usage\n",
    "source_folder = r'D:\\Kananat\\_Segmented_Preprocessed_expand5px'\n",
    "destination_folder = r'D:\\Kananat\\_dataset'\n",
    "split_dataset(source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def process_nii_files(folder_path):\n",
    "   # Initialize histogram bins\n",
    "   bins = np.arange(-1500, 2001)  # -1500 to 2000 inclusive\n",
    "   total_hist = np.zeros(len(bins)-1)\n",
    "   \n",
    "   # Process each file\n",
    "   for filename in os.listdir(folder_path):\n",
    "       if filename.endswith('.nii.gz'):\n",
    "           # Load image\n",
    "           print(filename)\n",
    "           img = nib.load(os.path.join(folder_path, filename))\n",
    "           data = img.get_fdata()\n",
    "           \n",
    "           # Calculate histogram for this image\n",
    "           hist, _ = np.histogram(data, bins=bins)\n",
    "           total_hist += hist\n",
    "           \n",
    "           # Clear memory\n",
    "           del data\n",
    "           img = None\n",
    "\n",
    "   return total_hist, bins\n",
    "\n",
    "# Usage\n",
    "folder_path = r'D:\\Kananat\\_dataset\\train'\n",
    "histogram, bin_edges = process_nii_files(folder_path)\n",
    "\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(bin_edges[:-1], histogram, width=1)\n",
    "plt.xlabel('Voxel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Voxel Value Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "\n",
    "def fit_gmm(histogram_data, bin_edges):\n",
    "   # Create dataset by repeating values according to histogram frequencies\n",
    "   data = []\n",
    "   for i in range(len(histogram_data)):\n",
    "       count = int(histogram_data[i])\n",
    "       if count > 0:\n",
    "           # Use bin edges to create uniform samples within each bin\n",
    "           samples = np.random.uniform(bin_edges[i], bin_edges[i+1], count)\n",
    "           data.extend(samples)\n",
    "   \n",
    "   data = np.array(data).reshape(-1, 1)\n",
    "   \n",
    "   # Fit GMM\n",
    "   gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "   gmm.fit(data)\n",
    "   \n",
    "   return gmm.means_.flatten(), gmm.covariances_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, variance = fit_gmm(histogram, bin_edges)\n",
    "print(mean,variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_y = histogram\n",
    "input_x = bin_edges\n",
    "\n",
    "# Plot with different variances for each peak\n",
    "peaks, heights, peak_centers = plot_histogram_peaks_normal(\n",
    "    input_y,\n",
    "    input_x,\n",
    "    variance1=23000,    # Variance for highest peak\n",
    "    variance2=34000,    # Variance for second peak\n",
    "    height=0,      # No minimum height\n",
    "    distance=500,       # Minimum 5 bins between peaks\n",
    "    prominence=50000     # Minimum prominence\n",
    ")\n",
    "\n",
    "print(\"Peak bin centers:\", peak_centers)\n",
    "print(\"Peak heights:\", heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\Kananat\\_dataset\\val\\58-42016 L.nii.gz'\n",
    "\n",
    "voxel = file_to_ndarray(filepath)\n",
    "\n",
    "slice_2d = voxel[112, :, :]\n",
    "\n",
    "# Target pixel value of 100 with some variance\n",
    "target_mean = 313\n",
    "variance = 23000  # Wide variance to show the effect\n",
    "\n",
    "# Rescale the image\n",
    "rescaled_image_air = rescale_by_probability(slice_2d, target_mean, variance)\n",
    "\n",
    "# Target pixel value of 100 with some variance\n",
    "target_mean = 875\n",
    "variance = 34000  # Wide variance to show the effect\n",
    "\n",
    "# Rescale the image\n",
    "rescaled_image_bone = rescale_by_probability(slice_2d, target_mean, variance)\n",
    "\n",
    "a = 313\n",
    "b = 875\n",
    "adjust_const = 0.25\n",
    "adjust = int((b-a)*adjust_const)\n",
    "print(b-a, adjust)\n",
    "\n",
    "area_of_uncertainty = rescale_range(slice_2d, a+adjust, b-adjust)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have three 2D arrays of the same shape\n",
    "# Convert them to uint8 if they aren't already\n",
    "red = rescaled_image_air.astype(np.uint8)\n",
    "green = area_of_uncertainty.astype(np.uint8)\n",
    "blue = rescaled_image_bone.astype(np.uint8)\n",
    "\n",
    "# Combine into RGB\n",
    "rgb_image = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(rgb_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_ndarray(filepath):\n",
    "    # Check the file extension\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "    #print(file_extension)\n",
    "    \n",
    "    try:\n",
    "        if file_extension in ['.nii', '.nii.gz', '.gz']:  # Handle gzipped or regular NIfTI files\n",
    "            # Load the NIfTI file\n",
    "            nii_img = nib.load(filepath)\n",
    "            # Convert to ndarray\n",
    "            data = nii_img.get_fdata()\n",
    "            #print(f\"Loaded NIfTI file: {filepath}\")\n",
    "        else:\n",
    "            print(\"Unsupported file format.\")\n",
    "            return None\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the file: {e}\")\n",
    "        return None\n",
    "\n",
    "def fit_gmm(histogram_data, bin_edges):\n",
    "   # Create dataset by repeating values according to histogram frequencies\n",
    "   data = []\n",
    "   for i in range(len(histogram_data)):\n",
    "       count = int(histogram_data[i])\n",
    "       if count > 0:\n",
    "           # Use bin edges to create uniform samples within each bin\n",
    "           samples = np.random.uniform(bin_edges[i], bin_edges[i+1], count)\n",
    "           data.extend(samples)\n",
    "   \n",
    "   data = np.array(data).reshape(-1, 1)\n",
    "   \n",
    "   # Fit GMM\n",
    "   gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "   gmm.fit(data)\n",
    "   \n",
    "   return gmm.means_.flatten(), gmm.covariances_.flatten()\n",
    "\n",
    "def rescale_by_probability(image, target_mean, variance):\n",
    "    \"\"\"\n",
    "    Rescale pixel values based on their probability of being the pixel of interest\n",
    "    under a normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : numpy.ndarray\n",
    "        Input grayscale image array\n",
    "    target_mean : float\n",
    "        The target pixel value (mean of the normal distribution)\n",
    "    variance : float\n",
    "        Variance of the normal distribution\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Rescaled image where each pixel value represents the probability\n",
    "        of that pixel being the pixel of interest\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    rescaled = image.copy().astype(float)\n",
    "    \n",
    "    # Calculate probability for each pixel value\n",
    "    probabilities = norm.pdf(rescaled, target_mean, np.sqrt(variance))\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    probabilities = (probabilities - probabilities.min()) / (probabilities.max() - probabilities.min())\n",
    "    \n",
    "    # Optional: Convert to uint8 for visualization (0-255)\n",
    "    rescaled = (probabilities * 255).astype(np.uint8)\n",
    "    \n",
    "    return rescaled\n",
    "\n",
    "def rescale_range(array, a, b):\n",
    "    \"\"\"\n",
    "    Rescale values in range [a,b] to [0,255] based on their position in the range.\n",
    "    Values outside [a,b] become 0.\n",
    "    \n",
    "    Parameters:\n",
    "    array: np.ndarray - Input array\n",
    "    a: float - Lower bound of the range\n",
    "    b: float - Upper bound of the range\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = array.copy()\n",
    "    \n",
    "    # Set values outside [a,b] to 0\n",
    "    result[result < a] = 0\n",
    "    result[result > b] = 0\n",
    "    \n",
    "    # Find values within the range\n",
    "    mask = (result >= a) & (result <= b)\n",
    "    \n",
    "    # Linear rescaling of values in range [a,b] to [0,255]\n",
    "    result[mask] = ((result[mask] - a) / (b - a)) * 255\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filepath = r\"C:\\Users\\acer\\Desktop\\Data\\47-4881 L 2014.nii.gz\"\n",
    "voxel = file_to_ndarray(filepath)\n",
    "\n",
    "# Compute histogram\n",
    "flat_array = voxel.flatten()\n",
    "bins = np.arange(-500, 1500, 1)  # -500 to 1500 size 1\n",
    "\n",
    "histogram_values, bin_edges = np.histogram(flat_array, bins=bins)\n",
    "mean, variance = fit_gmm(histogram_values, bin_edges)\n",
    "print(f\"mean : {mean}, variance : {variance}\")\n",
    "\n",
    "slice_2d = voxel[112, :, :]\n",
    "rescaled_image_air = rescale_by_probability(slice_2d, int(mean[0]), int(variance[0]))\n",
    "rescaled_image_bone = rescale_by_probability(slice_2d, int(mean[1]), int(variance[1]))\n",
    "\n",
    "a = mean[0]\n",
    "b = mean[1]\n",
    "adjust_const = 0.25\n",
    "adjust = int((b-a)*adjust_const)\n",
    "area_of_uncertainty = rescale_range(slice_2d, a+adjust, b-adjust)\n",
    "\n",
    "red = rescaled_image_air.astype(np.uint8)\n",
    "green = area_of_uncertainty.astype(np.uint8)\n",
    "blue = rescaled_image_bone.astype(np.uint8)\n",
    "\n",
    "# Combine into RGB\n",
    "rgb_image = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(rgb_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gmm(histogram_data, bin_edges):\n",
    "   # Create dataset by repeating values according to histogram frequencies\n",
    "   data = []\n",
    "   for i in range(len(histogram_data)):\n",
    "       count = int(histogram_data[i])\n",
    "       if count > 0:\n",
    "            # Use bin edges to create uniform samples within each bin\n",
    "            samples = np.random.uniform(bin_edges[i], bin_edges[i+1], count)\n",
    "            data.extend(samples)\n",
    "   \n",
    "   data = np.array(data).reshape(-1, 1)\n",
    "   \n",
    "   # Fit GMM\n",
    "   gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "   gmm.fit(data)\n",
    "\n",
    "   output_mean = np.sort(gmm.means_.flatten())\n",
    "   output_variances = gmm.covariances_.flatten()\n",
    "\n",
    "   if output_mean[0] != gmm.means_.flatten()[0] :\n",
    "        output_variances = np.flip(gmm.covariances_.flatten())\n",
    "\n",
    "   return output_mean, output_variances\n",
    "\n",
    "def rescale_uncertainty(array, a, b):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = array.copy()\n",
    "    \n",
    "    # Set values outside [a,b] to 0\n",
    "    result[result < a] = 0\n",
    "    result[result > b] = 0\n",
    "    \n",
    "    # Find values within the range\n",
    "    mask = (result >= a) & (result <= b)\n",
    "    \n",
    "    # Linear rescaling of values in range [a,b] to [0,255]\n",
    "    result[mask] = ((result[mask] - a) / (b - a)) * 255\n",
    "    \n",
    "    return result\n",
    "\n",
    "def rescale_by_probability(image, target_mean, variance):\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    rescaled = image.copy().astype(float)\n",
    "    \n",
    "    # Calculate probability for each pixel value\n",
    "    probabilities = norm.pdf(rescaled, target_mean, np.sqrt(variance))\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    probabilities = (probabilities - probabilities.min()) / (probabilities.max() - probabilities.min())\n",
    "    \n",
    "    # Optional: Convert to uint8 for visualization (0-255)\n",
    "    rescaled = (probabilities * 255).astype(np.uint8)\n",
    "    \n",
    "    return rescaled\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pad_image(image, target_size=(224, 224, 3)):\n",
    "    # Calculate padding amounts\n",
    "    h_padding = (target_size[0] - image.shape[0]) // 2\n",
    "    w_padding = (target_size[1] - image.shape[1]) // 2\n",
    "    \n",
    "    # Calculate extra padding if odd dimension\n",
    "    h_extra = (target_size[0] - image.shape[0]) % 2\n",
    "    w_extra = (target_size[1] - image.shape[1]) % 2\n",
    "    \n",
    "    # Pad the image\n",
    "    padded_image = np.pad(\n",
    "        image,\n",
    "        ((h_padding, h_padding + h_extra),  # Height padding\n",
    "         (w_padding, w_padding + w_extra),  # Width padding\n",
    "         (0, 0)),                          # No padding for channels\n",
    "        mode='constant',\n",
    "        constant_values=0\n",
    "    )\n",
    "    \n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def slice_extraction(input_folder, output_base_dir):\n",
    "    nii_files = glob(os.path.join(input_folder, '*.nii.gz'))\n",
    "\n",
    "    for nii_file in nii_files:\n",
    "        print(f\"Processing : {nii_file}\")\n",
    "\n",
    "        img = nib.load(nii_file)\n",
    "        data = img.get_fdata()\n",
    "        \n",
    "        base_name = os.path.splitext(os.path.splitext(os.path.basename(nii_file))[0])[0]\n",
    "\n",
    "        flat_array = data.flatten()\n",
    "        bins = np.arange(-500, 1500, 1)  # -500 to 1500 size 1\n",
    "        histogram_values, bin_edges = np.histogram(flat_array, bins=bins)\n",
    "\n",
    "        mean, variance = fit_gmm(histogram_values, bin_edges)\n",
    "        print(f\"mean : {mean}, variance : {variance}\")\n",
    "\n",
    "        for i in range(224):\n",
    "\n",
    "            if i%10 != 0 :\n",
    "                continue\n",
    "\n",
    "            slice_2d = data[i, 0:156 , 0:156]\n",
    "\n",
    "            empty_count = np.sum(slice_2d < -2000)\n",
    "            empty_ratio = empty_count / (slice_2d.shape[0] * slice_2d.shape[1])\n",
    "\n",
    "            if empty_ratio > 0.9:\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracting slice number : {i}\")\n",
    "\n",
    "            rescaled_image_air = rescale_by_probability(slice_2d, int(mean[0]), int(variance[0]))\n",
    "            rescaled_image_bone = rescale_by_probability(slice_2d, int(mean[1]), int(variance[1]))\n",
    "\n",
    "            adjust_const = 0.25\n",
    "            adjust = int((mean[1]-mean[0])*adjust_const)\n",
    "            area_of_uncertainty = rescale_uncertainty(slice_2d, mean[0]+adjust, mean[1]-adjust)\n",
    "\n",
    "            red = rescaled_image_air.astype(np.uint8)\n",
    "            green = area_of_uncertainty.astype(np.uint8)\n",
    "            blue = rescaled_image_bone.astype(np.uint8)\n",
    "\n",
    "            rgb_image = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "            rgb_image = pad_image(rgb_image)\n",
    "\n",
    "            out_file = os.path.join(output_base_dir, f\"{base_name}_{i:03d}.jpg\")\n",
    "\n",
    "            img = Image.fromarray(rgb_image)\n",
    "            img.save(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r\"D:\\Kananat\\_dataset\\test\\erosion_1\"\n",
    "output_base_dir = r\"D:\\Kananat\\_dataset_2d\\test\\erosion_1\"\n",
    "\n",
    "slice_extraction(input_folder, output_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_cbct_layer_with_gmm(nii_file_path, layer_number, background_value=-250):\n",
    "    \"\"\"\n",
    "    Extract a 2D layer from 3D CBCT data with GMM-based tissue classification.\n",
    "    GMM is fitted on the entire 3D volume for robust tissue classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nii_file_path : str\n",
    "        Path to the .nii.gz file\n",
    "    layer_number : int\n",
    "        Layer index to extract (0-based indexing)\n",
    "    background_value : float\n",
    "        Background voxel value to exclude from calculations (default: -250)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rgb_image : PIL.Image\n",
    "        RGB image where:\n",
    "        - Red channel: Soft tissue probability (0-255)\n",
    "        - Green channel: Original voxel values (0-255)\n",
    "        - Blue channel: Bone probability (0-255)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the NIfTI file\n",
    "    print(f\"Loading NIfTI file: {nii_file_path}\")\n",
    "    nii_img = nib.load(nii_file_path)\n",
    "    volume_data = nii_img.get_fdata()\n",
    "    \n",
    "    print(f\"Volume shape: {volume_data.shape}\")\n",
    "    print(f\"Volume value range: [{volume_data.min():.1f}, {volume_data.max():.1f}]\")\n",
    "    \n",
    "    # Validate layer number\n",
    "    if layer_number >= volume_data.shape[2]:\n",
    "        raise ValueError(f\"Layer {layer_number} exceeds volume depth {volume_data.shape[2]}\")\n",
    "    \n",
    "    # Remove background values from ENTIRE 3D volume for GMM fitting\n",
    "    print(\"Preparing 3D volume data for GMM fitting...\")\n",
    "    volume_non_background_mask = volume_data != background_value\n",
    "    volume_non_background_values = volume_data[volume_non_background_mask]\n",
    "    \n",
    "    print(f\"Total voxels in volume: {volume_data.size}\")\n",
    "    print(f\"Non-background voxels in volume: {len(volume_non_background_values)}\")\n",
    "    print(f\"Background voxels in volume: {volume_data.size - len(volume_non_background_values)}\")\n",
    "    print(f\"Volume non-background range: [{volume_non_background_values.min():.1f}, {volume_non_background_values.max():.1f}]\")\n",
    "    \n",
    "    if len(volume_non_background_values) < 10:\n",
    "        raise ValueError(\"Insufficient non-background voxels in entire volume for GMM fitting\")\n",
    "    \n",
    "    # For very large volumes, use a random sample for GMM fitting to speed up computation\n",
    "    max_samples_for_gmm = 1000000  # 1M samples should be sufficient\n",
    "    if len(volume_non_background_values) > max_samples_for_gmm:\n",
    "        print(f\"Volume has {len(volume_non_background_values)} non-background voxels.\")\n",
    "        print(f\"Using random sample of {max_samples_for_gmm} voxels for GMM fitting...\")\n",
    "        np.random.seed(42)  # For reproducible results\n",
    "        sample_indices = np.random.choice(len(volume_non_background_values), \n",
    "                                        size=max_samples_for_gmm, replace=False)\n",
    "        gmm_fitting_data = volume_non_background_values[sample_indices]\n",
    "    else:\n",
    "        gmm_fitting_data = volume_non_background_values\n",
    "    \n",
    "    print(f\"Using {len(gmm_fitting_data)} voxels for GMM fitting\")\n",
    "    \n",
    "    # Extract the specified layer for final processing\n",
    "    layer_2d = volume_data[layer_number, :, :]\n",
    "    layer_non_background_mask = layer_2d != background_value\n",
    "    print(f\"Extracted layer {layer_number} with shape: {layer_2d.shape}\")\n",
    "    print(f\"Non-background voxels in layer: {np.sum(layer_non_background_mask)}\")\n",
    "    \n",
    "    # Fit Gaussian Mixture Model with 2 components on 3D volume data\n",
    "    print(\"Fitting Gaussian Mixture Model on entire 3D volume...\")\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(gmm_fitting_data.reshape(-1, 1))\n",
    "    \n",
    "    # Get GMM parameters\n",
    "    means = gmm.means_.flatten()\n",
    "    stds = np.sqrt(gmm.covariances_).flatten()\n",
    "    weights = gmm.weights_\n",
    "    \n",
    "    print(f\"GMM Component 1: mean={means[0]:.1f}, std={stds[0]:.1f}, weight={weights[0]:.3f}\")\n",
    "    print(f\"GMM Component 2: mean={means[1]:.1f}, std={stds[1]:.1f}, weight={weights[1]:.3f}\")\n",
    "    \n",
    "    # Determine which component is bone (higher mean) and which is soft tissue\n",
    "    if means[0] > means[1]:\n",
    "        bone_idx, soft_tissue_idx = 0, 1\n",
    "    else:\n",
    "        bone_idx, soft_tissue_idx = 1, 0\n",
    "    \n",
    "    print(f\"Bone component (higher intensity): Component {bone_idx}\")\n",
    "    print(f\"Soft tissue component (lower intensity): Component {soft_tissue_idx}\")\n",
    "    \n",
    "    # Apply the trained GMM to the selected layer\n",
    "    print(f\"Applying GMM to layer {layer_number}...\")\n",
    "    # Calculate probabilities for all pixels in the layer\n",
    "    # For background pixels, set probabilities to 0\n",
    "    bone_prob = np.zeros_like(layer_2d)\n",
    "    soft_tissue_prob = np.zeros_like(layer_2d)\n",
    "    \n",
    "    # Only calculate probabilities for non-background pixels in the layer\n",
    "    if np.sum(layer_non_background_mask) > 0:\n",
    "        # Get probability predictions for the entire layer\n",
    "        all_probs = gmm.predict_proba(layer_2d.reshape(-1, 1))\n",
    "        all_probs_reshaped = all_probs.reshape(layer_2d.shape[0], layer_2d.shape[1], 2)\n",
    "        \n",
    "        # Extract bone and soft tissue probabilities\n",
    "        bone_prob = all_probs_reshaped[:, :, bone_idx]\n",
    "        soft_tissue_prob = all_probs_reshaped[:, :, soft_tissue_idx]\n",
    "        \n",
    "        # Set background pixels to 0 probability\n",
    "        bone_prob[~layer_non_background_mask] = 0\n",
    "        soft_tissue_prob[~layer_non_background_mask] = 0\n",
    "    \n",
    "    # Rescale original voxel values to [0, 255] using the VOLUME range for consistency\n",
    "    original_scaled = np.zeros_like(layer_2d)\n",
    "    if len(volume_non_background_values) > 0:\n",
    "        vol_min_val, vol_max_val = volume_non_background_values.min(), volume_non_background_values.max()\n",
    "        original_scaled[layer_non_background_mask] = 255 * (layer_2d[layer_non_background_mask] - vol_min_val) / (vol_max_val - vol_min_val)\n",
    "    \n",
    "    # Rescale probabilities to [0, 255]\n",
    "    bone_prob_scaled = (bone_prob * 255).astype(np.uint8)\n",
    "    soft_tissue_prob_scaled = (soft_tissue_prob * 255).astype(np.uint8)\n",
    "    original_scaled = original_scaled.astype(np.uint8)\n",
    "    \n",
    "    # Create RGB image\n",
    "    # Red: Soft tissue probability\n",
    "    # Green: Original voxel values  \n",
    "    # Blue: Bone probability\n",
    "    rgb_array = np.stack([soft_tissue_prob_scaled, original_scaled, bone_prob_scaled], axis=2)\n",
    "    rgb_image = Image.fromarray(rgb_array, 'RGB')\n",
    "    \n",
    "    # Create debugging visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Row 1: Original data and histograms\n",
    "    # Original layer\n",
    "    im1 = axes[0, 0].imshow(layer_2d, cmap='gray')\n",
    "    axes[0, 0].set_title(f'Original Layer {layer_number}')\n",
    "    axes[0, 0].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # Histogram of VOLUME non-background values (used for GMM)\n",
    "    axes[0, 1].hist(gmm_fitting_data, bins=50, alpha=0.7, density=True, color='gray')\n",
    "    axes[0, 1].set_title('Histogram of Volume Data (GMM Training)')\n",
    "    axes[0, 1].set_xlabel('Voxel Value')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    \n",
    "    # GMM overlay on histogram\n",
    "    x_range = np.linspace(gmm_fitting_data.min(), gmm_fitting_data.max(), 1000)\n",
    "    \n",
    "    # Plot individual components\n",
    "    for i in range(2):\n",
    "        component_pdf = weights[i] * (1/np.sqrt(2*np.pi*gmm.covariances_[i,0,0])) * \\\n",
    "                       np.exp(-0.5 * ((x_range - means[i])**2) / gmm.covariances_[i,0,0])\n",
    "        label = f'{\"Bone\" if i == bone_idx else \"Soft Tissue\"}'\n",
    "        color = 'blue' if i == bone_idx else 'red'\n",
    "        axes[0, 1].plot(x_range, component_pdf, color=color, label=label, linewidth=2)\n",
    "    \n",
    "    # Plot total GMM\n",
    "    total_pdf = np.exp(gmm.score_samples(x_range.reshape(-1, 1)))\n",
    "    axes[0, 1].plot(x_range, total_pdf, 'k--', label='Total GMM', linewidth=2)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # GMM classification result for the layer\n",
    "    classification = gmm.predict(layer_2d.reshape(-1, 1)).reshape(layer_2d.shape)\n",
    "    classification_display = np.full_like(layer_2d, -1, dtype=int)  # -1 for background\n",
    "    classification_display[layer_non_background_mask] = classification[layer_non_background_mask]\n",
    "    \n",
    "    im3 = axes[0, 2].imshow(classification_display, cmap='RdBu')\n",
    "    axes[0, 2].set_title(f'GMM Classification of Layer {layer_number}\\n(Red=Soft Tissue, Blue=Bone)')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Row 2: Probability maps and final RGB\n",
    "    # Bone probability\n",
    "    im4 = axes[1, 0].imshow(bone_prob, cmap='Blues', vmin=0, vmax=1)\n",
    "    axes[1, 0].set_title('Bone Probability')\n",
    "    axes[1, 0].axis('off')\n",
    "    plt.colorbar(im4, ax=axes[1, 0])\n",
    "    \n",
    "    # Soft tissue probability\n",
    "    im5 = axes[1, 1].imshow(soft_tissue_prob, cmap='Reds', vmin=0, vmax=1)\n",
    "    axes[1, 1].set_title('Soft Tissue Probability')\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im5, ax=axes[1, 1])\n",
    "    \n",
    "    # Final RGB result\n",
    "    axes[1, 2].imshow(rgb_array)\n",
    "    axes[1, 2].set_title('Final RGB Image\\n(R=Soft Tissue, G=Original, B=Bone)')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Summary Statistics ===\")\n",
    "    print(f\"Volume shape: {volume_data.shape}\")\n",
    "    print(f\"Volume non-background voxels: {len(volume_non_background_values)} / {volume_data.size}\")\n",
    "    print(f\"Layer shape: {layer_2d.shape}\")\n",
    "    print(f\"Layer non-background pixels: {np.sum(layer_non_background_mask)} / {layer_2d.size}\")\n",
    "    print(f\"Layer background pixels: {np.sum(~layer_non_background_mask)} / {layer_2d.size}\")\n",
    "    \n",
    "    if np.sum(layer_non_background_mask) > 0:\n",
    "        print(f\"Mean bone probability (layer non-background): {bone_prob[layer_non_background_mask].mean():.3f}\")\n",
    "        print(f\"Mean soft tissue probability (layer non-background): {soft_tissue_prob[layer_non_background_mask].mean():.3f}\")\n",
    "        print(f\"Layer voxel range: [{layer_2d[layer_non_background_mask].min():.1f}, {layer_2d[layer_non_background_mask].max():.1f}]\")\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "# Example usage:\n",
    "# rgb_img = extract_cbct_layer_with_gmm('path/to/your/cbct_data.nii.gz', layer_number=50)\n",
    "# rgb_img.save('cbct_layer_rgb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call (replace with your actual file path and layer number)\n",
    "image_path = r\"C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\training_dataset_3D\\test\\0\\50-30909 R_adjustedBG.nii.gz\"  # Replace with your NIfTI file path\n",
    "rgb_img = extract_cbct_layer_with_gmm(image_path, layer_number=112)\n",
    "rgb_img.save('cbct_layer_rgb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_cbct_layer_with_gmm(nii_file_path, output_image_path, layer_number, debug_folder_path, background_value=-250):\n",
    "    \"\"\"\n",
    "    Extract a 2D layer from 3D CBCT data with GMM-based tissue classification.\n",
    "    GMM is fitted on the entire 3D volume for robust tissue classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nii_file_path : str\n",
    "        Path to the .nii.gz file\n",
    "    output_image_path : str\n",
    "        Path to save the output RGB image (e.g., 'output.png')\n",
    "    layer_number : int\n",
    "        Layer index to extract (0-based indexing)\n",
    "    debug_folder_path : str\n",
    "        Path to folder where debug visualizations will be saved\n",
    "    background_value : float\n",
    "        Background voxel value to exclude from calculations (default: -250)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rgb_image : PIL.Image\n",
    "        RGB image where:\n",
    "        - Red channel: Soft tissue probability (0-255)\n",
    "        - Green channel: Original voxel values (0-255)\n",
    "        - Blue channel: Bone probability (0-255)\n",
    "    \n",
    "    Files Created:\n",
    "    --------------\n",
    "    - output_image_path: RGB image file\n",
    "    - debug_folder_path/gmm_analysis.png: Complete debugging visualization\n",
    "    - debug_folder_path/layer_original.png: Original layer grayscale\n",
    "    - debug_folder_path/bone_probability.png: Bone probability map\n",
    "    - debug_folder_path/soft_tissue_probability.png: Soft tissue probability map\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create debug folder if it doesn't exist\n",
    "    os.makedirs(debug_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Load the NIfTI file\n",
    "    print(f\"Loading NIfTI file: {nii_file_path}\")\n",
    "    nii_img = nib.load(nii_file_path)\n",
    "    volume_data = nii_img.get_fdata()\n",
    "    \n",
    "    print(f\"Volume shape: {volume_data.shape}\")\n",
    "    print(f\"Volume value range: [{volume_data.min():.1f}, {volume_data.max():.1f}]\")\n",
    "    \n",
    "    # Validate layer number\n",
    "    if layer_number >= volume_data.shape[2]:\n",
    "        raise ValueError(f\"Layer {layer_number} exceeds volume depth {volume_data.shape[2]}\")\n",
    "    \n",
    "    # Remove background values from ENTIRE 3D volume for GMM fitting\n",
    "    print(\"Preparing 3D volume data for GMM fitting...\")\n",
    "    volume_non_background_mask = volume_data != background_value\n",
    "    volume_non_background_values = volume_data[volume_non_background_mask]\n",
    "    \n",
    "    print(f\"Total voxels in volume: {volume_data.size}\")\n",
    "    print(f\"Non-background voxels in volume: {len(volume_non_background_values)}\")\n",
    "    print(f\"Background voxels in volume: {volume_data.size - len(volume_non_background_values)}\")\n",
    "    print(f\"Volume non-background range: [{volume_non_background_values.min():.1f}, {volume_non_background_values.max():.1f}]\")\n",
    "    \n",
    "    if len(volume_non_background_values) < 10:\n",
    "        raise ValueError(\"Insufficient non-background voxels in entire volume for GMM fitting\")\n",
    "    \n",
    "    # For very large volumes, use a random sample for GMM fitting to speed up computation\n",
    "    max_samples_for_gmm = 1000000  # 1M samples should be sufficient\n",
    "    if len(volume_non_background_values) > max_samples_for_gmm:\n",
    "        print(f\"Volume has {len(volume_non_background_values)} non-background voxels.\")\n",
    "        print(f\"Using random sample of {max_samples_for_gmm} voxels for GMM fitting...\")\n",
    "        np.random.seed(42)  # For reproducible results\n",
    "        sample_indices = np.random.choice(len(volume_non_background_values), \n",
    "                                        size=max_samples_for_gmm, replace=False)\n",
    "        gmm_fitting_data = volume_non_background_values[sample_indices]\n",
    "    else:\n",
    "        gmm_fitting_data = volume_non_background_values\n",
    "    \n",
    "    print(f\"Using {len(gmm_fitting_data)} voxels for GMM fitting\")\n",
    "    \n",
    "    # Extract the specified layer for final processing\n",
    "    layer_2d = volume_data[:, :, layer_number]\n",
    "    layer_non_background_mask = layer_2d != background_value\n",
    "    print(f\"Extracted layer {layer_number} with shape: {layer_2d.shape}\")\n",
    "    print(f\"Non-background voxels in layer: {np.sum(layer_non_background_mask)}\")\n",
    "    \n",
    "    # Fit Gaussian Mixture Model with 2 components on 3D volume data\n",
    "    print(\"Fitting Gaussian Mixture Model on entire 3D volume...\")\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(gmm_fitting_data.reshape(-1, 1))\n",
    "    \n",
    "    # Get GMM parameters\n",
    "    means = gmm.means_.flatten()\n",
    "    stds = np.sqrt(gmm.covariances_).flatten()\n",
    "    weights = gmm.weights_\n",
    "    \n",
    "    print(f\"GMM Component 1: mean={means[0]:.1f}, std={stds[0]:.1f}, weight={weights[0]:.3f}\")\n",
    "    print(f\"GMM Component 2: mean={means[1]:.1f}, std={stds[1]:.1f}, weight={weights[1]:.3f}\")\n",
    "    \n",
    "    # Determine which component is bone (higher mean) and which is soft tissue\n",
    "    if means[0] > means[1]:\n",
    "        bone_idx, soft_tissue_idx = 0, 1\n",
    "    else:\n",
    "        bone_idx, soft_tissue_idx = 1, 0\n",
    "    \n",
    "    print(f\"Bone component (higher intensity): Component {bone_idx}\")\n",
    "    print(f\"Soft tissue component (lower intensity): Component {soft_tissue_idx}\")\n",
    "    \n",
    "    # Apply the trained GMM to the selected layer\n",
    "    print(f\"Applying GMM to layer {layer_number}...\")\n",
    "    # Calculate probabilities for all pixels in the layer\n",
    "    # For background pixels, set probabilities to 0\n",
    "    bone_prob = np.zeros_like(layer_2d)\n",
    "    soft_tissue_prob = np.zeros_like(layer_2d)\n",
    "    \n",
    "    # Only calculate probabilities for non-background pixels in the layer\n",
    "    if np.sum(layer_non_background_mask) > 0:\n",
    "        # Get probability predictions for the entire layer\n",
    "        all_probs = gmm.predict_proba(layer_2d.reshape(-1, 1))\n",
    "        all_probs_reshaped = all_probs.reshape(layer_2d.shape[0], layer_2d.shape[1], 2)\n",
    "        \n",
    "        # Extract bone and soft tissue probabilities\n",
    "        bone_prob = all_probs_reshaped[:, :, bone_idx]\n",
    "        soft_tissue_prob = all_probs_reshaped[:, :, soft_tissue_idx]\n",
    "        \n",
    "        # Set background pixels to 0 probability\n",
    "        bone_prob[~layer_non_background_mask] = 0\n",
    "        soft_tissue_prob[~layer_non_background_mask] = 0\n",
    "    \n",
    "    # Rescale original voxel values to [0, 255] using the VOLUME range for consistency\n",
    "    original_scaled = np.zeros_like(layer_2d)\n",
    "    if len(volume_non_background_values) > 0:\n",
    "        vol_min_val, vol_max_val = volume_non_background_values.min(), volume_non_background_values.max()\n",
    "        original_scaled[layer_non_background_mask] = 255 * (layer_2d[layer_non_background_mask] - vol_min_val) / (vol_max_val - vol_min_val)\n",
    "    \n",
    "    # Rescale probabilities to [0, 255]\n",
    "    bone_prob_scaled = (bone_prob * 255).astype(np.uint8)\n",
    "    soft_tissue_prob_scaled = (soft_tissue_prob * 255).astype(np.uint8)\n",
    "    original_scaled = original_scaled.astype(np.uint8)\n",
    "    \n",
    "    # Create RGB image\n",
    "    # Red: Soft tissue probability\n",
    "    # Green: Original voxel values  \n",
    "    # Blue: Bone probability\n",
    "    rgb_array = np.stack([soft_tissue_prob_scaled, original_scaled, bone_prob_scaled], axis=2)\n",
    "    rgb_image = Image.fromarray(rgb_array, 'RGB')\n",
    "    \n",
    "    # Save the main RGB output image\n",
    "    print(f\"Saving RGB image to: {output_image_path}\")\n",
    "    rgb_image.save(output_image_path)\n",
    "    \n",
    "    # Save individual probability maps and original layer\n",
    "    print(f\"Saving debug images to: {debug_folder_path}\")\n",
    "    \n",
    "    # Save original layer as grayscale\n",
    "    original_layer_img = Image.fromarray((layer_2d - layer_2d.min()) / (layer_2d.max() - layer_2d.min()) * 255).convert('L')\n",
    "    original_layer_img.save(os.path.join(debug_folder_path, 'layer_original.png'))\n",
    "    \n",
    "    # Save bone probability map\n",
    "    bone_prob_img = Image.fromarray((bone_prob * 255).astype(np.uint8), 'L')\n",
    "    bone_prob_img.save(os.path.join(debug_folder_path, 'bone_probability.png'))\n",
    "    \n",
    "    # Save soft tissue probability map\n",
    "    soft_tissue_prob_img = Image.fromarray((soft_tissue_prob * 255).astype(np.uint8), 'L')\n",
    "    soft_tissue_prob_img.save(os.path.join(debug_folder_path, 'soft_tissue_probability.png'))\n",
    "    \n",
    "    # Create and save comprehensive debugging visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Row 1: Original data and histograms\n",
    "    # Original layer\n",
    "    im1 = axes[0, 0].imshow(layer_2d, cmap='gray')\n",
    "    axes[0, 0].set_title(f'Original Layer {layer_number}')\n",
    "    axes[0, 0].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # Histogram of VOLUME non-background values (used for GMM)\n",
    "    axes[0, 1].hist(gmm_fitting_data, bins=50, alpha=0.7, density=True, color='gray')\n",
    "    axes[0, 1].set_title('Histogram of Volume Data (GMM Training)')\n",
    "    axes[0, 1].set_xlabel('Voxel Value')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    \n",
    "    # GMM overlay on histogram\n",
    "    x_range = np.linspace(gmm_fitting_data.min(), gmm_fitting_data.max(), 1000)\n",
    "    \n",
    "    # Plot individual components\n",
    "    for i in range(2):\n",
    "        component_pdf = weights[i] * (1/np.sqrt(2*np.pi*gmm.covariances_[i,0,0])) * \\\n",
    "                       np.exp(-0.5 * ((x_range - means[i])**2) / gmm.covariances_[i,0,0])\n",
    "        label = f'{\"Bone\" if i == bone_idx else \"Soft Tissue\"}'\n",
    "        color = 'blue' if i == bone_idx else 'red'\n",
    "        axes[0, 1].plot(x_range, component_pdf, color=color, label=label, linewidth=2)\n",
    "    \n",
    "    # Plot total GMM\n",
    "    total_pdf = np.exp(gmm.score_samples(x_range.reshape(-1, 1)))\n",
    "    axes[0, 1].plot(x_range, total_pdf, 'k--', label='Total GMM', linewidth=2)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # GMM classification result for the layer\n",
    "    classification = gmm.predict(layer_2d.reshape(-1, 1)).reshape(layer_2d.shape)\n",
    "    classification_display = np.full_like(layer_2d, -1, dtype=int)  # -1 for background\n",
    "    classification_display[layer_non_background_mask] = classification[layer_non_background_mask]\n",
    "    \n",
    "    im3 = axes[0, 2].imshow(classification_display, cmap='RdBu')\n",
    "    axes[0, 2].set_title(f'GMM Classification of Layer {layer_number}\\n(Red=Soft Tissue, Blue=Bone)')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Row 2: Probability maps and final RGB\n",
    "    # Bone probability\n",
    "    im4 = axes[1, 0].imshow(bone_prob, cmap='Blues', vmin=0, vmax=1)\n",
    "    axes[1, 0].set_title('Bone Probability')\n",
    "    axes[1, 0].axis('off')\n",
    "    plt.colorbar(im4, ax=axes[1, 0])\n",
    "    \n",
    "    # Soft tissue probability\n",
    "    im5 = axes[1, 1].imshow(soft_tissue_prob, cmap='Reds', vmin=0, vmax=1)\n",
    "    axes[1, 1].set_title('Soft Tissue Probability')\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im5, ax=axes[1, 1])\n",
    "    \n",
    "    # Final RGB result\n",
    "    axes[1, 2].imshow(rgb_array)\n",
    "    axes[1, 2].set_title('Final RGB Image\\n(R=Soft Tissue, G=Original, B=Bone)')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the comprehensive debug visualization\n",
    "    debug_plot_path = os.path.join(debug_folder_path, 'gmm_analysis.png')\n",
    "    plt.savefig(debug_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Summary Statistics ===\")\n",
    "    print(f\"Volume shape: {volume_data.shape}\")\n",
    "    print(f\"Volume non-background voxels: {len(volume_non_background_values)} / {volume_data.size}\")\n",
    "    print(f\"Layer shape: {layer_2d.shape}\")\n",
    "    print(f\"Layer non-background pixels: {np.sum(layer_non_background_mask)} / {layer_2d.size}\")\n",
    "    print(f\"Layer background pixels: {np.sum(~layer_non_background_mask)} / {layer_2d.size}\")\n",
    "    \n",
    "    if np.sum(layer_non_background_mask) > 0:\n",
    "        print(f\"Mean bone probability (layer non-background): {bone_prob[layer_non_background_mask].mean():.3f}\")\n",
    "        print(f\"Mean soft tissue probability (layer non-background): {soft_tissue_prob[layer_non_background_mask].mean():.3f}\")\n",
    "        print(f\"Layer voxel range: [{layer_2d[layer_non_background_mask].min():.1f}, {layer_2d[layer_non_background_mask].max():.1f}]\")\n",
    "    \n",
    "    print(f\"\\n=== Files Created ===\")\n",
    "    print(f\"Main RGB output: {output_image_path}\")\n",
    "    print(f\"Debug folder: {debug_folder_path}\")\n",
    "    print(f\"  - gmm_analysis.png: Complete analysis visualization\")\n",
    "    print(f\"  - layer_original.png: Original layer grayscale\")\n",
    "    print(f\"  - bone_probability.png: Bone probability map\")\n",
    "    print(f\"  - soft_tissue_probability.png: Soft tissue probability map\")\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "# Example usage:\n",
    "# rgb_img = extract_cbct_layer_with_gmm(\n",
    "#     nii_file_path='path/to/cbct_data.nii.gz',\n",
    "#     output_image_path='output/layer_50_rgb.png',\n",
    "#     layer_number=50,\n",
    "#     debug_folder_path='debug_output/'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_file = r\"C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\training_dataset_3D\\test\\0\\50-30909 R_adjustedBG.nii.gz\"\n",
    "output_path = r\"test.png\"\n",
    "debug = r\"C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\debug\"\n",
    "\n",
    "rgb_img = extract_cbct_layer_with_gmm(\n",
    "    nii_file_path=nii_file,\n",
    "    output_image_path=output_path,\n",
    "    layer_number=50,\n",
    "    debug_folder_path=debug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "Input dataset: C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\training_dataset_3D\n",
      "Output dataset: C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\training_dataset_2D_v3\n",
      "Number of layers to extract: 10\n",
      "Minimum spacing between layers: 5\n",
      "Background value: -250\n",
      "Use GMM processing: True\n",
      "Debug folder: C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\debug\n",
      "✅ Input dataset found: C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\training_dataset_3D\n",
      "✅ Starting processing to extract 10 layers with 5 spacing...\n",
      "\n",
      "🚀 Starting dataset processing...\n",
      "\n",
      "==================================================\n",
      "Processing split: test\n",
      "==================================================\n",
      "\n",
      "📁 Processing class: 0\n",
      "Found 16 NIfTI files\n",
      "9 Processing 50-30909 R |██████████|                                    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 590\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;66;03m# Process the dataset\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🚀 Starting dataset processing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 590\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_DATASET_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DATASET_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_LAYERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMIN_SPACING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBACKGROUND_VALUE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;66;03m# Print final summary\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 556\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(input_dataset_dir, output_dataset_dir, n_layers, min_spacing, background_value)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nifti_file \u001b[38;5;129;01min\u001b[39;00m nifti_files:\n\u001b[0;32m    555\u001b[0m     stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_files\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 556\u001b[0m     layers_saved \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_nifti_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnifti_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_class_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_spacing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layers_saved \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    559\u001b[0m         stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_files\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 467\u001b[0m, in \u001b[0;36mprocess_nifti_file\u001b[1;34m(nifti_path, output_dir, n_layers, min_spacing, background_value)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GMM_PROCESSING:\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;66;03m# Use your comprehensive GMM function\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m         rgb_img \u001b[38;5;241m=\u001b[39m \u001b[43mextract_cbct_layer_with_gmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnii_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnifti_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_image_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdebug_folder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_debug_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbackground_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_value\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m         bar_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    476\u001b[0m         filled_length \u001b[38;5;241m=\u001b[39m layer_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[8], line 130\u001b[0m, in \u001b[0;36mextract_cbct_layer_with_gmm\u001b[1;34m(nii_file_path, output_image_path, layer_number, debug_folder_path, background_value)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# print(f\"    Extracted layer {layer_number} with shape: {layer_2d.shape}\")\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# print(f\"    Non-background voxels in layer: {np.sum(layer_non_background_mask)}\")\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Fit Gaussian Mixture Model with 2 components on 3D volume data\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# print(f\"    Fitting Gaussian Mixture Model on entire 3D volume...\")\u001b[39;00m\n\u001b[0;32m    129\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgmm_fitting_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Get GMM parameters\u001b[39;00m\n\u001b[0;32m    133\u001b[0m means \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mmeans_\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\envs\\2dmodelGPU\\Lib\\site-packages\\sklearn\\mixture\\_base.py:181\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\envs\\2dmodelGPU\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\envs\\2dmodelGPU\\Lib\\site-packages\\sklearn\\mixture\\_base.py:247\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    245\u001b[0m     prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[1;32m--> 247\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, log_resp)\n\u001b[0;32m    249\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\envs\\2dmodelGPU\\Lib\\site-packages\\sklearn\\mixture\\_base.py:306\u001b[0m, in \u001b[0;36mBaseMixture._e_step\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_e_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"E step.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(log_prob_norm), log_resp\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\envs\\2dmodelGPU\\Lib\\site-packages\\sklearn\\mixture\\_base.py:527\u001b[0m, in \u001b[0;36mBaseMixture._estimate_log_prob_resp\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03mCompute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;124;03m    logarithm of the responsibilities\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    526\u001b[0m weighted_log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_weighted_log_prob(X)\n\u001b[1;32m--> 527\u001b[0m log_prob_norm \u001b[38;5;241m=\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweighted_log_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n\u001b[0;32m    530\u001b[0m     log_resp \u001b[38;5;241m=\u001b[39m weighted_log_prob \u001b[38;5;241m-\u001b[39m log_prob_norm[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\envs\\2dmodelGPU\\Lib\\site-packages\\scipy\\special\\_logsumexp.py:106\u001b[0m, in \u001b[0;36mlogsumexp\u001b[1;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Scale by real part for complex inputs, because this affects\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# the magnitude of the exponential.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m initial_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msize(a) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m a_max \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a_max\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    109\u001b[0m     a_max[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misfinite(a_max)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\envs\\2dmodelGPU\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2827\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2814\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2816\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2817\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2818\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2825\u001b[0m \u001b[38;5;124;03m    ndarray.max : equivalent method\u001b[39;00m\n\u001b[0;32m   2826\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2828\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\envs\\2dmodelGPU\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3D NIfTI to 2D PNG Layer Extractor - Top N Informative Layers\n",
    "# Extracts the N most informative layers with minimum spacing between them\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Additional imports for GMM processing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging for Jupyter\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these parameters\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset paths\n",
    "INPUT_DATASET_DIR = r\"C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\training_dataset_3D\"  # Update this path\n",
    "OUTPUT_DATASET_DIR = r\"C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\training_dataset_2D_v3\"  # Update this path\n",
    "\n",
    "# Processing parameters\n",
    "N_LAYERS = 10          # Number of most informative layers to extract\n",
    "MIN_SPACING = 5       # Minimum spacing between selected layers (k parameter)\n",
    "BACKGROUND_VALUE = -250   # Background value to exclude\n",
    "\n",
    "# GMM processing parameters\n",
    "USE_GMM_PROCESSING = True  # Set to False to use simple normalization\n",
    "DEBUG_FOLDER_PATH = r\"C:\\Users\\acer\\Desktop\\Project_TMJOA\\Data\\debug\"   # Set to a path like 'debug_output/' for GMM debugging, or None to disable\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Input dataset: {INPUT_DATASET_DIR}\")\n",
    "print(f\"Output dataset: {OUTPUT_DATASET_DIR}\")\n",
    "print(f\"Number of layers to extract: {N_LAYERS}\")\n",
    "print(f\"Minimum spacing between layers: {MIN_SPACING}\")\n",
    "print(f\"Background value: {BACKGROUND_VALUE}\")\n",
    "print(f\"Use GMM processing: {USE_GMM_PROCESSING}\")\n",
    "print(f\"Debug folder: {DEBUG_FOLDER_PATH}\")\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Your comprehensive GMM-based layer extraction function\n",
    "def extract_cbct_layer_with_gmm(nii_file_path, output_image_path, layer_number, debug_folder_path, background_value=-250):\n",
    "    \"\"\"\n",
    "    Extract a 2D layer from 3D CBCT data with GMM-based tissue classification.\n",
    "    GMM is fitted on the entire 3D volume for robust tissue classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nii_file_path : str\n",
    "        Path to the .nii.gz file\n",
    "    output_image_path : str\n",
    "        Path to save the output RGB image (e.g., 'output.png')\n",
    "    layer_number : int\n",
    "        Layer index to extract (0-based indexing)\n",
    "    debug_folder_path : str\n",
    "        Path to folder where debug visualizations will be saved\n",
    "    background_value : float\n",
    "        Background voxel value to exclude from calculations (default: -250)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rgb_image : PIL.Image\n",
    "        RGB image where:\n",
    "        - Red channel: Soft tissue probability (0-255)\n",
    "        - Green channel: Original voxel values (0-255)\n",
    "        - Blue channel: Bone probability (0-255)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create debug folder if it doesn't exist\n",
    "    if debug_folder_path:\n",
    "        os.makedirs(debug_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Load the NIfTI file\n",
    "    # print(f\"    Processing NIfTI file: {nii_file_path}\")\n",
    "    nii_img = nib.load(nii_file_path)\n",
    "    volume_data = nii_img.get_fdata()\n",
    "    \n",
    "    # print(f\"    Volume shape: {volume_data.shape}\")\n",
    "    # print(f\"    Volume value range: [{volume_data.min():.1f}, {volume_data.max():.1f}]\")\n",
    "    \n",
    "    # Validate layer number\n",
    "    if layer_number >= volume_data.shape[0]:  # Changed from shape[2] to shape[0] for axis-0 extraction\n",
    "        raise ValueError(f\"Layer {layer_number} exceeds volume depth {volume_data.shape[0]}\")\n",
    "    \n",
    "    # Remove background values from ENTIRE 3D volume for GMM fitting\n",
    "    # print(f\"    Preparing 3D volume data for GMM fitting...\")\n",
    "    volume_non_background_mask = volume_data != background_value\n",
    "    volume_non_background_values = volume_data[volume_non_background_mask]\n",
    "    \n",
    "    # print(f\"    Total voxels in volume: {volume_data.size}\")\n",
    "    # print(f\"    Non-background voxels in volume: {len(volume_non_background_values)}\")\n",
    "    # print(f\"    Volume non-background range: [{volume_non_background_values.min():.1f}, {volume_non_background_values.max():.1f}]\")\n",
    "    \n",
    "    if len(volume_non_background_values) < 10:\n",
    "        raise ValueError(\"Insufficient non-background voxels in entire volume for GMM fitting\")\n",
    "    \n",
    "    # For very large volumes, use a random sample for GMM fitting to speed up computation\n",
    "    max_samples_for_gmm = 1000000  # 1M samples should be sufficient\n",
    "    if len(volume_non_background_values) > max_samples_for_gmm:\n",
    "        # print(f\"    Volume has {len(volume_non_background_values)} non-background voxels.\")\n",
    "        # print(f\"    Using random sample of {max_samples_for_gmm} voxels for GMM fitting...\")\n",
    "        np.random.seed(42)  # For reproducible results\n",
    "        sample_indices = np.random.choice(len(volume_non_background_values), \n",
    "                                        size=max_samples_for_gmm, replace=False)\n",
    "        gmm_fitting_data = volume_non_background_values[sample_indices]\n",
    "    else:\n",
    "        gmm_fitting_data = volume_non_background_values\n",
    "    \n",
    "    # print(f\"    Using {len(gmm_fitting_data)} voxels for GMM fitting\")\n",
    "    \n",
    "    # Extract the specified layer for final processing (along axis 0)\n",
    "    layer_2d = volume_data[layer_number, :, :]\n",
    "    layer_non_background_mask = layer_2d != background_value\n",
    "    # print(f\"    Extracted layer {layer_number} with shape: {layer_2d.shape}\")\n",
    "    # print(f\"    Non-background voxels in layer: {np.sum(layer_non_background_mask)}\")\n",
    "    \n",
    "    # Fit Gaussian Mixture Model with 2 components on 3D volume data\n",
    "    # print(f\"    Fitting Gaussian Mixture Model on entire 3D volume...\")\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(gmm_fitting_data.reshape(-1, 1))\n",
    "    \n",
    "    # Get GMM parameters\n",
    "    means = gmm.means_.flatten()\n",
    "    stds = np.sqrt(gmm.covariances_).flatten()\n",
    "    weights = gmm.weights_\n",
    "    \n",
    "    # print(f\"    GMM Component 1: mean={means[0]:.1f}, std={stds[0]:.1f}, weight={weights[0]:.3f}\")\n",
    "    # print(f\"    GMM Component 2: mean={means[1]:.1f}, std={stds[1]:.1f}, weight={weights[1]:.3f}\")\n",
    "    \n",
    "    # Determine which component is bone (higher mean) and which is soft tissue\n",
    "    if means[0] > means[1]:\n",
    "        bone_idx, soft_tissue_idx = 0, 1\n",
    "    else:\n",
    "        bone_idx, soft_tissue_idx = 1, 0\n",
    "    \n",
    "    # print(f\"    Bone component (higher intensity): Component {bone_idx}\")\n",
    "    # print(f\"    Soft tissue component (lower intensity): Component {soft_tissue_idx}\")\n",
    "    \n",
    "    # Apply the trained GMM to the selected layer\n",
    "    # print(f\"    Applying GMM to layer {layer_number}...\")\n",
    "    # Calculate probabilities for all pixels in the layer\n",
    "    # For background pixels, set probabilities to 0\n",
    "    bone_prob = np.zeros_like(layer_2d)\n",
    "    soft_tissue_prob = np.zeros_like(layer_2d)\n",
    "    \n",
    "    # Only calculate probabilities for non-background pixels in the layer\n",
    "    if np.sum(layer_non_background_mask) > 0:\n",
    "        # Get probability predictions for the entire layer\n",
    "        all_probs = gmm.predict_proba(layer_2d.reshape(-1, 1))\n",
    "        all_probs_reshaped = all_probs.reshape(layer_2d.shape[0], layer_2d.shape[1], 2)\n",
    "        \n",
    "        # Extract bone and soft tissue probabilities\n",
    "        bone_prob = all_probs_reshaped[:, :, bone_idx]\n",
    "        soft_tissue_prob = all_probs_reshaped[:, :, soft_tissue_idx]\n",
    "        \n",
    "        # Set background pixels to 0 probability\n",
    "        bone_prob[~layer_non_background_mask] = 0\n",
    "        soft_tissue_prob[~layer_non_background_mask] = 0\n",
    "    \n",
    "    # Rescale original voxel values to [0, 255] using the VOLUME range for consistency\n",
    "    original_scaled = np.zeros_like(layer_2d)\n",
    "    if len(volume_non_background_values) > 0:\n",
    "        vol_min_val, vol_max_val = volume_non_background_values.min(), volume_non_background_values.max()\n",
    "        original_scaled[layer_non_background_mask] = 255 * (layer_2d[layer_non_background_mask] - vol_min_val) / (vol_max_val - vol_min_val)\n",
    "    \n",
    "    # Rescale probabilities to [0, 255]\n",
    "    bone_prob_scaled = (bone_prob * 255).astype(np.uint8)\n",
    "    soft_tissue_prob_scaled = (soft_tissue_prob * 255).astype(np.uint8)\n",
    "    original_scaled = original_scaled.astype(np.uint8)\n",
    "    \n",
    "    # Create RGB image\n",
    "    # Red: Soft tissue probability\n",
    "    # Green: Original voxel values  \n",
    "    # Blue: Bone probability\n",
    "    rgb_array = np.stack([soft_tissue_prob_scaled, original_scaled, bone_prob_scaled], axis=2)\n",
    "    rgb_image = Image.fromarray(rgb_array, 'RGB')\n",
    "    \n",
    "    # Save the main RGB output image\n",
    "    # print(f\"    Saving RGB image to: {output_image_path}\")\n",
    "    rgb_image.save(output_image_path)\n",
    "    \n",
    "    # Save debug visualizations only if debug folder is provided\n",
    "\n",
    "    patient_id = os.path.basename(nii_file_path)  # Use the file name as patient ID\n",
    "    patient_id = patient_id.split('_')[0]  # Remove file extension\n",
    "    debug_plot_path = os.path.join(debug_folder_path, f'{patient_id}_gmm_analysis.png')\n",
    "\n",
    "    if debug_folder_path and not os.path.exists(debug_plot_path):\n",
    "        \n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "            \n",
    "            # Row 1: Original data and histograms\n",
    "            # Original layer\n",
    "            im1 = axes[0, 0].imshow(layer_2d, cmap='gray')\n",
    "            axes[0, 0].set_title(f'Original Layer {layer_number}')\n",
    "            axes[0, 0].axis('off')\n",
    "            plt.colorbar(im1, ax=axes[0, 0])\n",
    "            \n",
    "            # Histogram of VOLUME non-background values (used for GMM)\n",
    "            axes[0, 1].hist(gmm_fitting_data, bins=50, alpha=0.7, density=True, color='gray')\n",
    "            axes[0, 1].set_title('Histogram of Volume Data (GMM Training)')\n",
    "            axes[0, 1].set_xlabel('Voxel Value')\n",
    "            axes[0, 1].set_ylabel('Density')\n",
    "            \n",
    "            # GMM overlay on histogram\n",
    "            x_range = np.linspace(gmm_fitting_data.min(), gmm_fitting_data.max(), 1000)\n",
    "            \n",
    "            # Plot individual components\n",
    "            for i in range(2):\n",
    "                component_pdf = weights[i] * (1/np.sqrt(2*np.pi*gmm.covariances_[i,0,0])) * \\\n",
    "                               np.exp(-0.5 * ((x_range - means[i])**2) / gmm.covariances_[i,0,0])\n",
    "                label = f'{\"Bone\" if i == bone_idx else \"Soft Tissue\"}'\n",
    "                color = 'blue' if i == bone_idx else 'red'\n",
    "                axes[0, 1].plot(x_range, component_pdf, color=color, label=label, linewidth=2)\n",
    "            \n",
    "            # Plot total GMM\n",
    "            total_pdf = np.exp(gmm.score_samples(x_range.reshape(-1, 1)))\n",
    "            axes[0, 1].plot(x_range, total_pdf, 'k--', label='Total GMM', linewidth=2)\n",
    "            axes[0, 1].legend()\n",
    "            \n",
    "            # GMM classification result for the layer\n",
    "            classification = gmm.predict(layer_2d.reshape(-1, 1)).reshape(layer_2d.shape)\n",
    "            classification_display = np.full_like(layer_2d, -1, dtype=int)  # -1 for background\n",
    "            classification_display[layer_non_background_mask] = classification[layer_non_background_mask]\n",
    "            \n",
    "            im3 = axes[0, 2].imshow(classification_display, cmap='RdBu')\n",
    "            axes[0, 2].set_title(f'GMM Classification of Layer {layer_number}\\n(Red=Soft Tissue, Blue=Bone)')\n",
    "            axes[0, 2].axis('off')\n",
    "            \n",
    "            # Row 2: Probability maps and final RGB\n",
    "            # Bone probability\n",
    "            im4 = axes[1, 0].imshow(bone_prob, cmap='Blues', vmin=0, vmax=1)\n",
    "            axes[1, 0].set_title('Bone Probability')\n",
    "            axes[1, 0].axis('off')\n",
    "            plt.colorbar(im4, ax=axes[1, 0])\n",
    "            \n",
    "            # Soft tissue probability\n",
    "            im5 = axes[1, 1].imshow(soft_tissue_prob, cmap='Reds', vmin=0, vmax=1)\n",
    "            axes[1, 1].set_title('Soft Tissue Probability')\n",
    "            axes[1, 1].axis('off')\n",
    "            plt.colorbar(im5, ax=axes[1, 1])\n",
    "            \n",
    "            # Final RGB result\n",
    "            axes[1, 2].imshow(rgb_array)\n",
    "            axes[1, 2].set_title('Final RGB Image\\n(R=Soft Tissue, G=Original, B=Bone)')\n",
    "            axes[1, 2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the comprehensive debug visualization\n",
    "            plt.savefig(debug_plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()  # Close the figure to free memory\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not create debug plot: {str(e)}\")\n",
    "\n",
    "    return rgb_image\n",
    "\n",
    "def normalize_voxel_values(data, background_value=-250):\n",
    "    \"\"\"\n",
    "    Normalize voxel values from [-250, max_value] to [0, 255]\n",
    "    \n",
    "    Args:\n",
    "        data: 2D or 3D numpy array\n",
    "        background_value: Background value (default: -250)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized data as uint8\n",
    "    \"\"\"\n",
    "    # Clip values to ensure minimum is background_value\n",
    "    data = np.clip(data, background_value, None)\n",
    "    \n",
    "    # Get min and max values\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "    \n",
    "    if max_val == min_val:\n",
    "        # Handle case where all values are the same\n",
    "        return np.zeros_like(data, dtype=np.uint8)\n",
    "    \n",
    "    # Normalize to [0, 255]\n",
    "    normalized = ((data - min_val) / (max_val - min_val)) * 255\n",
    "    return normalized.astype(np.uint8)\n",
    "\n",
    "def calculate_layer_informativeness(data, background_value=-250):\n",
    "    \"\"\"\n",
    "    Calculate informativeness score for each layer\n",
    "    Uses non-background ratio as the primary metric\n",
    "    \n",
    "    Args:\n",
    "        data: 3D numpy array\n",
    "        background_value: Background value to exclude\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (layer_index, informativeness_score)\n",
    "    \"\"\"\n",
    "    layer_scores = []\n",
    "    total_voxels_per_layer = data.shape[1] * data.shape[2]\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        layer = data[i, :, :]\n",
    "        \n",
    "        # Calculate non-background ratio\n",
    "        non_background_count = np.sum(layer != background_value)\n",
    "        non_background_ratio = non_background_count / total_voxels_per_layer\n",
    "        \n",
    "        # Additional informativeness metrics can be added here:\n",
    "        # - Variance of non-background voxels\n",
    "        # - Edge content\n",
    "        # - Texture measures\n",
    "        \n",
    "        # For now, using non-background ratio as informativeness score\n",
    "        informativeness_score = non_background_ratio\n",
    "        \n",
    "        layer_scores.append((i, informativeness_score))\n",
    "    \n",
    "    return layer_scores\n",
    "\n",
    "def select_top_layers_with_spacing(layer_scores, n_layers, min_spacing):\n",
    "    \"\"\"\n",
    "    Select top N layers ensuring minimum spacing between them\n",
    "    \n",
    "    Args:\n",
    "        layer_scores: List of tuples (layer_index, score)\n",
    "        n_layers: Number of layers to select\n",
    "        min_spacing: Minimum spacing between selected layers\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (layer_index, score) for selected layers\n",
    "    \"\"\"\n",
    "    # Sort by score in descending order\n",
    "    sorted_layers = sorted(layer_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    selected_layers = []\n",
    "    used_indices = set()\n",
    "    \n",
    "    # print(f\"  Selecting {n_layers} layers with minimum spacing of {min_spacing}...\")\n",
    "    \n",
    "    for layer_idx, score in sorted_layers:\n",
    "        # Check if this layer conflicts with already selected layers\n",
    "        conflict = False\n",
    "        for selected_idx, _ in selected_layers:\n",
    "            if abs(layer_idx - selected_idx) < min_spacing:\n",
    "                conflict = True\n",
    "                break\n",
    "        \n",
    "        if not conflict:\n",
    "            selected_layers.append((layer_idx, score))\n",
    "            used_indices.add(layer_idx)\n",
    "            # print(f\"    Selected layer {layer_idx} (score: {score:.4f})\")\n",
    "            \n",
    "            if len(selected_layers) >= n_layers:\n",
    "                break\n",
    "    \n",
    "    # Sort selected layers by index for consistent naming\n",
    "    selected_layers.sort(key=lambda x: x[0])\n",
    "    \n",
    "    if len(selected_layers) < n_layers:\n",
    "        print(f\"    ⚠️ Only found {len(selected_layers)} layers that satisfy spacing constraint\")\n",
    "    \n",
    "    return selected_layers\n",
    "\n",
    "def extract_selected_layers(data, selected_layers):\n",
    "    \"\"\"\n",
    "    Extract the selected layers\n",
    "    \n",
    "    Args:\n",
    "        data: 3D numpy array\n",
    "        selected_layers: List of tuples (layer_index, score)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of layer information for processing\n",
    "    \"\"\"\n",
    "    layers = {}\n",
    "    \n",
    "    for i, (layer_idx, score) in enumerate(selected_layers):\n",
    "        layer_key = f\"layer_{layer_idx:03d}_rank_{i+1:02d}_score_{score:.3f}\"\n",
    "        layers[layer_key] = {\n",
    "            'layer_index': layer_idx,\n",
    "            'score': score,\n",
    "            'rank': i + 1\n",
    "        }\n",
    "        # print(f\"  Selected layer {layer_idx} (rank {i+1}, score: {score:.4f})\")\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def save_layer_as_png(layer_data, output_path):\n",
    "    \"\"\"\n",
    "    Save 2D layer as PNG with 3 identical channels (fallback method)\n",
    "    \n",
    "    Args:\n",
    "        layer_data: 2D numpy array (normalized to 0-255)\n",
    "        output_path: Output file path\n",
    "    \"\"\"\n",
    "    # Create 3-channel image (RGB) with identical values\n",
    "    rgb_image = np.stack([layer_data, layer_data, layer_data], axis=-1)\n",
    "    \n",
    "    # Convert to PIL Image and save\n",
    "    pil_image = Image.fromarray(rgb_image, mode='RGB')\n",
    "    pil_image.save(output_path)\n",
    "\n",
    "def process_nifti_file(nifti_path, output_dir, n_layers, min_spacing, background_value=-250):\n",
    "    \"\"\"\n",
    "    Process a single NIfTI file and extract top N informative layers with spacing\n",
    "    \n",
    "    Args:\n",
    "        nifti_path: Path to .nii.gz file\n",
    "        output_dir: Output directory for this file's layers\n",
    "        n_layers: Number of layers to extract\n",
    "        min_spacing: Minimum spacing between layers\n",
    "        background_value: Background value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(f\"\\nProcessing: {nifti_path}\")\n",
    "        \n",
    "        # Load NIfTI file\n",
    "        nifti_img = nib.load(nifti_path)\n",
    "        data = nifti_img.get_fdata()\n",
    "        \n",
    "        # print(f\"  Data shape: {data.shape}\")\n",
    "        # print(f\"  Data range: [{data.min():.2f}, {data.max():.2f}]\")\n",
    "        \n",
    "        # Calculate informativeness for all layers\n",
    "        layer_scores = calculate_layer_informativeness(data, background_value)\n",
    "        \n",
    "        # Select top N layers with spacing constraint\n",
    "        selected_layers = select_top_layers_with_spacing(layer_scores, n_layers, min_spacing)\n",
    "        \n",
    "        if not selected_layers:\n",
    "            print(f\"  ⚠️ No suitable layers found\")\n",
    "            return 0\n",
    "        \n",
    "        # Get layer information\n",
    "        layers = extract_selected_layers(data, selected_layers)\n",
    "        \n",
    "        # Process and save each layer\n",
    "        filename_base = Path(nifti_path).stem.replace('.nii', '')  # Remove .nii.gz extension\n",
    "        filename_base = filename_base.split('_')[0]  # Remove any file extension\n",
    "        saved_count = 0\n",
    "        \n",
    "        # Create debug folder for this file if enabled\n",
    "        file_debug_folder = None\n",
    "        if DEBUG_FOLDER_PATH:\n",
    "            file_debug_folder = DEBUG_FOLDER_PATH\n",
    "            os.makedirs(file_debug_folder, exist_ok=True)\n",
    "        \n",
    "        for layer_name, layer_info in layers.items():\n",
    "            layer_idx = layer_info['layer_index']\n",
    "            \n",
    "            # Create output filename\n",
    "            output_filename = f\"{filename_base}_{layer_info['rank']}.png\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            try:\n",
    "                if USE_GMM_PROCESSING:\n",
    "                    # Use your comprehensive GMM function\n",
    "                    rgb_img = extract_cbct_layer_with_gmm(\n",
    "                        nii_file_path=str(nifti_path),\n",
    "                        output_image_path=output_path,\n",
    "                        layer_number=layer_idx,\n",
    "                        debug_folder_path=file_debug_folder,\n",
    "                        background_value=background_value\n",
    "                    )\n",
    "\n",
    "                    bar_length = 10\n",
    "                    filled_length = layer_info['rank']\n",
    "                    \n",
    "                    # Create the bar\n",
    "                    bar = '█' * filled_length + '-' * (bar_length - filled_length)\n",
    "                    print(f\"    \\r   Processing {filename_base} |{bar}|\", end=\"\")\n",
    "                else:\n",
    "                    # Use simple normalization as fallback\n",
    "                    layer_data = data[layer_idx, :, :]\n",
    "                    normalized_layer = normalize_voxel_values(layer_data, background_value)\n",
    "                    save_layer_as_png(normalized_layer, output_path)\n",
    "                    # print(f\"    ✅ Simple processed and saved: {output_filename}\")\n",
    "                \n",
    "                saved_count += 1\n",
    "                \n",
    "            except Exception as layer_error:\n",
    "                print(f\"    ❌ Error processing layer {layer_idx}: {str(layer_error)}\")\n",
    "                continue\n",
    "        \n",
    "        # print(f\"  ✅ Saved {saved_count}/{len(layers)} layers successfully\")\n",
    "        return saved_count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing {nifti_path}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def process_dataset(input_dataset_dir, output_dataset_dir, n_layers, min_spacing, background_value=-250):\n",
    "    \"\"\"\n",
    "    Process entire dataset\n",
    "    \n",
    "    Args:\n",
    "        input_dataset_dir: Input dataset directory\n",
    "        output_dataset_dir: Output dataset directory\n",
    "        n_layers: Number of layers to extract per file\n",
    "        min_spacing: Minimum spacing between layers\n",
    "        background_value: Background value\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dataset_dir)\n",
    "    output_path = Path(output_dataset_dir)\n",
    "    \n",
    "    stats = {\n",
    "        'total_files': 0,\n",
    "        'processed_files': 0,\n",
    "        'total_layers_saved': 0,\n",
    "        'splits': {}\n",
    "    }\n",
    "    \n",
    "    # Iterate through train/val/test folders\n",
    "    for split_dir in input_path.iterdir():\n",
    "        if split_dir.is_dir() and split_dir.name in ['train', 'val', 'test']:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing split: {split_dir.name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            stats['splits'][split_dir.name] = {'classes': {}}\n",
    "            \n",
    "            # Iterate through class folders (0, 1)\n",
    "            for class_dir in split_dir.iterdir():\n",
    "                if class_dir.is_dir() and class_dir.name in ['0', '1']:\n",
    "                    print(f\"\\n📁 Processing class: {class_dir.name}\")\n",
    "                    \n",
    "                    # Create output directory\n",
    "                    output_class_dir = output_path / split_dir.name / class_dir.name\n",
    "                    output_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    # Process all .nii.gz files in this directory\n",
    "                    nifti_files = list(class_dir.glob('*.nii.gz'))\n",
    "                    print(f\"Found {len(nifti_files)} NIfTI files\")\n",
    "                    \n",
    "                    class_stats = {\n",
    "                        'total_files': len(nifti_files),\n",
    "                        'processed_files': 0,\n",
    "                        'total_layers': 0\n",
    "                    }\n",
    "                    \n",
    "                    for nifti_file in nifti_files:\n",
    "\n",
    "                        stats['total_files'] += 1\n",
    "                        layers_saved = process_nifti_file(nifti_file, output_class_dir, n_layers, min_spacing, background_value)\n",
    "\n",
    "                        if layers_saved > 0:\n",
    "                            stats['processed_files'] += 1\n",
    "                            class_stats['processed_files'] += 1\n",
    "                            stats['total_layers_saved'] += layers_saved\n",
    "                            class_stats['total_layers'] += layers_saved\n",
    "                    \n",
    "                    stats['splits'][split_dir.name]['classes'][class_dir.name] = class_stats\n",
    "                    print(f\"Class {class_dir.name} summary: {class_stats['processed_files']}/{class_stats['total_files']} files processed, {class_stats['total_layers']} layers saved\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "# Validate input directory\n",
    "if not os.path.exists(INPUT_DATASET_DIR):\n",
    "    print(f\"❌ Input dataset directory does not exist: {INPUT_DATASET_DIR}\")\n",
    "    print(\"Please update the INPUT_DATASET_DIR variable with the correct path\")\n",
    "else:\n",
    "    print(f\"✅ Input dataset found: {INPUT_DATASET_DIR}\")\n",
    "    \n",
    "    # Validate parameters\n",
    "    if N_LAYERS <= 0:\n",
    "        print(f\"❌ N_LAYERS must be positive, got: {N_LAYERS}\")\n",
    "    elif MIN_SPACING < 0:\n",
    "        print(f\"❌ MIN_SPACING must be non-negative, got: {MIN_SPACING}\")\n",
    "    else:\n",
    "        print(f\"✅ Starting processing to extract {N_LAYERS} layers with {MIN_SPACING} spacing...\")\n",
    "        \n",
    "        # Process the dataset\n",
    "        print(f\"\\n🚀 Starting dataset processing...\")\n",
    "        stats = process_dataset(INPUT_DATASET_DIR, OUTPUT_DATASET_DIR, N_LAYERS, MIN_SPACING, BACKGROUND_VALUE)\n",
    "        \n",
    "        # Print final summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"📊 PROCESSING SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total files processed: {stats['processed_files']}/{stats['total_files']}\")\n",
    "        print(f\"Total layers extracted: {stats['total_layers_saved']}\")\n",
    "        print(f\"Average layers per file: {stats['total_layers_saved']/max(stats['processed_files'], 1):.1f}\")\n",
    "        print(f\"Output directory: {OUTPUT_DATASET_DIR}\")\n",
    "        \n",
    "        for split_name, split_data in stats['splits'].items():\n",
    "            print(f\"\\n{split_name.upper()}:\")\n",
    "            for class_name, class_data in split_data['classes'].items():\n",
    "                avg_layers = class_data['total_layers'] / max(class_data['processed_files'], 1)\n",
    "                print(f\"  Class {class_name}: {class_data['processed_files']}/{class_data['total_files']} files → {class_data['total_layers']} layers (avg: {avg_layers:.1f})\")\n",
    "        \n",
    "        print(f\"\\n✅ Processing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dmodelGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
